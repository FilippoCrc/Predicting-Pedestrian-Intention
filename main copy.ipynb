{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaad_data import JAAD\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import network\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JAAD_PATH = '../JAAD'\n",
    "DEEPLAB_PATH = '../best_deeplabv3plus_resnet101_cityscapes_os16.pth'\n",
    "SUBSET_PATH = '../subset'\n",
    "FILENAME_SUB = '../masks_results_sub.pkl'\n",
    "FILENAME_BIG = '../masks_results_big.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Generating action sequence data\n",
      "fstride: 10\n",
      "sample_type: beh\n",
      "subset: default\n",
      "height_rng: [0, inf]\n",
      "squarify_ratio: 0\n",
      "data_split_type: default\n",
      "seq_type: intention\n",
      "min_track_size: 15\n",
      "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
      "kfold_params: {'num_folds': 5, 'fold': 1}\n",
      "---------------------------------------------------------\n",
      "Generating database for jaad\n",
      "jaad database loaded from c:\\Users\\jacop\\Documents\\ComputerVision\\subset\\data_cache\\jaad_database.pkl\n",
      "---------------------------------------------------------\n",
      "Generating intention data\n",
      "Split: train\n",
      "Number of pedestrians: 4 \n",
      "Total number of samples: 2 \n",
      "---------------------------------------------------------\n",
      "Generating action sequence data\n",
      "fstride: 10\n",
      "sample_type: beh\n",
      "subset: default\n",
      "height_rng: [0, inf]\n",
      "squarify_ratio: 0\n",
      "data_split_type: default\n",
      "seq_type: intention\n",
      "min_track_size: 15\n",
      "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
      "kfold_params: {'num_folds': 5, 'fold': 1}\n",
      "---------------------------------------------------------\n",
      "Generating database for jaad\n",
      "jaad database loaded from c:\\Users\\jacop\\Documents\\ComputerVision\\subset\\data_cache\\jaad_database.pkl\n",
      "---------------------------------------------------------\n",
      "Generating intention data\n",
      "Split: test\n",
      "Number of pedestrians: 6 \n",
      "Total number of samples: 0 \n"
     ]
    }
   ],
   "source": [
    "# Load the JAAD dataset\n",
    "jaad_dt = JAAD(data_path=SUBSET_PATH)\n",
    "#jaad.generate_database()\n",
    "#jaad_dt.get_data_stats()\n",
    "\n",
    "data_opts = {\n",
    "    'fstride': 10,\n",
    "    'sample_type': 'beh',\n",
    "    #'height_rng': [10, 50],\n",
    "    'fstride': 10,\n",
    "    'sample_type': 'beh'\n",
    "}\n",
    "\n",
    "seq_train = jaad_dt.generate_data_trajectory_sequence('train', **data_opts)  \n",
    "seq_test = jaad_dt.generate_data_trajectory_sequence('test', **data_opts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Ridimensiona le immagini a 256x256\n",
    "    transforms.Resize((512, 512)),  # Ridimensiona le immagini a 256x256\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" deeplab_model = models.segmentation.deeplabv3_resnet101(pretrained=True).to(device)\n",
    "deeplab_model.eval()  # Imposta il modello in modalità di valutazione \"\"\"\n",
    "#deeplab_model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "deeplab_model = network.modeling.__dict__['deeplabv3plus_resnet101'](num_classes=19)\n",
    "deeplab_model.load_state_dict(torch.load(DEEPLAB_PATH)['model_state'])\n",
    "deeplab_model.to(device)\n",
    "deeplab_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL_NAMES = np.asarray([\n",
    "#             'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
    "#             'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',\n",
    "#             'bus', 'train', 'motorcycle', 'bycycle'])\n",
    "\n",
    "# FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
    "# FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_mask(image_path, model, preprocess):\n",
    "    # Load the image\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = preprocess(input_image).to(device)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Create a batch with a single image\n",
    "    \n",
    "    # Pass the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    # Check if output is a tensor or a dictionary\n",
    "    if isinstance(output, dict):\n",
    "        output = output['out'][0]\n",
    "    elif isinstance(output, torch.Tensor):\n",
    "        output = output[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected output type: {type(output)}\")\n",
    "    \n",
    "    # Convert the output to a mask\n",
    "    output_predictions = output.argmax(0).cpu()\n",
    "    return output_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_frames(seq_train, model, preprocess):\n",
    "    all_masks = []\n",
    "    for video_frames in seq_train['image']:\n",
    "        video_masks = []\n",
    "        #sampled_frame_paths = video_frames[::3]\n",
    "\n",
    "        for frame_path in video_frames:\n",
    "        for frame_path in video_frames:\n",
    "            mask = get_segmentation_mask(frame_path, model, preprocess)\n",
    "            #visualize_mask(frame_path, mask)\n",
    "            #visualize_mask(frame_path, mask)\n",
    "            video_masks.append(mask)\n",
    "        all_masks.append(video_masks)\n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza la maschera semantica\n",
    "def visualize_mask(image_path, mask):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((256, 256))  # Ridimensiona per la visualizzazione\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap='jet')\n",
    "    plt.title(\"Semantic Mask\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_video_masks = process_video_frames(seq_train, deeplab_model, train_transforms)\n",
    "seq_train['masks'] = all_video_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apri il file in modalità scrittura binaria e salva il dizionario\n",
    "with open(FILENAME_BIG, 'wb') as f:\n",
    "    pickle.dump(seq_train['masks'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedestrianIntentionModel(torch.nn.Module):\n",
    "    def __init__(self, num_hidden_units=256, cell_type='gru'):\n",
    "        super(PedestrianIntentionModel, self).__init__()\n",
    "        \n",
    "        # Load the pretrained VGG16 model\n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        self.feature_extractor = torch.nn.Sequential(*list(vgg.features.children()))\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = torch.nn.GRU(input_size=2*512*7*7, hidden_size=num_hidden_units, batch_first=True)  # Adjust input size\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = torch.nn.Linear(num_hidden_units, 1)\n",
    "        \n",
    "        # Sigmoid activation\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, images, masks):\n",
    "        batch_size, seq_len, c, h, w = images.size()\n",
    "        \n",
    "        # Extract features using VGG16 for images\n",
    "        images = images.view(batch_size * seq_len, c, h, w)\n",
    "        img_features = self.feature_extractor(images)\n",
    "        \n",
    "        # Extract features using VGG16 for masks\n",
    "        masks = masks.view(batch_size * seq_len, c, h, w)\n",
    "        mask_features = self.feature_extractor(masks)\n",
    "        \n",
    "        # Concatenate image features and mask features\n",
    "        features = torch.cat((img_features, mask_features), dim=1)\n",
    "        \n",
    "        # Reshape features for GRU\n",
    "        features = features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Pass through GRU\n",
    "        gru_out, _ = self.gru(features)\n",
    "        \n",
    "        # Use the output of the last time step\n",
    "        last_hidden_state = gru_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(last_hidden_state)\n",
    "        \n",
    "        # Sigmoid activation\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JAADDataset(Dataset):\n",
    "    def __init__(self,seq_data, transform=None):\n",
    "        self.seq_data = seq_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx]).convert('RGB')\n",
    "        mask = Image.fromarray(self.masks[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return image, mask, label\n",
    "\n",
    "# Example usage:\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.blur(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Assume seq_train contains lists of image paths, mask arrays, and labels\n",
    "train_dataset = JAADDataset(seq_train['image'], seq_train['masks'], seq_train['intent'], transform=train_transforms)\n",
    "dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10,  2,  2],\n",
      "        [10, 10, 10,  ..., 10,  2,  2],\n",
      "        [10, 10, 10,  ..., 10,  2,  2],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ...,  2,  2,  2],\n",
      "        [10, 10, 10,  ...,  2,  2,  2],\n",
      "        [10, 10, 10,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10,  2,  2],\n",
      "        [10, 10, 10,  ..., 10,  2,  2],\n",
      "        [10, 10, 10,  ..., 10,  2,  2],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]])], [tensor([[ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        [ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        [ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        [ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        [ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])]]\n"
     ]
    }
   ],
   "source": [
    "#recover data:\n",
    "with open(FILENAME_SUB, 'rb') as f:\n",
    "    seq_train['masks'] = pickle.load(f)\n",
    "\n",
    "# Verifica che i risultati siano stati caricati correttamente\n",
    "print(seq_train['masks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" for seq in seq_train['image']:\\n    for image_path in seq:\\n        if os.path.exists(image_path):\\n            mask = get_semantic_mask(image_path, model, preprocess)\\n            mask_image_path = image_path.replace('.jpg', '_mask.png')\\n            plt.imsave(mask_image_path, mask, cmap='jet') \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming seq_train['image'] and seq_train['labels'] are lists of file paths and corresponding labels\n",
    "def process_images_and_masks(seq_train, deeplab_model, transform):\n",
    "    all_images = []\n",
    "    all_masks = []\n",
    "    all_labels = seq_train['intent']\n",
    "\n",
    "    for video_frames in seq_train['image']:\n",
    "        video_images = []\n",
    "        video_masks = []\n",
    "\n",
    "        for frame_path in video_frames:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(frame_path).convert(\"RGB\")\n",
    "            image = transform(image)\n",
    "            video_images.append(image)\n",
    "\n",
    "            # Get segmentation mask\n",
    "            mask = get_segmentation_mask(frame_path, deeplab_model, transform)\n",
    "            video_masks.append(mask)\n",
    "\n",
    "        all_images.append(video_images)\n",
    "        all_masks.append(video_masks)\n",
    "    \n",
    "    return all_images, all_masks, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_images, all_masks, all_labels \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_images_and_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeeplab_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_transforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m JAADDataset(seq_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m], transform\u001b[38;5;241m=\u001b[39mtrain_transforms)\n\u001b[0;32m      4\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[29], line 18\u001b[0m, in \u001b[0;36mprocess_images_and_masks\u001b[1;34m(seq_train, deeplab_model, transform)\u001b[0m\n\u001b[0;32m     15\u001b[0m     video_images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Get segmentation mask\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mget_segmentation_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeeplab_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     video_masks\u001b[38;5;241m.\u001b[39mappend(mask)\n\u001b[0;32m     21\u001b[0m all_images\u001b[38;5;241m.\u001b[39mappend(video_images)\n",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m, in \u001b[0;36mget_segmentation_mask\u001b[1;34m(image_path, model, preprocess)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_segmentation_mask\u001b[39m(image_path, model, preprocess):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Load the image\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     input_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m preprocess(input_image)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m     input_batch \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Create a batch with a single image\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:916\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    870\u001b[0m ):\n\u001b[0;32m    871\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_images, all_masks, all_labels = process_images_and_masks(seq_train, deeplab_model, train_transforms)\n",
    "\n",
    "train_dataset = JAADDataset(seq_train['image'], transform=train_transforms)\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute '__array_interface__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[36], line 13\u001b[0m, in \u001b[0;36mJAADDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     12\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     16\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3084\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3037\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromarray\u001b[39m(obj, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3038\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3039\u001b[0m \u001b[38;5;124;03m    Creates an image memory from an object exporting the array interface\u001b[39;00m\n\u001b[0;32m   3040\u001b[0m \u001b[38;5;124;03m    (using the buffer protocol)::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3082\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.1.6\u001b[39;00m\n\u001b[0;32m   3083\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3084\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__array_interface__\u001b[49m\n\u001b[0;32m   3085\u001b[0m     shape \u001b[38;5;241m=\u001b[39m arr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   3086\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute '__array_interface__'"
     ]
    }
   ],
   "source": [
    "model = PedestrianIntentionModel(num_hidden_units=256).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1  # Set number of epochs as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, masks, labels in dataloader:\n",
    "        images, masks, labels = images.to(device), masks.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, masks)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "     \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedestrianIntentModel(torch.nn.Module):\n",
    "    def __init__(self, vgg19):\n",
    "        super(PedestrianIntentModel, self).__init__()\n",
    "        self.vgg19 = vgg19\n",
    "        self.gru = torch.nn.GRU(input_size=512 + 3, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(256, 2)  # Output: crossing or not crossing\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, images, bboxes, centers, occlusions):\n",
    "        batch_size, seq_len, c, h, w = images.size()\n",
    "        \n",
    "        # Estrai feature dalle immagini con VGG19\n",
    "        vgg_features = []\n",
    "        for i in range(seq_len):\n",
    "            img = images[:, i]\n",
    "            vgg_feat_img = self.vgg19.features(img)\n",
    "            vgg_feat_img = vgg_feat_img.view(batch_size, -1)\n",
    "            vgg_features.append(vgg_feat_img)\n",
    "        \n",
    "        vgg_features = torch.stack(vgg_features, dim=1).squeeze(2)\n",
    "        #vgg_features = vgg_features.flatten(start_dim=2)\n",
    "        # Combina con i metadati\n",
    "        print(len(bboxes), len(centers), len(occlusions))\n",
    "        print(bboxes.shape, centers.shape, occlusions.shape)\n",
    "\n",
    "        # metadatas = torch.cat((bboxes, centers, occlusions), dim=-1)\n",
    "        # features = torch.cat((vgg_features, metadatas), dim=-1)\n",
    "        \n",
    "        # # Passa attraverso la GRU\n",
    "        # gru_out, _ = self.gru(features)\n",
    "        # out = self.sigmoid(self.fc(gru_out[:, -1, :]))\n",
    "        out = self.sigmoid(self.fc(vgg_features[:, -1, :]))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello VGG19 pre-addestrato\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "\n",
    "# Congela i pesi dei primi strati\n",
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Sostituisci l'ultimo strato della VGG19 per il tuo task specifico\n",
    "num_features = vgg19.classifier[6].in_features\n",
    "vgg19.classifier[6] = torch.nn.Linear(num_features, 2)  # Assuming 2 classes for intent\n",
    "\n",
    "# Abilita il training solo per i nuovi strati aggiunti\n",
    "for param in vgg19.classifier[6].parameters():\n",
    "    param.requires_grad = True\n",
    "# Inizializza il modello con VGG19\n",
    "model = PedestrianIntentModel(vgg19).to(device)\n",
    "\n",
    "# Definisci la loss e l'ottimizzatore\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JAADDataset(Dataset):\n",
    "    def __init__(self, seq_data, transform=None):\n",
    "        self.seq_data = seq_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Carica le immagini e le maschere\n",
    "        print(\"ciao1\")\n",
    "        img_paths = self.seq_data['image'][idx]\n",
    "\n",
    "        images = [Image.open(img_path).convert(\"RGB\") for img_path in img_paths]\n",
    "        print(\"ciao2\")\n",
    "\n",
    "        #mask_paths = self.seq_data['masks'][idx]\n",
    "        print(\"ciao3\")\n",
    "\n",
    "        #masks = [Image.fromarray(mask.numpy()) for mask in mask_paths]\n",
    "        print(\"ciao4\")\n",
    "        if self.transform:\n",
    "            images = [self.transform(img) for img in images]\n",
    "            #masks = [self.transform(mask) for mask in masks]\n",
    "\n",
    "        # Carica i metadati\n",
    "        #pid = torch.tensor(self.seq_data['pid'][idx], dtype=torch.string)\n",
    "        print(len(images))\n",
    "        bboxes = torch.tensor(self.seq_data['bbox'][idx], dtype=torch.float32)\n",
    "        centers = torch.tensor(self.seq_data['center'][idx], dtype=torch.float32)\n",
    "        occlusions = torch.tensor(self.seq_data['occlusion'][idx], dtype=torch.float32)\n",
    "        intents = torch.tensor(self.seq_data['intent'][idx], dtype=torch.long)\n",
    "        #return images,masks, bboxes, centers, occlusions, intents\n",
    "        return images, bboxes, centers, occlusions, intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao1\n",
      "ciao2\n",
      "ciao3\n",
      "ciao4\n",
      "37\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "train_dataset = JAADDataset(seq_train, transform=train_transforms)\n",
    "(train_dataset.__getitem__(0))\n",
    "print(len(train_dataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "ciao1\n",
      "ciao2\n",
      "ciao3\n",
      "ciao4\n",
      "18\n",
      "ciaogfdknkfdnlgfd\n",
      "1 1 1\n",
      "torch.Size([1, 18, 4]) torch.Size([1, 18, 2]) torch.Size([1, 18])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x131072 and 256x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#     bboxes = bboxes.stack(images, dim=1).squeeze(2).to(device)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#     centers = centers.stack(images, dim=1).squeeze(2).to(device)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#     occlusions = occlusions.stack(images, dim=1).squeeze(2).to(device)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#     intents = intents.stack(images, dim=1).squeeze(2).to(device)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#print(images)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocclusions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, intents)\n\u001b[0;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[48], line 32\u001b[0m, in \u001b[0;36mPedestrianIntentModel.forward\u001b[1;34m(self, images, bboxes, centers, occlusions)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(bboxes\u001b[38;5;241m.\u001b[39mshape, centers\u001b[38;5;241m.\u001b[39mshape, occlusions\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# metadatas = torch.cat((bboxes, centers, occlusions), dim=-1)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# features = torch.cat((vgg_features, metadatas), dim=-1)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# # Passa attraverso la GRU\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# gru_out, _ = self.gru(features)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# out = self.sigmoid(self.fc(gru_out[:, -1, :]))\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvgg_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x131072 and 256x2)"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    model.train()\n",
    "    for images, bboxes, centers, occlusions, intents in train_loader:\n",
    "        print(\"ciaogfdknkfdnlgfd\")\n",
    "        images = torch.stack(images, dim=1).squeeze(2).to(device)  # Converte la lista di immagini in un tensor\n",
    "        #masks = torch.stack(masks, dim=1).squeeze(2).to(device)  # Converte la lista di maschere in un tensor\n",
    "        #pid = pid.stack(images, dim=1).squeeze(2).to(device)\n",
    "        bboxes = bboxes.to(device)\n",
    "        centers = centers.to(device)\n",
    "        occlusions = occlusions.to(device)\n",
    "        intents = intents.to(device)\n",
    "    #     bboxes = bboxes.stack(images, dim=1).squeeze(2).to(device)\n",
    "    #     centers = centers.stack(images, dim=1).squeeze(2).to(device)\n",
    "    #     occlusions = occlusions.stack(images, dim=1).squeeze(2).to(device)\n",
    "    #     intents = intents.stack(images, dim=1).squeeze(2).to(device)\n",
    "        #print(images)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, bboxes, centers, occlusions)\n",
    "        loss = criterion(outputs, intents)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, model, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            images = batch['image']\n",
    "            masks = batch['mask']\n",
    "            intents = batch['intent']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images, masks)\n",
    "            loss = criterion(outputs, intents.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "train_model(train_loader, model, criterion, optimizer, num_epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
