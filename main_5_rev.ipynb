{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaad_data import JAAD\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import network\n",
    "import openpose\n",
    "from openpose import model\n",
    "from openpose import util\n",
    "from openpose.body import Body\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchmetrics import Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JAAD_PATH = '../JAAD'\n",
    "DEEPLAB_PATH = '../best_deeplabv3plus_resnet101_cityscapes_os16.pth'\n",
    "SUBSET_PATH = '../subset'\n",
    "\n",
    "RESULTS_MASK_SUB = '../masks_results_sub.pkl'\n",
    "RESULTS_MASK_BIG = '../masks_results_big.pkl'\n",
    "RESULTS_MASK_BIG_TEST = '../masks_results_big_test.pkl'\n",
    "RESULTS_MASK_SUB_TEST = '../masks_results_sub_test.pkl'\n",
    "\n",
    "RESULTS_POSE_BIG = '../pose_results_big.pkl'\n",
    "RESULTS_POSE_SUB = '../pose_results_sub.pkl'\n",
    "RESULTS_POSE_BIG_TEST = '../pose_results_big_test.pkl'\n",
    "RESULTS_POSE_SUB_TEST = '../pose_results_sub_test.pkl'\n",
    "\n",
    "POSE_PATH = '../body_pose_model.pth'   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG: \n",
    "RUN = False\n",
    "BIG =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BIG:\n",
    "    MASK_CMD = RESULTS_MASK_BIG\n",
    "    POSE_CMD = RESULTS_POSE_BIG\n",
    "    MASK_CMD_TEST = RESULTS_MASK_BIG_TEST\n",
    "    POSE_CMD_TEST = RESULTS_POSE_BIG_TEST\n",
    "    DT_CMD = JAAD_PATH\n",
    "else:\n",
    "    MASK_CMD = RESULTS_MASK_SUB\n",
    "    POSE_CMD = RESULTS_POSE_SUB\n",
    "    MASK_CMD_TEST = RESULTS_MASK_SUB_TEST\n",
    "    POSE_CMD_TEST = RESULTS_POSE_SUB_TEST\n",
    "    DT_CMD = SUBSET_PATH\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stampa una sequenza di informazioni del dataset\n",
    "\n",
    "#jaad_dt.generate_database() # in particolare, questo stampa un sacco di informazioni come borse vestiti semafori ecc\n",
    "#jaad_dt.get_data_stats() # mentre questo stampa informazioni sui numeri di frame, video, pedoni, bbox ecc\n",
    "#print('Train sequences:', len(seq_train['image'][0][0]))  # stampa la lunghezza della sequenza di training selezionata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JAAD dataset\n",
    "jaad_dt = JAAD(data_path=DT_CMD)\n",
    "\n",
    "data_opts = {\n",
    "    'fstride': 15,\n",
    "    'sample_type': 'beh'\n",
    "}\n",
    "\n",
    "seq_train = jaad_dt.generate_data_trajectory_sequence('train', **data_opts)  \n",
    "seq_test = jaad_dt.generate_data_trajectory_sequence('test', **data_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((seq_train['intent'][1]))\n",
    "# print((seq_train['image'][1]))\n",
    "# print(len(seq_train['bbox']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trasformazioni che vengono usate dentro global context\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Ridimensiona le immagini a 256x256\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasformazioni per le immaginin che vengono usate nel local context e nel pose extractor\n",
    "transform_lc = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL CONTEXT EXTRACTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    deeplab_model = network.modeling.__dict__['deeplabv3plus_resnet101'](num_classes=19)\n",
    "    deeplab_model.load_state_dict(torch.load(DEEPLAB_PATH)['model_state'])\n",
    "    deeplab_model.to(device)\n",
    "    deeplab_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GC_trans = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ridimensiona le immagini a 256x256\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_mask(image_path, model, preprocess):\n",
    "    \"\"\" funzione che prende in input le path delle imagini, il modello e la funzione di preprocessamento \n",
    "    e restituisce la maschera segmentata dell'immagine resizata a 224x224\"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = preprocess(input_image).to(device)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Create a batch with a single image\n",
    "    \n",
    "    # Pass the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    # Check if output is a tensor or a dictionary\n",
    "    if isinstance(output, dict):\n",
    "        output = output['out'][0]\n",
    "    elif isinstance(output, torch.Tensor):\n",
    "        output = output[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected output type: {type(output)}\")\n",
    "    \n",
    "    # Convert the output to a mask\n",
    "    output_predictions = output.argmax(0)\n",
    "    \n",
    "    # Aggiungi una dimensione batch e canale alla maschera per il ridimensionamento\n",
    "    output_predictions = output_predictions.unsqueeze(0).unsqueeze(0).float()\n",
    "    #print(output_predictions.shape)\n",
    "    tr = transforms.ToPILImage()\n",
    "    pic = tr(output_predictions.squeeze(1))\n",
    "    pic= pic.convert(\"RGB\")\n",
    "    resized_mask = GC_trans(pic)\n",
    "\n",
    "    #print(resized_mask.shape)\n",
    "    # resized_maskk = tr(resized_mask)\n",
    "    # plt.imshow(resized_maskk)\n",
    "    # plt.show()\n",
    "    return resized_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_video_frames(seq_train, model, preprocess):\n",
    "#     \"\"\"funzione che prende in input la sequenza di training, il modello e la funzione di preprocessamento, restituisce una\n",
    "#     lista di segmentation mask per ogni frame di ogni video della sequenza di training\"\"\"\n",
    "    \n",
    "#     all_masks = []\n",
    "#     for video_frames in seq_train['image']:\n",
    "#         video_masks = []\n",
    "\n",
    "#         for frame_path in video_frames:\n",
    "#             mask = get_segmentation_mask(frame_path, model, preprocess)\n",
    "#             #visualize_mask(frame_path, mask)\n",
    "#             video_masks.append(mask)\n",
    "#         all_masks.append(video_masks)\n",
    "#     return all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_frames(seq_train, model, preprocess):\n",
    "    \"\"\"funzione che prende in input la sequenza di training, il modello e la funzione di preprocessamento, restituisce una\n",
    "    lista di segmentation mask per ogni frame di ogni video della sequenza di training\"\"\"\n",
    "    \n",
    "    all_masks = []\n",
    "    for video_frames in tqdm(seq_train['image'], desc=\"Processing videos\"):\n",
    "        video_masks = []\n",
    "\n",
    "        for frame_path in tqdm(video_frames, desc=\"Processing frames\", leave=False):\n",
    "            mask = get_segmentation_mask(frame_path, model, preprocess)\n",
    "            #visualize_mask(frame_path, mask)\n",
    "            video_masks.append(mask)\n",
    "        all_masks.append(video_masks)\n",
    "    \n",
    "    return all_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza la maschera semantica\n",
    "def visualize_mask(image_path, mask):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((256, 256))  # Ridimensiona per la visualizzazione\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap='jet')\n",
    "    plt.title(\"Semantic Mask\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    all_video_masks = process_video_frames(seq_train, deeplab_model, train_transforms)\n",
    "    seq_train['masks'] = all_video_masks\n",
    "    all_video_masks_test = process_video_frames(seq_test, deeplab_model, train_transforms)\n",
    "    seq_test['masks'] = all_video_masks_test\n",
    "    del deeplab_model\n",
    "    torch.cuda.empty_cache()\n",
    "    # Apri il file in modalità scrittura binaria e salva il dizionario\n",
    "    with open(MASK_CMD, 'wb') as f:\n",
    "        pickle.dump(seq_train['masks'], f)\n",
    "    with open(MASK_CMD_TEST, 'wb') as f:\n",
    "        pickle.dump(seq_test['masks'], f)\n",
    "else:\n",
    "    #recover data:\n",
    "    with open(MASK_CMD, 'rb') as f:\n",
    "        seq_train['masks'] = pickle.load(f)\n",
    "    with open(MASK_CMD_TEST, 'rb') as f:\n",
    "        seq_test['masks'] = pickle.load(f)\n",
    "    # Verifica che i risultati siano stati caricati correttamente\n",
    "    #print(seq_train['masks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOCAL CONTEXT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, bbox):\n",
    "    \"\"\" funzione che croppa i frames sul bounding boxes, le imagini sono in formato PIL\"\"\"\n",
    "\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return img.crop((x1, y1, x2, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_cv2(img, bbox):\n",
    "    \"\"\" funzione che croppa i frames sul bounding boxes, le imagini sono in formato cv2\"\"\"\n",
    "    \n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return img[int(y1):int(y2), int(x1):int(x2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trasformation for the local context's images, enhance the quality of the images by appling gaussian filter, unsharp mask e bilateral filter\"\"\"\n",
    "all_images = []\n",
    "for i in tqdm(range(len(seq_train['image'])), desc=\"Processing videos\"):\n",
    "    aux_list = []\n",
    "    for j in tqdm(range(len(seq_train['image'][i])), desc=\"Processing frames\", leave=False):\n",
    "        \n",
    "        # Open the images from the paths\n",
    "        img_path = seq_train['image'][i][j]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        \n",
    "        bbox = seq_train['bbox'][i][j]\n",
    "        # Compute local context cropping images around the bboxes\n",
    "        cropped_images = crop_image_cv2(img, bbox)\n",
    "        \n",
    "        # Enhance the image using various filters\n",
    "        blurred_image = cv2.GaussianBlur(cropped_images, (5, 5), 0)\n",
    "\n",
    "        sharpness = 1.5  # Sharpness factor\n",
    "        blurred_for_sharp = cv2.GaussianBlur(blurred_image, (0, 0), 5)\n",
    "        sharpened_image = cv2.addWeighted(blurred_image, 1.0 + sharpness, blurred_for_sharp, -sharpness, 0)\n",
    "\n",
    "        # Noise reduction using bilateral filter\n",
    "        denoised_image = cv2.bilateralFilter(sharpened_image, 9, 75, 75)\n",
    "\n",
    "        aux_list.append(denoised_image)\n",
    "    all_images.append(aux_list)\n",
    "\n",
    "# print(type(all_images))\n",
    "# print(all_images[0][0])\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(all_images[0][0])\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Original (Denormalized) Image\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_test = []\n",
    "for i in tqdm(range(len(seq_test['image'])), desc=\"Processing videos\"):\n",
    "    aux_list = []\n",
    "    for j in tqdm(range(len(seq_test['image'][i])), desc=\"Processing frames\", leave=False):\n",
    "        \n",
    "        # Open the images from the paths\n",
    "        img_path = seq_test['image'][i][j]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        \n",
    "        bbox = seq_test['bbox'][i][j]\n",
    "        # Compute local context cropping images around the bboxes\n",
    "        cropped_images = crop_image_cv2(img, bbox)\n",
    "        \n",
    "        # Enhance the image using various filters\n",
    "        blurred_image = cv2.GaussianBlur(cropped_images, (5, 5), 0)\n",
    "\n",
    "        sharpness = 1.5  # Sharpness factor\n",
    "        blurred_for_sharp = cv2.GaussianBlur(blurred_image, (0, 0), 5)\n",
    "        sharpened_image = cv2.addWeighted(blurred_image, 1.0 + sharpness, blurred_for_sharp, -sharpness, 0)\n",
    "\n",
    "        # Noise reduction using bilateral filter\n",
    "        denoised_image = cv2.bilateralFilter(sharpened_image, 9, 75, 75)\n",
    "\n",
    "        aux_list.append(denoised_image)\n",
    "    all_images_test.append(aux_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSE KEYPOINTS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"block of code to extract the pose from the images\"\"\"\n",
    "# body_estimation = Body('../body_pose_model.pth')\n",
    "# test_image = 'pic3.png'\n",
    "# oriImg = cv2.imread(test_image)  # B,G,R order\n",
    "# candidate, subset = body_estimation(oriImg)\n",
    "# canvas = copy.deepcopy(oriImg)\n",
    "# canvas = util.draw_bodypose(canvas, candidate, subset)\n",
    "# plt.imshow(canvas[:, :, [2, 1, 0]])\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_sequence(frames, body_model):\n",
    "    \"\"\"funzione che prende in input i frames e il modello di openpose e restituisce una lista di tensori di pose \n",
    "    per ciascuna persona nel tempo\"\"\"\n",
    "    \n",
    "    pose_sequences = []  # Lista di pose per ciascuna persona nel tempo\n",
    "    for frame in frames:\n",
    "        candidate, subset = body_model(frame)\n",
    "\n",
    "        # block to visualize the poses on the images by printing both\n",
    "        \n",
    "        # canvas = copy.deepcopy(frame)\n",
    "        # canvas = util.draw_bodypose(canvas, candidate, subset)\n",
    "        # plt.imshow(canvas[:, :, [2, 1, 0]])  # OpenCV usa BGR, matplotlib usa RGB\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "\n",
    "        frame_poses = []\n",
    "        for person in subset:\n",
    "            if person[-1] >= 4:  # Almeno 4 punti chiave rilevati\n",
    "                pose = []\n",
    "                for i in range(18):\n",
    "                    if person[i] != -1:\n",
    "                        x, y = candidate[int(person[i])][:2]\n",
    "                    else:\n",
    "                        x, y = -1, -1  # Punti chiave mancanti\n",
    "                    pose.extend([x, y])\n",
    "                frame_poses.append(pose)\n",
    "        \n",
    "        pose_sequences.append(frame_poses)\n",
    "\n",
    "    # Trasponi la lista di liste per ottenere le sequenze temporali per ciascuna persona\n",
    "    person_pose_sequences = list(map(list, zip(*pose_sequences)))\n",
    "    person_pose_sequences = [torch.tensor(person_poses, dtype=torch.float32) for person_poses in person_pose_sequences]\n",
    "    return person_pose_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if RUN:\n",
    "    body_model = Body(POSE_PATH)\n",
    "\n",
    "    # Caricamento dei frame e estrazione delle pose\n",
    "    all_poses = []\n",
    "    all_poses_test = []\n",
    "    #itera tra i video e prendi i frames\n",
    "    for pics in tqdm(all_images, desc=\"Extracting poses from image sequences\"):\n",
    "        pose_sequences = extract_pose_sequence(pics, body_model)\n",
    "        all_poses.append(pose_sequences)\n",
    "\n",
    "    # Aggiungi le pose estratte alla sequenza di allenamento\n",
    "    seq_train['poses'] = all_poses\n",
    "\n",
    "    for pics in tqdm(all_images_test, desc=\"Extracting poses from image sequences of test set\"):\n",
    "        pose_sequences = extract_pose_sequence(pics, body_model)\n",
    "        all_poses_test.append(pose_sequences)\n",
    "    # Aggiungi le pose estratte alla sequenza di allenamento\n",
    "    seq_test['poses'] = all_poses_test\n",
    "    del body_model\n",
    "    torch.cuda.empty_cache()\n",
    "    # Apri il file in modalità scrittura binaria e salva il dizionario\n",
    "\n",
    "    with open(POSE_CMD, 'wb') as f:\n",
    "        pickle.dump(seq_train['poses'], f)\n",
    "    with open(POSE_CMD_TEST, 'wb') as f:\n",
    "        pickle.dump(seq_test['poses'], f)\n",
    "else:\n",
    "    #recover data:\n",
    "    with open(POSE_CMD, 'rb') as f:\n",
    "        seq_train['poses'] = pickle.load(f)\n",
    "    with open(POSE_CMD_TEST, 'rb') as f:\n",
    "        seq_test['poses'] = pickle.load(f)\n",
    "    # Verifica che i risultati siano stati caricati correttamente\n",
    "    #print(seq_train['masks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trasforma le pose in tensor con la forma desiderata [batch_size, num_frames, 36]\n",
    "# def pad_and_stack(pose_sequences, num_frames):\n",
    "#     padded_sequences = []\n",
    "#     for person_poses in pose_sequences:\n",
    "#         if person_poses.shape[0] < num_frames:\n",
    "#             padding = torch.full((num_frames - person_poses.shape[0], 36), -1)\n",
    "#             person_poses = torch.cat((person_poses, padding), dim=0)\n",
    "#         padded_sequences.append(person_poses)\n",
    "#     if padded_sequences:\n",
    "#         return torch.stack(padded_sequences)\n",
    "#     return torch.tensor([])\n",
    "\n",
    "# num_frames = max((len(seq) for batch in all_poses for seq in batch), default=0)\n",
    "# batch_size = len(all_poses)\n",
    "\n",
    "# if num_frames > 0:\n",
    "#     # Aggrega tutte le pose in un tensor\n",
    "#     all_poses_tensor = []\n",
    "#     for batch in all_poses:\n",
    "#         batch_poses = pad_and_stack(batch, num_frames)\n",
    "#         if batch_poses.size(0) > 0:\n",
    "#             all_poses_tensor.append(batch_poses)\n",
    "\n",
    "#     if all_poses_tensor:\n",
    "#         all_poses_tensor = torch.stack(all_poses_tensor)\n",
    "#         print(all_poses_tensor.shape)  # Dovrebbe essere [batch_size, num_frames, 36]\n",
    "\n",
    "#         # Aggiungi le pose tensorizzate alla sequenza di allenamento\n",
    "#         seq_train['poses'] = all_poses_tensor\n",
    "#     else:\n",
    "#         print(\"Nessuna pose valida trovata nelle sequenze di immagini.\")\n",
    "# else:\n",
    "#     print(\"Nessuna sequenza di pose trovata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionBranchLocal(torch.nn.Module):\n",
    "    \"\"\"definizione del modello per il local context, prende in input le immagini croppate e restituisce un tensore,\n",
    "    le immagini croppate vengono fatte passare dentro una VGG16, una GRU e un attention block\"\"\"\n",
    "\n",
    "    def __init__(self, vgg16):\n",
    "        super(VisionBranchLocal, self).__init__()\n",
    "        self.vgg16 = vgg16\n",
    "        self.avgpool = torch.nn.AvgPool2d(kernel_size=14)  # Pooling layer con kernel 14x14\n",
    "        self.gru = torch.nn.GRU(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(256, 2)    # Fully connected layer\n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, cropped_images):\n",
    "        seq_len, c, h, w = cropped_images.size()\n",
    "        \n",
    "        # Estrai feature dalle immagini con VGG16\n",
    "        vgg_features = []\n",
    "        for i in range(seq_len):            \n",
    "\n",
    "            img = cropped_images[i]            \n",
    "            vgg_feat_img = self.vgg16.features(img)\n",
    "            pooled_feat_img = self.avgpool(vgg_feat_img)  # Applica il pooling\n",
    "            vgg_feat_img = pooled_feat_img.view(pooled_feat_img.size(0), -1)  # Flatten features\n",
    "            vgg_features.append(vgg_feat_img)\n",
    "        \n",
    "        vgg_features = torch.stack(vgg_features, dim=1).permute(2,1,0)\n",
    "\n",
    "        gru_out, _ = self.gru(vgg_features)\n",
    "        attn_weights = torch.softmax(self.attn(gru_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n",
    "        \n",
    "       # print(\"SIZE vgg features:\",vgg_features.shape)\n",
    "       # print(\"SIZE context v local context:\",context_vector.shape)\n",
    "        #out = self.sigmoid(self.fc(gru_out[:, -1, :]))   \n",
    "        out = self.tanh((context_vector))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionBranchGlobal(torch.nn.Module):\n",
    "    \"\"\"definizione del modello per il global context, prende in input le maskere semantiche e restituisce un tensore,\n",
    "    le maskere semantiche vengono fatte passare dentro una VGG16, una GRU e un attention block\"\"\"\n",
    "\n",
    "    def __init__(self, vgg16):\n",
    "        super(VisionBranchGlobal, self).__init__()\n",
    "        self.vgg16 = vgg16\n",
    "        self.avgpool = torch.nn.AvgPool2d(kernel_size=14)  # Pooling layer con kernel 14x14\n",
    "        self.gru = torch.nn.GRU(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(256, 2)    # Fully connected layer\n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, masks):\n",
    "        seq_len = masks.size()[0]\n",
    "        #print(\"size forward:\",seq_len)\n",
    "        # Estrai feature dalle immagini con VGG16\n",
    "        vgg_features = []\n",
    "        for i in range(seq_len):            \n",
    "            img = masks[i]            \n",
    "            vgg_feat_img = self.vgg16.features(img)\n",
    "            pooled_feat_img = self.avgpool(vgg_feat_img)  # Applica il pooling\n",
    "            vgg_feat_img = pooled_feat_img.view(pooled_feat_img.size(0), -1)  # Flatten features\n",
    "            vgg_features.append(vgg_feat_img)\n",
    "        \n",
    "        vgg_features = torch.stack(vgg_features, dim=1).permute(2,1,0)\n",
    "\n",
    "        gru_out, _ = self.gru(vgg_features)\n",
    "        attn_scores = self.attn(gru_out)  # shape: (batch_size, seq_length, 1)\n",
    "       # print(\"SIZE attention scores GLO:\", attn_scores.shape)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)  # shape: (batch_size, seq_length, 1)\n",
    "        #print(\"SIZE attention weights GLO:\", attn_weights.shape)\n",
    "        \n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)  # shape: (batch_size, 256)\n",
    "        #print(\"SIZE context vector GLO:\", context_vector.shape)\n",
    "        \n",
    "        out = self.tanh((context_vector))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NVisionBranch(torch.nn.Module):\n",
    "    \"\"\"classe relativa al non-vision brach, prende in input le pose e le bbox in formato tensore, esse vengono fatte passare\n",
    "    dentro una GRU e un attention block, l'ordine influenza la prestazioni\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NVisionBranch, self).__init__()\n",
    "        self.gru = torch.nn.GRU(input_size=36, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.gru2 = torch.nn.GRU(input_size=256+4, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        #self.fc = torch.nn.Linear(256, 2)  \n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, poses,bbox):\n",
    "        gru_out, _ = self.gru(poses)\n",
    "        #print(\"SIZE outuyput gru posa:\",gru_out.shape)\n",
    "        #print(\"bbox:\",bbox.shape)\n",
    "        LP = torch.cat((gru_out,bbox),dim=-1)\n",
    "        #print(\"SIZE outuyput gru posa + bbox:\",LP.shape)\n",
    "        gru_out, _ = self.gru2(LP)\n",
    "       # print(\"SIZE output gru:\",gru_out.shape)\n",
    "\n",
    "        # Attention mechanism\n",
    "        #features = torch.stack([gru_out[:,i,:] for i in range(gru_out.size(1))], dim=1)\n",
    "        attn_scores = self.attn(gru_out)  # shape: (batch_size, seq_length, 1)\n",
    "       # print(\"SIZE attention scores:\", attn_scores.shape)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)  # shape: (batch_size, seq_length, 1)\n",
    "        #print(\"SIZE attention weights:\", attn_weights.shape)\n",
    "        \n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)  # shape: (batch_size, 256)\n",
    "        #print(\"SIZE context vector:\", context_vector.shape)\n",
    "        \n",
    "        #out = self.sigmoid(self.fc(gru_out[:, -1, :]))   \n",
    "        out = self.tanh(context_vector)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(torch.nn.Module):\n",
    "#     def __init__(self, hidden_dim):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.ws = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.wc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "#     def forward(self, hidden_states, final_hidden):\n",
    "#         # hidden_states: (T, hidden_dim)\n",
    "#         # final_hidden: (hidden_dim)\n",
    "        \n",
    "#         # Expand final_hidden to match the dimensions of hidden_states\n",
    "#         final_hidden_exp = final_hidden.unsqueeze(0).expand(hidden_states.size(0), -1)  # (T, hidden_dim)\n",
    "        \n",
    "#         # Compute attention scores\n",
    "#         scores = torch.matmul(hidden_states, self.ws(final_hidden_exp).transpose(0, 1))  # (T, T)\n",
    "#         scores = scores[-1]  # We only need the scores for the last hidden state\n",
    "#         attn_weights = F.softmax(scores, dim=0)  # (T)\n",
    "        \n",
    "#         # Compute the context vector as the weighted sum of hidden states\n",
    "#         context_vector = torch.sum(attn_weights.unsqueeze(-1) * hidden_states, dim=0)  # (hidden_dim)\n",
    "        \n",
    "#         # Combine the context vector with the final hidden state\n",
    "#         combined = torch.cat((context_vector, final_hidden), dim=-1)  # (2 * hidden_dim)\n",
    "#         attention_output = torch.tanh(self.wc(combined))  # (hidden_dim)\n",
    "        \n",
    "#         return attention_output\n",
    "# class PedestrianIntentModel(nn.Module):\n",
    "#     def __init__(self, vision_branch_local, vision_branch_global, non_vision_branch, hidden_dim):\n",
    "#         super(PedestrianIntentModel, self).__init__()\n",
    "#         self.vision_branch_local = vision_branch_local\n",
    "#         self.vision_branch_global = vision_branch_global\n",
    "#         self.non_vision_branch = non_vision_branch\n",
    "        \n",
    "#         self.attention = Attention(hidden_dim)\n",
    "#         self.fc = nn.Linear(hidden_dim, 1)  # Output per frame\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, cropped_images, bboxes, masks, poses):\n",
    "#         vision_out_local = self.vision_branch_local(cropped_images)  # (T, h_local)\n",
    "#         vision_out_global = self.vision_branch_global(masks)  # (T, h_global)\n",
    "#         non_vision_out = self.non_vision_branch(poses, bboxes)  # (T, h_non_vision)\n",
    "        \n",
    "#         # Concatenare gli output dei due vision branch\n",
    "#         vision_out = torch.cat((vision_out_local, vision_out_global), dim=-1)  # (T, h_local + h_global)\n",
    "#         # Concatenare con l'output del non-vision branch\n",
    "#         final_fusion = torch.cat((vision_out, non_vision_out), dim=-1)  # (T, h_total)\n",
    "        \n",
    "#         # Usare l'ultima hidden state del final_fusion per l'attenzione\n",
    "#         final_hidden = final_fusion[-1]  # (h_total)\n",
    "        \n",
    "#         # Applicare il modulo di attenzione\n",
    "#         attention_output = self.attention(final_fusion, final_hidden)  # (hidden_dim)\n",
    "        \n",
    "#         # Applicare il fully connected layer per ogni frame\n",
    "#         out = self.sigmoid(self.fc(attention_output))  # (1)\n",
    "        \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedestrianIntentModel(torch.nn.Module):\n",
    "    \"\"\"definizione del modello finale, prende in input gli output del vision brach e del non vision branch, viene fatta\n",
    "    una concatenazione che in seguito passa dentro un attention e un fully connected layer, l'output è la predizione,\n",
    "    (passa non passa)\"\"\"\n",
    "\n",
    "    def __init__(self, vision_branch_local,vision_branch_global,non_vision_branch):\n",
    "        super(PedestrianIntentModel, self).__init__()\n",
    "        self.vision_branch_local = vision_branch_local\n",
    "        self.vision_branch_global = vision_branch_global\n",
    "        self.non_vision_branch = non_vision_branch\n",
    "        self.attn = torch.nn.Linear(768, 768)  # Attention layer\n",
    "\n",
    "        self.fc = torch.nn.Linear(768, 1) # Output: crossing or not crossing\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, cropped_images, bboxes,masks,poses):\n",
    "        vision_out_local = self.vision_branch_local(cropped_images)\n",
    "        vision_out_global = self.vision_branch_global(masks)\n",
    "        non_vision_out = self.non_vision_branch(poses,bboxes)\n",
    "        #print(\"size local,global,nv:\",vision_out_local.shape,vision_out_global.shape,non_vision_out.shape)\n",
    "        vision_out = torch.cat((vision_out_local, vision_out_global), dim=-1)\n",
    "        final_fusion = torch.cat((vision_out, non_vision_out), dim=-1)\n",
    "        #print(\"SIZE final fusion:\",final_fusion.shape)\n",
    "        #print(\"SIZE vision ouyt :\",vision_out.shape)\n",
    "        attn_scores =self.attn(final_fusion)\n",
    "        #print(\"SIZE attn scores:\",attn_scores.shape)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        #print(\"SIZE att weights final:\",attn_weights.shape) \n",
    "        context_vector = torch.sum(attn_weights * final_fusion, dim=0)\n",
    "        #print(\"SIZE context vector:\",context_vector.shape)\n",
    "        #print(\"SIZE context vector after:\",context_vector.shape)\n",
    "        torch.cuda.empty_cache()\n",
    "        out = self.sigmoid(self.fc(context_vector))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_FeatureExtractor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGG16_FeatureExtractor, self).__init__()\n",
    "        self.features = torch.nn.Sequential(*list(vgg16.features.children())[:24]) # block4_pool è il 23° livello\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello VGG19 pre-addestrato\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "#cut the model at the 24th layer:\n",
    "vgg16_fe = VGG16_FeatureExtractor()\n",
    "vgg16_fe\n",
    "\n",
    "\n",
    "# define the models of each branches\n",
    "model_local = VisionBranchLocal(vgg16_fe).to(device)\n",
    "model_global = VisionBranchGlobal(vgg16_fe).to(device)\n",
    "model_non_vision = NVisionBranch().to(device)\n",
    "model = PedestrianIntentModel(model_local,model_global,model_non_vision).to(device)\n",
    "#model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET & DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JAADDataset(Dataset):\n",
    "    \"\"\"definizione della classe per il custom dataset, prende in input la seq_train, le immagini e le trasformazioni,\n",
    "    restituisce il tensore delle immagini croppate, le bboxes, le maschere, le pose e la lables\"\"\"\n",
    "\n",
    "    def __init__(self, seq_data, all_images, transform=None):\n",
    "        self.seq_data = seq_data\n",
    "        self.all_images = all_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bbox_sequence = self.seq_data['bbox'][idx]\n",
    "        masks = self.seq_data['masks'][idx]\n",
    "        poses = self.seq_data['poses'][idx]\n",
    "        all_images = self.all_images[idx]\n",
    "        #open the images from the paths \n",
    "        #images = [Image.open(img_path).convert(\"RGB\") for img_path in img_paths]\n",
    "\n",
    "        #compute local context cropping images arroun the bboxes\n",
    "        #cropped_images = [crop_image(img, bbox) for img, bbox in zip(images, bbox_sequence)]\n",
    "        # #mask_paths = self.seq_data['masks'][idx]\n",
    "\n",
    "        cropped_images = [cv2.resize(img,(224,224), interpolation=cv2.INTER_CUBIC) for img in all_images]\n",
    "        #masks = [Image.fromarray(mask.numpy()) for mask in mask_paths]\n",
    "        if self.transform:\n",
    "            #images = [self.transform(img) for img in images]\n",
    "            tensor_images = [self.transform(img) for img in cropped_images]\n",
    "            #\n",
    "            #masks = [mask.Resize(224, 224) for mask in masks]\n",
    "        #print(seq_train['poses'][idx])\n",
    "        #print(len(poses))\n",
    "        bboxes = torch.tensor(self.seq_data['bbox'][idx], dtype=torch.float32)\n",
    "        intents = torch.tensor(self.seq_data['intent'][idx], dtype=torch.float32)\n",
    "        return  tensor_images, bboxes, masks, poses, intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JAADDataset(seq_train,all_images=all_images, transform=transform_lc)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataset = JAADDataset(seq_test,all_images=all_images_test, transform=transform_lc)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "#def show_pic(image):    \n",
    "    #numpy_data = image.cpu().numpy()\n",
    "    # Create a new figure\n",
    "    #plt.figure(figsize=(8, 8))\n",
    "    # Display the image\n",
    "    #plt.imshow(numpy_data)  # Use 'gray' colormap for grayscale images\n",
    "    # Remove axes ticks\n",
    "    #plt.axis('off')\n",
    "    # Add a title if needed\n",
    "    #plt.title(\"Your Image Title\")\n",
    "    # Show the plot\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net, loader, device):\n",
    "  acc = Accuracy(task='binary', num_classes=2).to(device)\n",
    "  pose_placeholder = torch.zeros((36,), dtype=torch.float32) \n",
    "\n",
    "  for tensor_images, bboxes, masks, poses, intents in loader:\n",
    "      poses = [pose_placeholder for _ in range(len(tensor_images))]\n",
    "      poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "      #poses = poses.squeeze(0)\n",
    "      poses = poses.view(len(tensor_images), -1, 36).permute(1,0,2)  # Reshape to (batch_size, numeroFrames, 36)\n",
    "      #print(\"TRAINING:\")\n",
    "      #print(\"poses\",len(poses))\n",
    "      # Convert poses to tensor and reshape\n",
    "      #print(\"poses shape:\",poses.shape)   \n",
    "\n",
    "      # Move tensors to device\n",
    "      tensor_images = torch.stack(tensor_images, dim=1).squeeze(0).permute(0, 1, 2,3).to(device)  # Converte la lista di immagini in un tensor\n",
    "      masks = torch.stack(masks,dim=1).squeeze(0).float().to(device)  # Converte la lista di maschere in un tensor\n",
    "      bboxes = bboxes.to(device)\n",
    "      poses = poses.to(device)\n",
    "      intents = intents.squeeze(0)[0].to(device)\n",
    "      ypred = net(tensor_images,bboxes,masks,poses)\n",
    "      tensor_images.cpu()\n",
    "      masks.cpu()\n",
    "      bboxes.cpu()\n",
    "      poses.cpu()\n",
    "      del tensor_images, bboxes, masks, poses\n",
    "      torch.cuda.empty_cache()\n",
    "      _ = acc(ypred, intents)\n",
    "      intents.cpu()\n",
    "      del intents\n",
    "      torch.cuda.empty_cache()\n",
    "  return acc.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 1\n",
    "\n",
    "# free the memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Placeholder tensor for empty poses\n",
    "pose_placeholder = torch.zeros((36,), dtype=torch.float32) \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    model.train()\n",
    "\n",
    "    for tensor_images, bboxes, masks, poses, intents in train_loader:\n",
    "        #case when there are no poses in the frame \n",
    "        # if len(poses) == 0:\n",
    "        #     print(\"Empty poses detected, adding placeholder tensor.\")\n",
    "        #     poses = [pose_placeholder for _ in range(len(tensor_images))]\n",
    "        #     poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        #     #poses = poses.squeeze(0)\n",
    "        #     poses = poses.view(len(tensor_images), -1, 36).permute(1,0,2)  # Reshape to (batch_size, numeroFrames, 36)\n",
    "        # else:\n",
    "        #     #poses = [torch.tensor(p, dtype=torch.float32) for p in poses]\n",
    "        #     poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        #     poses = poses.squeeze(0)\n",
    "        poses = [pose_placeholder for _ in range(len(tensor_images))]\n",
    "        poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        #poses = poses.squeeze(0)\n",
    "        poses = poses.view(len(tensor_images), -1, 36).permute(1,0,2)  # Reshape to (batch_size, numeroFrames, 36)\n",
    "        #print(\"TRAINING:\")\n",
    "        #print(\"poses\",len(poses))\n",
    "        # Convert poses to tensor and reshape\n",
    "        #print(\"poses shape:\",poses.shape)   \n",
    "\n",
    "        # Move tensors to device\n",
    "        tensor_images = torch.stack(tensor_images, dim=1).squeeze(0).permute(0, 1, 2,3).to(device)  # Converte la lista di immagini in un tensor\n",
    "        masks = torch.stack(masks,dim=1).squeeze(0).float().to(device)  # Converte la lista di maschere in un tensor\n",
    "        bboxes = bboxes.to(device)\n",
    "        poses = poses.to(device)\n",
    "        intents = intents.squeeze(0)[0].to(device)\n",
    "        #print(\"ti\",tensor_images.shape)\n",
    "        # print(\"BBOX:\", bboxes.shape)\n",
    "        # print(\"MASKS:\", masks.shape)\n",
    "        # print(\"POSES:\", poses.shape)\n",
    "        # print(\"INTENTS:\", intents.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(tensor_images, bboxes, masks, poses)\n",
    "        tensor_images.cpu()\n",
    "        masks.cpu()\n",
    "        bboxes.cpu()\n",
    "        poses.cpu()\n",
    "        loss = criterion(outputs, intents)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del tensor_images, masks, bboxes, poses, intents, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(f'Accuracy at epoch {epoch}: {accuracy(model, test_loader, device)}')\n",
    "    #print(f'Accuracy at epoch {epoch}: {accuracy(model, test_loader, device)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(seq_test['image'][0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
