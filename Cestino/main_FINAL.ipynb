{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaad_data import JAAD\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import network\n",
    "import openpose\n",
    "from openpose import model\n",
    "from openpose import util\n",
    "from openpose.body import Body\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchmetrics\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "JAAD_PATH = '../JAAD'\n",
    "DEEPLAB_PATH = '../best_deeplabv3plus_resnet101_cityscapes_os16.pth'\n",
    "SUBSET_PATH = '../subset2'\n",
    "\n",
    "RESULTS_MASK_SUB = '../masks_results_sub.pkl'\n",
    "RESULTS_MASK_BIG = '../masks_results_big.pkl'\n",
    "RESULTS_MASK_BIG_TEST = '../masks_results_big_test.pkl'\n",
    "RESULTS_MASK_SUB_TEST = '../masks_results_sub_test.pkl'\n",
    "\n",
    "RESULTS_POSE_BIG = '../pose_results_big.pkl'\n",
    "RESULTS_POSE_SUB = '../pose_results_sub.pkl'\n",
    "RESULTS_POSE_BIG_TEST = '../pose_results_big_test.pkl'\n",
    "RESULTS_POSE_SUB_TEST = '../pose_results_sub_test.pkl'\n",
    "\n",
    "POSE_PATH = '../body_pose_model.pth'   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG (change these params for changing the experiment): \n",
    "RUN = False\n",
    "BIG =False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BIG:\n",
    "    MASK_CMD = RESULTS_MASK_BIG\n",
    "    POSE_CMD = RESULTS_POSE_BIG\n",
    "    MASK_CMD_TEST = RESULTS_MASK_BIG_TEST\n",
    "    POSE_CMD_TEST = RESULTS_POSE_BIG_TEST\n",
    "    DT_CMD = JAAD_PATH\n",
    "else:\n",
    "    MASK_CMD = RESULTS_MASK_SUB\n",
    "    POSE_CMD = RESULTS_POSE_SUB\n",
    "    MASK_CMD_TEST = RESULTS_MASK_SUB_TEST\n",
    "    POSE_CMD_TEST = RESULTS_POSE_SUB_TEST\n",
    "    DT_CMD = SUBSET_PATH\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Generating action sequence data\n",
      "fstride: 15\n",
      "sample_type: all\n",
      "subset: default\n",
      "height_rng: [0, inf]\n",
      "squarify_ratio: 0\n",
      "data_split_type: default\n",
      "seq_type: intention\n",
      "min_track_size: 15\n",
      "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
      "kfold_params: {'num_folds': 5, 'fold': 1}\n",
      "---------------------------------------------------------\n",
      "Generating database for jaad\n",
      "jaad database loaded from c:\\Users\\Filippo\\Documents\\VSCode\\Pedestrian_Intention\\subset2\\data_cache\\jaad_database.pkl\n",
      "---------------------------------------------------------\n",
      "Generating intention data\n",
      "Split: train\n",
      "Number of pedestrians: 185 \n",
      "Total number of samples: 60 \n",
      "---------------------------------------------------------\n",
      "Generating action sequence data\n",
      "fstride: 15\n",
      "sample_type: all\n",
      "subset: default\n",
      "height_rng: [0, inf]\n",
      "squarify_ratio: 0\n",
      "data_split_type: default\n",
      "seq_type: intention\n",
      "min_track_size: 15\n",
      "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
      "kfold_params: {'num_folds': 5, 'fold': 1}\n",
      "---------------------------------------------------------\n",
      "Generating database for jaad\n",
      "jaad database loaded from c:\\Users\\Filippo\\Documents\\VSCode\\Pedestrian_Intention\\subset2\\data_cache\\jaad_database.pkl\n",
      "---------------------------------------------------------\n",
      "Generating intention data\n",
      "Split: test\n",
      "Number of pedestrians: 251 \n",
      "Total number of samples: 97 \n"
     ]
    }
   ],
   "source": [
    "# Load the JAAD dataset\n",
    "jaad_dt = JAAD(data_path=DT_CMD)\n",
    "\n",
    "data_opts = {\n",
    "    'fstride': 15,\n",
    "    #'subset': 'high_visibility',\n",
    "    'sample_type': 'all'\n",
    "}\n",
    "\n",
    "seq_train = jaad_dt.generate_data_trajectory_sequence('train', **data_opts)  \n",
    "seq_test = jaad_dt.generate_data_trajectory_sequence('test', **data_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['../subset2\\\\images\\\\video_0001\\\\00000.png', '../subset2\\\\images\\\\video_0001\\\\00015.png', '../subset2\\\\images\\\\video_0001\\\\00030.png', '../subset2\\\\images\\\\video_0001\\\\00045.png', '../subset2\\\\images\\\\video_0001\\\\00060.png', '../subset2\\\\images\\\\video_0001\\\\00075.png', '../subset2\\\\images\\\\video_0001\\\\00090.png', '../subset2\\\\images\\\\video_0001\\\\00105.png', '../subset2\\\\images\\\\video_0001\\\\00120.png', '../subset2\\\\images\\\\video_0001\\\\00135.png', '../subset2\\\\images\\\\video_0001\\\\00150.png', '../subset2\\\\images\\\\video_0001\\\\00165.png', '../subset2\\\\images\\\\video_0001\\\\00180.png', '../subset2\\\\images\\\\video_0001\\\\00195.png', '../subset2\\\\images\\\\video_0001\\\\00210.png', '../subset2\\\\images\\\\video_0001\\\\00225.png', '../subset2\\\\images\\\\video_0001\\\\00240.png', '../subset2\\\\images\\\\video_0001\\\\00255.png', '../subset2\\\\images\\\\video_0001\\\\00270.png', '../subset2\\\\images\\\\video_0001\\\\00285.png', '../subset2\\\\images\\\\video_0001\\\\00300.png', '../subset2\\\\images\\\\video_0001\\\\00315.png', '../subset2\\\\images\\\\video_0001\\\\00330.png', '../subset2\\\\images\\\\video_0001\\\\00345.png', '../subset2\\\\images\\\\video_0001\\\\00360.png'], ['../subset2\\\\images\\\\video_0014\\\\00000.png', '../subset2\\\\images\\\\video_0014\\\\00015.png', '../subset2\\\\images\\\\video_0014\\\\00030.png', '../subset2\\\\images\\\\video_0014\\\\00045.png', '../subset2\\\\images\\\\video_0014\\\\00060.png', '../subset2\\\\images\\\\video_0014\\\\00075.png', '../subset2\\\\images\\\\video_0014\\\\00090.png', '../subset2\\\\images\\\\video_0014\\\\00105.png', '../subset2\\\\images\\\\video_0014\\\\00120.png', '../subset2\\\\images\\\\video_0014\\\\00135.png', '../subset2\\\\images\\\\video_0014\\\\00150.png', '../subset2\\\\images\\\\video_0014\\\\00165.png', '../subset2\\\\images\\\\video_0014\\\\00180.png', '../subset2\\\\images\\\\video_0014\\\\00195.png', '../subset2\\\\images\\\\video_0014\\\\00210.png', '../subset2\\\\images\\\\video_0014\\\\00225.png', '../subset2\\\\images\\\\video_0014\\\\00240.png'], ['../subset2\\\\images\\\\video_0019\\\\00000.png', '../subset2\\\\images\\\\video_0019\\\\00015.png', '../subset2\\\\images\\\\video_0019\\\\00030.png', '../subset2\\\\images\\\\video_0019\\\\00045.png', '../subset2\\\\images\\\\video_0019\\\\00060.png', '../subset2\\\\images\\\\video_0019\\\\00075.png', '../subset2\\\\images\\\\video_0019\\\\00090.png', '../subset2\\\\images\\\\video_0019\\\\00105.png', '../subset2\\\\images\\\\video_0019\\\\00120.png', '../subset2\\\\images\\\\video_0019\\\\00135.png', '../subset2\\\\images\\\\video_0019\\\\00150.png', '../subset2\\\\images\\\\video_0019\\\\00165.png', '../subset2\\\\images\\\\video_0019\\\\00180.png', '../subset2\\\\images\\\\video_0019\\\\00195.png', '../subset2\\\\images\\\\video_0019\\\\00210.png', '../subset2\\\\images\\\\video_0019\\\\00225.png', '../subset2\\\\images\\\\video_0019\\\\00240.png', '../subset2\\\\images\\\\video_0019\\\\00255.png', '../subset2\\\\images\\\\video_0019\\\\00270.png', '../subset2\\\\images\\\\video_0019\\\\00285.png', '../subset2\\\\images\\\\video_0019\\\\00300.png', '../subset2\\\\images\\\\video_0019\\\\00315.png', '../subset2\\\\images\\\\video_0019\\\\00330.png', '../subset2\\\\images\\\\video_0019\\\\00345.png', '../subset2\\\\images\\\\video_0019\\\\00360.png', '../subset2\\\\images\\\\video_0019\\\\00375.png', '../subset2\\\\images\\\\video_0019\\\\00390.png', '../subset2\\\\images\\\\video_0019\\\\00405.png'], ['../subset2\\\\images\\\\video_0020\\\\00099.png', '../subset2\\\\images\\\\video_0020\\\\00114.png', '../subset2\\\\images\\\\video_0020\\\\00129.png', '../subset2\\\\images\\\\video_0020\\\\00144.png', '../subset2\\\\images\\\\video_0020\\\\00159.png', '../subset2\\\\images\\\\video_0020\\\\00174.png', '../subset2\\\\images\\\\video_0020\\\\00189.png', '../subset2\\\\images\\\\video_0020\\\\00204.png', '../subset2\\\\images\\\\video_0020\\\\00219.png', '../subset2\\\\images\\\\video_0020\\\\00234.png', '../subset2\\\\images\\\\video_0020\\\\00249.png', '../subset2\\\\images\\\\video_0020\\\\00264.png', '../subset2\\\\images\\\\video_0020\\\\00279.png', '../subset2\\\\images\\\\video_0020\\\\00294.png', '../subset2\\\\images\\\\video_0020\\\\00309.png', '../subset2\\\\images\\\\video_0020\\\\00324.png', '../subset2\\\\images\\\\video_0020\\\\00339.png', '../subset2\\\\images\\\\video_0020\\\\00354.png', '../subset2\\\\images\\\\video_0020\\\\00369.png', '../subset2\\\\images\\\\video_0020\\\\00384.png', '../subset2\\\\images\\\\video_0020\\\\00399.png', '../subset2\\\\images\\\\video_0020\\\\00414.png', '../subset2\\\\images\\\\video_0020\\\\00429.png', '../subset2\\\\images\\\\video_0020\\\\00444.png', '../subset2\\\\images\\\\video_0020\\\\00459.png', '../subset2\\\\images\\\\video_0020\\\\00474.png', '../subset2\\\\images\\\\video_0020\\\\00489.png', '../subset2\\\\images\\\\video_0020\\\\00504.png', '../subset2\\\\images\\\\video_0020\\\\00519.png', '../subset2\\\\images\\\\video_0020\\\\00534.png'], ['../subset2\\\\images\\\\video_0020\\\\00000.png', '../subset2\\\\images\\\\video_0020\\\\00015.png', '../subset2\\\\images\\\\video_0020\\\\00030.png', '../subset2\\\\images\\\\video_0020\\\\00045.png', '../subset2\\\\images\\\\video_0020\\\\00060.png', '../subset2\\\\images\\\\video_0020\\\\00075.png', '../subset2\\\\images\\\\video_0020\\\\00090.png', '../subset2\\\\images\\\\video_0020\\\\00105.png', '../subset2\\\\images\\\\video_0020\\\\00120.png', '../subset2\\\\images\\\\video_0020\\\\00135.png', '../subset2\\\\images\\\\video_0020\\\\00150.png', '../subset2\\\\images\\\\video_0020\\\\00165.png', '../subset2\\\\images\\\\video_0020\\\\00180.png', '../subset2\\\\images\\\\video_0020\\\\00195.png', '../subset2\\\\images\\\\video_0020\\\\00210.png', '../subset2\\\\images\\\\video_0020\\\\00225.png', '../subset2\\\\images\\\\video_0020\\\\00240.png', '../subset2\\\\images\\\\video_0020\\\\00255.png', '../subset2\\\\images\\\\video_0020\\\\00270.png', '../subset2\\\\images\\\\video_0020\\\\00285.png'], ['../subset2\\\\images\\\\video_0020\\\\00000.png', '../subset2\\\\images\\\\video_0020\\\\00015.png', '../subset2\\\\images\\\\video_0020\\\\00030.png', '../subset2\\\\images\\\\video_0020\\\\00045.png', '../subset2\\\\images\\\\video_0020\\\\00060.png', '../subset2\\\\images\\\\video_0020\\\\00075.png', '../subset2\\\\images\\\\video_0020\\\\00090.png', '../subset2\\\\images\\\\video_0020\\\\00105.png', '../subset2\\\\images\\\\video_0020\\\\00120.png', '../subset2\\\\images\\\\video_0020\\\\00135.png', '../subset2\\\\images\\\\video_0020\\\\00150.png', '../subset2\\\\images\\\\video_0020\\\\00165.png', '../subset2\\\\images\\\\video_0020\\\\00180.png', '../subset2\\\\images\\\\video_0020\\\\00195.png', '../subset2\\\\images\\\\video_0020\\\\00210.png', '../subset2\\\\images\\\\video_0020\\\\00225.png', '../subset2\\\\images\\\\video_0020\\\\00240.png', '../subset2\\\\images\\\\video_0020\\\\00255.png', '../subset2\\\\images\\\\video_0020\\\\00270.png', '../subset2\\\\images\\\\video_0020\\\\00285.png', '../subset2\\\\images\\\\video_0020\\\\00300.png', '../subset2\\\\images\\\\video_0020\\\\00315.png', '../subset2\\\\images\\\\video_0020\\\\00330.png', '../subset2\\\\images\\\\video_0020\\\\00345.png', '../subset2\\\\images\\\\video_0020\\\\00360.png', '../subset2\\\\images\\\\video_0020\\\\00375.png'], ['../subset2\\\\images\\\\video_0020\\\\00099.png', '../subset2\\\\images\\\\video_0020\\\\00114.png', '../subset2\\\\images\\\\video_0020\\\\00129.png', '../subset2\\\\images\\\\video_0020\\\\00144.png', '../subset2\\\\images\\\\video_0020\\\\00159.png', '../subset2\\\\images\\\\video_0020\\\\00174.png', '../subset2\\\\images\\\\video_0020\\\\00189.png', '../subset2\\\\images\\\\video_0020\\\\00204.png', '../subset2\\\\images\\\\video_0020\\\\00219.png', '../subset2\\\\images\\\\video_0020\\\\00234.png', '../subset2\\\\images\\\\video_0020\\\\00249.png', '../subset2\\\\images\\\\video_0020\\\\00264.png', '../subset2\\\\images\\\\video_0020\\\\00279.png', '../subset2\\\\images\\\\video_0020\\\\00294.png', '../subset2\\\\images\\\\video_0020\\\\00309.png', '../subset2\\\\images\\\\video_0020\\\\00324.png', '../subset2\\\\images\\\\video_0020\\\\00339.png', '../subset2\\\\images\\\\video_0020\\\\00354.png', '../subset2\\\\images\\\\video_0020\\\\00369.png', '../subset2\\\\images\\\\video_0020\\\\00384.png', '../subset2\\\\images\\\\video_0020\\\\00399.png', '../subset2\\\\images\\\\video_0020\\\\00414.png', '../subset2\\\\images\\\\video_0020\\\\00429.png', '../subset2\\\\images\\\\video_0020\\\\00444.png', '../subset2\\\\images\\\\video_0020\\\\00459.png', '../subset2\\\\images\\\\video_0020\\\\00474.png', '../subset2\\\\images\\\\video_0020\\\\00489.png', '../subset2\\\\images\\\\video_0020\\\\00504.png', '../subset2\\\\images\\\\video_0020\\\\00519.png', '../subset2\\\\images\\\\video_0020\\\\00534.png'], ['../subset2\\\\images\\\\video_0038\\\\00000.png', '../subset2\\\\images\\\\video_0038\\\\00015.png', '../subset2\\\\images\\\\video_0038\\\\00030.png', '../subset2\\\\images\\\\video_0038\\\\00045.png', '../subset2\\\\images\\\\video_0038\\\\00060.png', '../subset2\\\\images\\\\video_0038\\\\00075.png', '../subset2\\\\images\\\\video_0038\\\\00090.png', '../subset2\\\\images\\\\video_0038\\\\00105.png', '../subset2\\\\images\\\\video_0038\\\\00120.png', '../subset2\\\\images\\\\video_0038\\\\00135.png', '../subset2\\\\images\\\\video_0038\\\\00150.png', '../subset2\\\\images\\\\video_0038\\\\00165.png', '../subset2\\\\images\\\\video_0038\\\\00180.png', '../subset2\\\\images\\\\video_0038\\\\00195.png', '../subset2\\\\images\\\\video_0038\\\\00210.png', '../subset2\\\\images\\\\video_0038\\\\00225.png', '../subset2\\\\images\\\\video_0038\\\\00240.png', '../subset2\\\\images\\\\video_0038\\\\00255.png', '../subset2\\\\images\\\\video_0038\\\\00270.png', '../subset2\\\\images\\\\video_0038\\\\00285.png', '../subset2\\\\images\\\\video_0038\\\\00300.png', '../subset2\\\\images\\\\video_0038\\\\00315.png', '../subset2\\\\images\\\\video_0038\\\\00330.png', '../subset2\\\\images\\\\video_0038\\\\00345.png', '../subset2\\\\images\\\\video_0038\\\\00360.png', '../subset2\\\\images\\\\video_0038\\\\00375.png', '../subset2\\\\images\\\\video_0038\\\\00390.png', '../subset2\\\\images\\\\video_0038\\\\00405.png', '../subset2\\\\images\\\\video_0038\\\\00420.png', '../subset2\\\\images\\\\video_0038\\\\00435.png', '../subset2\\\\images\\\\video_0038\\\\00450.png', '../subset2\\\\images\\\\video_0038\\\\00465.png', '../subset2\\\\images\\\\video_0038\\\\00480.png', '../subset2\\\\images\\\\video_0038\\\\00495.png', '../subset2\\\\images\\\\video_0038\\\\00510.png', '../subset2\\\\images\\\\video_0038\\\\00525.png', '../subset2\\\\images\\\\video_0038\\\\00540.png', '../subset2\\\\images\\\\video_0038\\\\00555.png', '../subset2\\\\images\\\\video_0038\\\\00570.png'], ['../subset2\\\\images\\\\video_0038\\\\00000.png', '../subset2\\\\images\\\\video_0038\\\\00015.png', '../subset2\\\\images\\\\video_0038\\\\00030.png', '../subset2\\\\images\\\\video_0038\\\\00045.png', '../subset2\\\\images\\\\video_0038\\\\00060.png', '../subset2\\\\images\\\\video_0038\\\\00075.png', '../subset2\\\\images\\\\video_0038\\\\00090.png', '../subset2\\\\images\\\\video_0038\\\\00105.png', '../subset2\\\\images\\\\video_0038\\\\00120.png', '../subset2\\\\images\\\\video_0038\\\\00135.png', '../subset2\\\\images\\\\video_0038\\\\00150.png', '../subset2\\\\images\\\\video_0038\\\\00165.png', '../subset2\\\\images\\\\video_0038\\\\00180.png', '../subset2\\\\images\\\\video_0038\\\\00195.png', '../subset2\\\\images\\\\video_0038\\\\00210.png', '../subset2\\\\images\\\\video_0038\\\\00225.png', '../subset2\\\\images\\\\video_0038\\\\00240.png', '../subset2\\\\images\\\\video_0038\\\\00255.png', '../subset2\\\\images\\\\video_0038\\\\00270.png', '../subset2\\\\images\\\\video_0038\\\\00285.png', '../subset2\\\\images\\\\video_0038\\\\00300.png', '../subset2\\\\images\\\\video_0038\\\\00315.png', '../subset2\\\\images\\\\video_0038\\\\00330.png', '../subset2\\\\images\\\\video_0038\\\\00345.png', '../subset2\\\\images\\\\video_0038\\\\00360.png', '../subset2\\\\images\\\\video_0038\\\\00375.png', '../subset2\\\\images\\\\video_0038\\\\00390.png', '../subset2\\\\images\\\\video_0038\\\\00405.png', '../subset2\\\\images\\\\video_0038\\\\00420.png', '../subset2\\\\images\\\\video_0038\\\\00435.png', '../subset2\\\\images\\\\video_0038\\\\00450.png'], ['../subset2\\\\images\\\\video_0039\\\\00000.png', '../subset2\\\\images\\\\video_0039\\\\00015.png', '../subset2\\\\images\\\\video_0039\\\\00030.png', '../subset2\\\\images\\\\video_0039\\\\00045.png', '../subset2\\\\images\\\\video_0039\\\\00060.png', '../subset2\\\\images\\\\video_0039\\\\00075.png', '../subset2\\\\images\\\\video_0039\\\\00090.png', '../subset2\\\\images\\\\video_0039\\\\00105.png', '../subset2\\\\images\\\\video_0039\\\\00120.png', '../subset2\\\\images\\\\video_0039\\\\00135.png', '../subset2\\\\images\\\\video_0039\\\\00150.png', '../subset2\\\\images\\\\video_0039\\\\00165.png', '../subset2\\\\images\\\\video_0039\\\\00180.png', '../subset2\\\\images\\\\video_0039\\\\00195.png', '../subset2\\\\images\\\\video_0039\\\\00210.png', '../subset2\\\\images\\\\video_0039\\\\00225.png', '../subset2\\\\images\\\\video_0039\\\\00240.png', '../subset2\\\\images\\\\video_0039\\\\00255.png', '../subset2\\\\images\\\\video_0039\\\\00270.png', '../subset2\\\\images\\\\video_0039\\\\00285.png', '../subset2\\\\images\\\\video_0039\\\\00300.png', '../subset2\\\\images\\\\video_0039\\\\00315.png', '../subset2\\\\images\\\\video_0039\\\\00330.png', '../subset2\\\\images\\\\video_0039\\\\00345.png'], ['../subset2\\\\images\\\\video_0039\\\\00014.png', '../subset2\\\\images\\\\video_0039\\\\00029.png', '../subset2\\\\images\\\\video_0039\\\\00044.png', '../subset2\\\\images\\\\video_0039\\\\00059.png', '../subset2\\\\images\\\\video_0039\\\\00074.png', '../subset2\\\\images\\\\video_0039\\\\00089.png', '../subset2\\\\images\\\\video_0039\\\\00104.png', '../subset2\\\\images\\\\video_0039\\\\00119.png', '../subset2\\\\images\\\\video_0039\\\\00134.png', '../subset2\\\\images\\\\video_0039\\\\00149.png', '../subset2\\\\images\\\\video_0039\\\\00164.png', '../subset2\\\\images\\\\video_0039\\\\00179.png', '../subset2\\\\images\\\\video_0039\\\\00194.png', '../subset2\\\\images\\\\video_0039\\\\00209.png', '../subset2\\\\images\\\\video_0039\\\\00224.png', '../subset2\\\\images\\\\video_0039\\\\00239.png', '../subset2\\\\images\\\\video_0039\\\\00254.png'], ['../subset2\\\\images\\\\video_0039\\\\00026.png', '../subset2\\\\images\\\\video_0039\\\\00041.png', '../subset2\\\\images\\\\video_0039\\\\00056.png', '../subset2\\\\images\\\\video_0039\\\\00071.png', '../subset2\\\\images\\\\video_0039\\\\00086.png', '../subset2\\\\images\\\\video_0039\\\\00101.png', '../subset2\\\\images\\\\video_0039\\\\00116.png', '../subset2\\\\images\\\\video_0039\\\\00131.png', '../subset2\\\\images\\\\video_0039\\\\00146.png', '../subset2\\\\images\\\\video_0039\\\\00161.png', '../subset2\\\\images\\\\video_0039\\\\00176.png', '../subset2\\\\images\\\\video_0039\\\\00191.png', '../subset2\\\\images\\\\video_0039\\\\00206.png', '../subset2\\\\images\\\\video_0039\\\\00221.png', '../subset2\\\\images\\\\video_0039\\\\00236.png', '../subset2\\\\images\\\\video_0039\\\\00251.png', '../subset2\\\\images\\\\video_0039\\\\00266.png', '../subset2\\\\images\\\\video_0039\\\\00281.png', '../subset2\\\\images\\\\video_0039\\\\00296.png'], ['../subset2\\\\images\\\\video_0039\\\\00039.png', '../subset2\\\\images\\\\video_0039\\\\00054.png', '../subset2\\\\images\\\\video_0039\\\\00069.png', '../subset2\\\\images\\\\video_0039\\\\00084.png', '../subset2\\\\images\\\\video_0039\\\\00099.png', '../subset2\\\\images\\\\video_0039\\\\00114.png', '../subset2\\\\images\\\\video_0039\\\\00129.png', '../subset2\\\\images\\\\video_0039\\\\00144.png', '../subset2\\\\images\\\\video_0039\\\\00159.png', '../subset2\\\\images\\\\video_0039\\\\00174.png', '../subset2\\\\images\\\\video_0039\\\\00189.png', '../subset2\\\\images\\\\video_0039\\\\00204.png', '../subset2\\\\images\\\\video_0039\\\\00219.png', '../subset2\\\\images\\\\video_0039\\\\00234.png', '../subset2\\\\images\\\\video_0039\\\\00249.png', '../subset2\\\\images\\\\video_0039\\\\00264.png', '../subset2\\\\images\\\\video_0039\\\\00279.png', '../subset2\\\\images\\\\video_0039\\\\00294.png', '../subset2\\\\images\\\\video_0039\\\\00309.png', '../subset2\\\\images\\\\video_0039\\\\00324.png', '../subset2\\\\images\\\\video_0039\\\\00339.png', '../subset2\\\\images\\\\video_0039\\\\00354.png'], ['../subset2\\\\images\\\\video_0039\\\\00059.png', '../subset2\\\\images\\\\video_0039\\\\00074.png', '../subset2\\\\images\\\\video_0039\\\\00089.png', '../subset2\\\\images\\\\video_0039\\\\00104.png', '../subset2\\\\images\\\\video_0039\\\\00119.png', '../subset2\\\\images\\\\video_0039\\\\00134.png', '../subset2\\\\images\\\\video_0039\\\\00149.png', '../subset2\\\\images\\\\video_0039\\\\00164.png', '../subset2\\\\images\\\\video_0039\\\\00179.png', '../subset2\\\\images\\\\video_0039\\\\00194.png', '../subset2\\\\images\\\\video_0039\\\\00209.png', '../subset2\\\\images\\\\video_0039\\\\00224.png', '../subset2\\\\images\\\\video_0039\\\\00239.png', '../subset2\\\\images\\\\video_0039\\\\00254.png', '../subset2\\\\images\\\\video_0039\\\\00269.png', '../subset2\\\\images\\\\video_0039\\\\00284.png', '../subset2\\\\images\\\\video_0039\\\\00299.png', '../subset2\\\\images\\\\video_0039\\\\00314.png', '../subset2\\\\images\\\\video_0039\\\\00329.png', '../subset2\\\\images\\\\video_0039\\\\00344.png'], ['../subset2\\\\images\\\\video_0039\\\\00000.png', '../subset2\\\\images\\\\video_0039\\\\00015.png', '../subset2\\\\images\\\\video_0039\\\\00030.png', '../subset2\\\\images\\\\video_0039\\\\00045.png', '../subset2\\\\images\\\\video_0039\\\\00060.png', '../subset2\\\\images\\\\video_0039\\\\00075.png', '../subset2\\\\images\\\\video_0039\\\\00090.png', '../subset2\\\\images\\\\video_0039\\\\00105.png', '../subset2\\\\images\\\\video_0039\\\\00120.png', '../subset2\\\\images\\\\video_0039\\\\00135.png', '../subset2\\\\images\\\\video_0039\\\\00150.png', '../subset2\\\\images\\\\video_0039\\\\00165.png', '../subset2\\\\images\\\\video_0039\\\\00180.png', '../subset2\\\\images\\\\video_0039\\\\00195.png', '../subset2\\\\images\\\\video_0039\\\\00210.png', '../subset2\\\\images\\\\video_0039\\\\00225.png', '../subset2\\\\images\\\\video_0039\\\\00240.png', '../subset2\\\\images\\\\video_0039\\\\00255.png', '../subset2\\\\images\\\\video_0039\\\\00270.png', '../subset2\\\\images\\\\video_0039\\\\00285.png', '../subset2\\\\images\\\\video_0039\\\\00300.png', '../subset2\\\\images\\\\video_0039\\\\00315.png', '../subset2\\\\images\\\\video_0039\\\\00330.png', '../subset2\\\\images\\\\video_0039\\\\00345.png'], ['../subset2\\\\images\\\\video_0049\\\\00034.png', '../subset2\\\\images\\\\video_0049\\\\00049.png', '../subset2\\\\images\\\\video_0049\\\\00064.png', '../subset2\\\\images\\\\video_0049\\\\00079.png', '../subset2\\\\images\\\\video_0049\\\\00094.png', '../subset2\\\\images\\\\video_0049\\\\00109.png', '../subset2\\\\images\\\\video_0049\\\\00124.png', '../subset2\\\\images\\\\video_0049\\\\00139.png', '../subset2\\\\images\\\\video_0049\\\\00154.png', '../subset2\\\\images\\\\video_0049\\\\00169.png', '../subset2\\\\images\\\\video_0049\\\\00184.png', '../subset2\\\\images\\\\video_0049\\\\00199.png', '../subset2\\\\images\\\\video_0049\\\\00214.png', '../subset2\\\\images\\\\video_0049\\\\00229.png', '../subset2\\\\images\\\\video_0049\\\\00244.png', '../subset2\\\\images\\\\video_0049\\\\00259.png'], ['../subset2\\\\images\\\\video_0050\\\\00000.png', '../subset2\\\\images\\\\video_0050\\\\00015.png', '../subset2\\\\images\\\\video_0050\\\\00030.png', '../subset2\\\\images\\\\video_0050\\\\00045.png', '../subset2\\\\images\\\\video_0050\\\\00060.png', '../subset2\\\\images\\\\video_0050\\\\00075.png', '../subset2\\\\images\\\\video_0050\\\\00090.png', '../subset2\\\\images\\\\video_0050\\\\00105.png', '../subset2\\\\images\\\\video_0050\\\\00120.png', '../subset2\\\\images\\\\video_0050\\\\00135.png', '../subset2\\\\images\\\\video_0050\\\\00150.png', '../subset2\\\\images\\\\video_0050\\\\00165.png', '../subset2\\\\images\\\\video_0050\\\\00180.png', '../subset2\\\\images\\\\video_0050\\\\00195.png', '../subset2\\\\images\\\\video_0050\\\\00210.png', '../subset2\\\\images\\\\video_0050\\\\00225.png', '../subset2\\\\images\\\\video_0050\\\\00240.png', '../subset2\\\\images\\\\video_0050\\\\00255.png'], ['../subset2\\\\images\\\\video_0052\\\\00000.png', '../subset2\\\\images\\\\video_0052\\\\00015.png', '../subset2\\\\images\\\\video_0052\\\\00030.png', '../subset2\\\\images\\\\video_0052\\\\00045.png', '../subset2\\\\images\\\\video_0052\\\\00060.png', '../subset2\\\\images\\\\video_0052\\\\00075.png', '../subset2\\\\images\\\\video_0052\\\\00090.png', '../subset2\\\\images\\\\video_0052\\\\00105.png', '../subset2\\\\images\\\\video_0052\\\\00120.png', '../subset2\\\\images\\\\video_0052\\\\00135.png', '../subset2\\\\images\\\\video_0052\\\\00150.png', '../subset2\\\\images\\\\video_0052\\\\00165.png', '../subset2\\\\images\\\\video_0052\\\\00180.png', '../subset2\\\\images\\\\video_0052\\\\00195.png', '../subset2\\\\images\\\\video_0052\\\\00210.png', '../subset2\\\\images\\\\video_0052\\\\00225.png', '../subset2\\\\images\\\\video_0052\\\\00240.png', '../subset2\\\\images\\\\video_0052\\\\00255.png', '../subset2\\\\images\\\\video_0052\\\\00270.png', '../subset2\\\\images\\\\video_0052\\\\00285.png'], ['../subset2\\\\images\\\\video_0052\\\\00000.png', '../subset2\\\\images\\\\video_0052\\\\00015.png', '../subset2\\\\images\\\\video_0052\\\\00030.png', '../subset2\\\\images\\\\video_0052\\\\00045.png', '../subset2\\\\images\\\\video_0052\\\\00060.png', '../subset2\\\\images\\\\video_0052\\\\00075.png', '../subset2\\\\images\\\\video_0052\\\\00090.png', '../subset2\\\\images\\\\video_0052\\\\00105.png', '../subset2\\\\images\\\\video_0052\\\\00120.png', '../subset2\\\\images\\\\video_0052\\\\00135.png', '../subset2\\\\images\\\\video_0052\\\\00150.png', '../subset2\\\\images\\\\video_0052\\\\00165.png', '../subset2\\\\images\\\\video_0052\\\\00180.png', '../subset2\\\\images\\\\video_0052\\\\00195.png', '../subset2\\\\images\\\\video_0052\\\\00210.png', '../subset2\\\\images\\\\video_0052\\\\00225.png', '../subset2\\\\images\\\\video_0052\\\\00240.png', '../subset2\\\\images\\\\video_0052\\\\00255.png', '../subset2\\\\images\\\\video_0052\\\\00270.png', '../subset2\\\\images\\\\video_0052\\\\00285.png'], ['../subset2\\\\images\\\\video_0056\\\\00000.png', '../subset2\\\\images\\\\video_0056\\\\00015.png', '../subset2\\\\images\\\\video_0056\\\\00030.png', '../subset2\\\\images\\\\video_0056\\\\00045.png', '../subset2\\\\images\\\\video_0056\\\\00060.png', '../subset2\\\\images\\\\video_0056\\\\00075.png', '../subset2\\\\images\\\\video_0056\\\\00090.png', '../subset2\\\\images\\\\video_0056\\\\00105.png', '../subset2\\\\images\\\\video_0056\\\\00120.png', '../subset2\\\\images\\\\video_0056\\\\00135.png', '../subset2\\\\images\\\\video_0056\\\\00150.png', '../subset2\\\\images\\\\video_0056\\\\00165.png', '../subset2\\\\images\\\\video_0056\\\\00180.png', '../subset2\\\\images\\\\video_0056\\\\00195.png', '../subset2\\\\images\\\\video_0056\\\\00210.png', '../subset2\\\\images\\\\video_0056\\\\00225.png', '../subset2\\\\images\\\\video_0056\\\\00240.png'], ['../subset2\\\\images\\\\video_0056\\\\00000.png', '../subset2\\\\images\\\\video_0056\\\\00015.png', '../subset2\\\\images\\\\video_0056\\\\00030.png', '../subset2\\\\images\\\\video_0056\\\\00045.png', '../subset2\\\\images\\\\video_0056\\\\00060.png', '../subset2\\\\images\\\\video_0056\\\\00075.png', '../subset2\\\\images\\\\video_0056\\\\00090.png', '../subset2\\\\images\\\\video_0056\\\\00105.png', '../subset2\\\\images\\\\video_0056\\\\00120.png', '../subset2\\\\images\\\\video_0056\\\\00135.png', '../subset2\\\\images\\\\video_0056\\\\00150.png', '../subset2\\\\images\\\\video_0056\\\\00165.png', '../subset2\\\\images\\\\video_0056\\\\00180.png', '../subset2\\\\images\\\\video_0056\\\\00195.png', '../subset2\\\\images\\\\video_0056\\\\00210.png', '../subset2\\\\images\\\\video_0056\\\\00225.png', '../subset2\\\\images\\\\video_0056\\\\00240.png', '../subset2\\\\images\\\\video_0056\\\\00255.png'], ['../subset2\\\\images\\\\video_0095\\\\00000.png', '../subset2\\\\images\\\\video_0095\\\\00015.png', '../subset2\\\\images\\\\video_0095\\\\00030.png', '../subset2\\\\images\\\\video_0095\\\\00045.png', '../subset2\\\\images\\\\video_0095\\\\00060.png', '../subset2\\\\images\\\\video_0095\\\\00075.png', '../subset2\\\\images\\\\video_0095\\\\00090.png', '../subset2\\\\images\\\\video_0095\\\\00105.png', '../subset2\\\\images\\\\video_0095\\\\00120.png', '../subset2\\\\images\\\\video_0095\\\\00135.png', '../subset2\\\\images\\\\video_0095\\\\00150.png', '../subset2\\\\images\\\\video_0095\\\\00165.png', '../subset2\\\\images\\\\video_0095\\\\00180.png', '../subset2\\\\images\\\\video_0095\\\\00195.png', '../subset2\\\\images\\\\video_0095\\\\00210.png', '../subset2\\\\images\\\\video_0095\\\\00225.png'], ['../subset2\\\\images\\\\video_0134\\\\00349.png', '../subset2\\\\images\\\\video_0134\\\\00364.png', '../subset2\\\\images\\\\video_0134\\\\00379.png', '../subset2\\\\images\\\\video_0134\\\\00394.png', '../subset2\\\\images\\\\video_0134\\\\00409.png', '../subset2\\\\images\\\\video_0134\\\\00424.png', '../subset2\\\\images\\\\video_0134\\\\00439.png', '../subset2\\\\images\\\\video_0134\\\\00454.png', '../subset2\\\\images\\\\video_0134\\\\00469.png', '../subset2\\\\images\\\\video_0134\\\\00484.png', '../subset2\\\\images\\\\video_0134\\\\00499.png', '../subset2\\\\images\\\\video_0134\\\\00514.png', '../subset2\\\\images\\\\video_0134\\\\00529.png', '../subset2\\\\images\\\\video_0134\\\\00544.png', '../subset2\\\\images\\\\video_0134\\\\00559.png', '../subset2\\\\images\\\\video_0134\\\\00574.png', '../subset2\\\\images\\\\video_0134\\\\00589.png'], ['../subset2\\\\images\\\\video_0134\\\\00080.png', '../subset2\\\\images\\\\video_0134\\\\00095.png', '../subset2\\\\images\\\\video_0134\\\\00110.png', '../subset2\\\\images\\\\video_0134\\\\00125.png', '../subset2\\\\images\\\\video_0134\\\\00140.png', '../subset2\\\\images\\\\video_0134\\\\00155.png', '../subset2\\\\images\\\\video_0134\\\\00170.png', '../subset2\\\\images\\\\video_0134\\\\00185.png', '../subset2\\\\images\\\\video_0134\\\\00200.png', '../subset2\\\\images\\\\video_0134\\\\00215.png', '../subset2\\\\images\\\\video_0134\\\\00230.png', '../subset2\\\\images\\\\video_0134\\\\00245.png', '../subset2\\\\images\\\\video_0134\\\\00260.png', '../subset2\\\\images\\\\video_0134\\\\00275.png', '../subset2\\\\images\\\\video_0134\\\\00290.png', '../subset2\\\\images\\\\video_0134\\\\00305.png', '../subset2\\\\images\\\\video_0134\\\\00320.png', '../subset2\\\\images\\\\video_0134\\\\00335.png', '../subset2\\\\images\\\\video_0134\\\\00350.png', '../subset2\\\\images\\\\video_0134\\\\00365.png', '../subset2\\\\images\\\\video_0134\\\\00380.png', '../subset2\\\\images\\\\video_0134\\\\00395.png', '../subset2\\\\images\\\\video_0134\\\\00410.png', '../subset2\\\\images\\\\video_0134\\\\00425.png', '../subset2\\\\images\\\\video_0134\\\\00440.png', '../subset2\\\\images\\\\video_0134\\\\00455.png', '../subset2\\\\images\\\\video_0134\\\\00470.png', '../subset2\\\\images\\\\video_0134\\\\00485.png', '../subset2\\\\images\\\\video_0134\\\\00500.png', '../subset2\\\\images\\\\video_0134\\\\00515.png', '../subset2\\\\images\\\\video_0134\\\\00530.png', '../subset2\\\\images\\\\video_0134\\\\00545.png', '../subset2\\\\images\\\\video_0134\\\\00560.png', '../subset2\\\\images\\\\video_0134\\\\00575.png', '../subset2\\\\images\\\\video_0134\\\\00590.png'], ['../subset2\\\\images\\\\video_0134\\\\00013.png', '../subset2\\\\images\\\\video_0134\\\\00028.png', '../subset2\\\\images\\\\video_0134\\\\00043.png', '../subset2\\\\images\\\\video_0134\\\\00058.png', '../subset2\\\\images\\\\video_0134\\\\00073.png', '../subset2\\\\images\\\\video_0134\\\\00088.png', '../subset2\\\\images\\\\video_0134\\\\00103.png', '../subset2\\\\images\\\\video_0134\\\\00118.png', '../subset2\\\\images\\\\video_0134\\\\00133.png', '../subset2\\\\images\\\\video_0134\\\\00148.png', '../subset2\\\\images\\\\video_0134\\\\00163.png', '../subset2\\\\images\\\\video_0134\\\\00178.png', '../subset2\\\\images\\\\video_0134\\\\00193.png', '../subset2\\\\images\\\\video_0134\\\\00208.png', '../subset2\\\\images\\\\video_0134\\\\00223.png', '../subset2\\\\images\\\\video_0134\\\\00238.png', '../subset2\\\\images\\\\video_0134\\\\00253.png', '../subset2\\\\images\\\\video_0134\\\\00268.png'], ['../subset2\\\\images\\\\video_0134\\\\00018.png', '../subset2\\\\images\\\\video_0134\\\\00033.png', '../subset2\\\\images\\\\video_0134\\\\00048.png', '../subset2\\\\images\\\\video_0134\\\\00063.png', '../subset2\\\\images\\\\video_0134\\\\00078.png', '../subset2\\\\images\\\\video_0134\\\\00093.png', '../subset2\\\\images\\\\video_0134\\\\00108.png', '../subset2\\\\images\\\\video_0134\\\\00123.png', '../subset2\\\\images\\\\video_0134\\\\00138.png', '../subset2\\\\images\\\\video_0134\\\\00153.png', '../subset2\\\\images\\\\video_0134\\\\00168.png', '../subset2\\\\images\\\\video_0134\\\\00183.png', '../subset2\\\\images\\\\video_0134\\\\00198.png', '../subset2\\\\images\\\\video_0134\\\\00213.png', '../subset2\\\\images\\\\video_0134\\\\00228.png', '../subset2\\\\images\\\\video_0134\\\\00243.png', '../subset2\\\\images\\\\video_0134\\\\00258.png', '../subset2\\\\images\\\\video_0134\\\\00273.png', '../subset2\\\\images\\\\video_0134\\\\00288.png', '../subset2\\\\images\\\\video_0134\\\\00303.png', '../subset2\\\\images\\\\video_0134\\\\00318.png', '../subset2\\\\images\\\\video_0134\\\\00333.png', '../subset2\\\\images\\\\video_0134\\\\00348.png', '../subset2\\\\images\\\\video_0134\\\\00363.png', '../subset2\\\\images\\\\video_0134\\\\00378.png', '../subset2\\\\images\\\\video_0134\\\\00393.png', '../subset2\\\\images\\\\video_0134\\\\00408.png', '../subset2\\\\images\\\\video_0134\\\\00423.png', '../subset2\\\\images\\\\video_0134\\\\00438.png', '../subset2\\\\images\\\\video_0134\\\\00453.png', '../subset2\\\\images\\\\video_0134\\\\00468.png', '../subset2\\\\images\\\\video_0134\\\\00483.png', '../subset2\\\\images\\\\video_0134\\\\00498.png', '../subset2\\\\images\\\\video_0134\\\\00513.png', '../subset2\\\\images\\\\video_0134\\\\00528.png', '../subset2\\\\images\\\\video_0134\\\\00543.png', '../subset2\\\\images\\\\video_0134\\\\00558.png', '../subset2\\\\images\\\\video_0134\\\\00573.png', '../subset2\\\\images\\\\video_0134\\\\00588.png'], ['../subset2\\\\images\\\\video_0136\\\\00000.png', '../subset2\\\\images\\\\video_0136\\\\00015.png', '../subset2\\\\images\\\\video_0136\\\\00030.png', '../subset2\\\\images\\\\video_0136\\\\00045.png', '../subset2\\\\images\\\\video_0136\\\\00060.png', '../subset2\\\\images\\\\video_0136\\\\00075.png', '../subset2\\\\images\\\\video_0136\\\\00090.png', '../subset2\\\\images\\\\video_0136\\\\00105.png', '../subset2\\\\images\\\\video_0136\\\\00120.png', '../subset2\\\\images\\\\video_0136\\\\00135.png', '../subset2\\\\images\\\\video_0136\\\\00150.png', '../subset2\\\\images\\\\video_0136\\\\00165.png', '../subset2\\\\images\\\\video_0136\\\\00180.png', '../subset2\\\\images\\\\video_0136\\\\00195.png', '../subset2\\\\images\\\\video_0136\\\\00210.png', '../subset2\\\\images\\\\video_0136\\\\00225.png'], ['../subset2\\\\images\\\\video_0136\\\\00000.png', '../subset2\\\\images\\\\video_0136\\\\00015.png', '../subset2\\\\images\\\\video_0136\\\\00030.png', '../subset2\\\\images\\\\video_0136\\\\00045.png', '../subset2\\\\images\\\\video_0136\\\\00060.png', '../subset2\\\\images\\\\video_0136\\\\00075.png', '../subset2\\\\images\\\\video_0136\\\\00090.png', '../subset2\\\\images\\\\video_0136\\\\00105.png', '../subset2\\\\images\\\\video_0136\\\\00120.png', '../subset2\\\\images\\\\video_0136\\\\00135.png', '../subset2\\\\images\\\\video_0136\\\\00150.png', '../subset2\\\\images\\\\video_0136\\\\00165.png', '../subset2\\\\images\\\\video_0136\\\\00180.png', '../subset2\\\\images\\\\video_0136\\\\00195.png', '../subset2\\\\images\\\\video_0136\\\\00210.png', '../subset2\\\\images\\\\video_0136\\\\00225.png'], ['../subset2\\\\images\\\\video_0138\\\\00008.png', '../subset2\\\\images\\\\video_0138\\\\00023.png', '../subset2\\\\images\\\\video_0138\\\\00038.png', '../subset2\\\\images\\\\video_0138\\\\00053.png', '../subset2\\\\images\\\\video_0138\\\\00068.png', '../subset2\\\\images\\\\video_0138\\\\00083.png', '../subset2\\\\images\\\\video_0138\\\\00098.png', '../subset2\\\\images\\\\video_0138\\\\00113.png', '../subset2\\\\images\\\\video_0138\\\\00128.png', '../subset2\\\\images\\\\video_0138\\\\00143.png', '../subset2\\\\images\\\\video_0138\\\\00158.png', '../subset2\\\\images\\\\video_0138\\\\00173.png', '../subset2\\\\images\\\\video_0138\\\\00188.png', '../subset2\\\\images\\\\video_0138\\\\00203.png', '../subset2\\\\images\\\\video_0138\\\\00218.png', '../subset2\\\\images\\\\video_0138\\\\00233.png', '../subset2\\\\images\\\\video_0138\\\\00248.png', '../subset2\\\\images\\\\video_0138\\\\00263.png', '../subset2\\\\images\\\\video_0138\\\\00278.png', '../subset2\\\\images\\\\video_0138\\\\00293.png'], ['../subset2\\\\images\\\\video_0138\\\\00000.png', '../subset2\\\\images\\\\video_0138\\\\00015.png', '../subset2\\\\images\\\\video_0138\\\\00030.png', '../subset2\\\\images\\\\video_0138\\\\00045.png', '../subset2\\\\images\\\\video_0138\\\\00060.png', '../subset2\\\\images\\\\video_0138\\\\00075.png', '../subset2\\\\images\\\\video_0138\\\\00090.png', '../subset2\\\\images\\\\video_0138\\\\00105.png', '../subset2\\\\images\\\\video_0138\\\\00120.png', '../subset2\\\\images\\\\video_0138\\\\00135.png', '../subset2\\\\images\\\\video_0138\\\\00150.png', '../subset2\\\\images\\\\video_0138\\\\00165.png', '../subset2\\\\images\\\\video_0138\\\\00180.png', '../subset2\\\\images\\\\video_0138\\\\00195.png', '../subset2\\\\images\\\\video_0138\\\\00210.png', '../subset2\\\\images\\\\video_0138\\\\00225.png', '../subset2\\\\images\\\\video_0138\\\\00240.png', '../subset2\\\\images\\\\video_0138\\\\00255.png'], ['../subset2\\\\images\\\\video_0138\\\\00000.png', '../subset2\\\\images\\\\video_0138\\\\00015.png', '../subset2\\\\images\\\\video_0138\\\\00030.png', '../subset2\\\\images\\\\video_0138\\\\00045.png', '../subset2\\\\images\\\\video_0138\\\\00060.png', '../subset2\\\\images\\\\video_0138\\\\00075.png', '../subset2\\\\images\\\\video_0138\\\\00090.png', '../subset2\\\\images\\\\video_0138\\\\00105.png', '../subset2\\\\images\\\\video_0138\\\\00120.png', '../subset2\\\\images\\\\video_0138\\\\00135.png', '../subset2\\\\images\\\\video_0138\\\\00150.png', '../subset2\\\\images\\\\video_0138\\\\00165.png', '../subset2\\\\images\\\\video_0138\\\\00180.png', '../subset2\\\\images\\\\video_0138\\\\00195.png', '../subset2\\\\images\\\\video_0138\\\\00210.png', '../subset2\\\\images\\\\video_0138\\\\00225.png', '../subset2\\\\images\\\\video_0138\\\\00240.png', '../subset2\\\\images\\\\video_0138\\\\00255.png'], ['../subset2\\\\images\\\\video_0138\\\\00012.png', '../subset2\\\\images\\\\video_0138\\\\00027.png', '../subset2\\\\images\\\\video_0138\\\\00042.png', '../subset2\\\\images\\\\video_0138\\\\00057.png', '../subset2\\\\images\\\\video_0138\\\\00072.png', '../subset2\\\\images\\\\video_0138\\\\00087.png', '../subset2\\\\images\\\\video_0138\\\\00102.png', '../subset2\\\\images\\\\video_0138\\\\00117.png', '../subset2\\\\images\\\\video_0138\\\\00132.png', '../subset2\\\\images\\\\video_0138\\\\00147.png', '../subset2\\\\images\\\\video_0138\\\\00162.png', '../subset2\\\\images\\\\video_0138\\\\00177.png', '../subset2\\\\images\\\\video_0138\\\\00192.png', '../subset2\\\\images\\\\video_0138\\\\00207.png', '../subset2\\\\images\\\\video_0138\\\\00222.png', '../subset2\\\\images\\\\video_0138\\\\00237.png', '../subset2\\\\images\\\\video_0138\\\\00252.png', '../subset2\\\\images\\\\video_0138\\\\00267.png', '../subset2\\\\images\\\\video_0138\\\\00282.png', '../subset2\\\\images\\\\video_0138\\\\00297.png'], ['../subset2\\\\images\\\\video_0138\\\\00000.png', '../subset2\\\\images\\\\video_0138\\\\00015.png', '../subset2\\\\images\\\\video_0138\\\\00030.png', '../subset2\\\\images\\\\video_0138\\\\00045.png', '../subset2\\\\images\\\\video_0138\\\\00060.png', '../subset2\\\\images\\\\video_0138\\\\00075.png', '../subset2\\\\images\\\\video_0138\\\\00090.png', '../subset2\\\\images\\\\video_0138\\\\00105.png', '../subset2\\\\images\\\\video_0138\\\\00120.png', '../subset2\\\\images\\\\video_0138\\\\00135.png', '../subset2\\\\images\\\\video_0138\\\\00150.png', '../subset2\\\\images\\\\video_0138\\\\00165.png', '../subset2\\\\images\\\\video_0138\\\\00180.png', '../subset2\\\\images\\\\video_0138\\\\00195.png', '../subset2\\\\images\\\\video_0138\\\\00210.png', '../subset2\\\\images\\\\video_0138\\\\00225.png', '../subset2\\\\images\\\\video_0138\\\\00240.png', '../subset2\\\\images\\\\video_0138\\\\00255.png'], ['../subset2\\\\images\\\\video_0138\\\\00005.png', '../subset2\\\\images\\\\video_0138\\\\00020.png', '../subset2\\\\images\\\\video_0138\\\\00035.png', '../subset2\\\\images\\\\video_0138\\\\00050.png', '../subset2\\\\images\\\\video_0138\\\\00065.png', '../subset2\\\\images\\\\video_0138\\\\00080.png', '../subset2\\\\images\\\\video_0138\\\\00095.png', '../subset2\\\\images\\\\video_0138\\\\00110.png', '../subset2\\\\images\\\\video_0138\\\\00125.png', '../subset2\\\\images\\\\video_0138\\\\00140.png', '../subset2\\\\images\\\\video_0138\\\\00155.png', '../subset2\\\\images\\\\video_0138\\\\00170.png', '../subset2\\\\images\\\\video_0138\\\\00185.png', '../subset2\\\\images\\\\video_0138\\\\00200.png', '../subset2\\\\images\\\\video_0138\\\\00215.png', '../subset2\\\\images\\\\video_0138\\\\00230.png', '../subset2\\\\images\\\\video_0138\\\\00245.png'], ['../subset2\\\\images\\\\video_0138\\\\00000.png', '../subset2\\\\images\\\\video_0138\\\\00015.png', '../subset2\\\\images\\\\video_0138\\\\00030.png', '../subset2\\\\images\\\\video_0138\\\\00045.png', '../subset2\\\\images\\\\video_0138\\\\00060.png', '../subset2\\\\images\\\\video_0138\\\\00075.png', '../subset2\\\\images\\\\video_0138\\\\00090.png', '../subset2\\\\images\\\\video_0138\\\\00105.png', '../subset2\\\\images\\\\video_0138\\\\00120.png', '../subset2\\\\images\\\\video_0138\\\\00135.png', '../subset2\\\\images\\\\video_0138\\\\00150.png', '../subset2\\\\images\\\\video_0138\\\\00165.png', '../subset2\\\\images\\\\video_0138\\\\00180.png', '../subset2\\\\images\\\\video_0138\\\\00195.png', '../subset2\\\\images\\\\video_0138\\\\00210.png', '../subset2\\\\images\\\\video_0138\\\\00225.png', '../subset2\\\\images\\\\video_0138\\\\00240.png', '../subset2\\\\images\\\\video_0138\\\\00255.png', '../subset2\\\\images\\\\video_0138\\\\00270.png', '../subset2\\\\images\\\\video_0138\\\\00285.png'], ['../subset2\\\\images\\\\video_0147\\\\00000.png', '../subset2\\\\images\\\\video_0147\\\\00015.png', '../subset2\\\\images\\\\video_0147\\\\00030.png', '../subset2\\\\images\\\\video_0147\\\\00045.png', '../subset2\\\\images\\\\video_0147\\\\00060.png', '../subset2\\\\images\\\\video_0147\\\\00075.png', '../subset2\\\\images\\\\video_0147\\\\00090.png', '../subset2\\\\images\\\\video_0147\\\\00105.png', '../subset2\\\\images\\\\video_0147\\\\00120.png', '../subset2\\\\images\\\\video_0147\\\\00135.png', '../subset2\\\\images\\\\video_0147\\\\00150.png', '../subset2\\\\images\\\\video_0147\\\\00165.png', '../subset2\\\\images\\\\video_0147\\\\00180.png', '../subset2\\\\images\\\\video_0147\\\\00195.png', '../subset2\\\\images\\\\video_0147\\\\00210.png', '../subset2\\\\images\\\\video_0147\\\\00225.png', '../subset2\\\\images\\\\video_0147\\\\00240.png', '../subset2\\\\images\\\\video_0147\\\\00255.png', '../subset2\\\\images\\\\video_0147\\\\00270.png', '../subset2\\\\images\\\\video_0147\\\\00285.png', '../subset2\\\\images\\\\video_0147\\\\00300.png', '../subset2\\\\images\\\\video_0147\\\\00315.png', '../subset2\\\\images\\\\video_0147\\\\00330.png', '../subset2\\\\images\\\\video_0147\\\\00345.png'], ['../subset2\\\\images\\\\video_0147\\\\00000.png', '../subset2\\\\images\\\\video_0147\\\\00015.png', '../subset2\\\\images\\\\video_0147\\\\00030.png', '../subset2\\\\images\\\\video_0147\\\\00045.png', '../subset2\\\\images\\\\video_0147\\\\00060.png', '../subset2\\\\images\\\\video_0147\\\\00075.png', '../subset2\\\\images\\\\video_0147\\\\00090.png', '../subset2\\\\images\\\\video_0147\\\\00105.png', '../subset2\\\\images\\\\video_0147\\\\00120.png', '../subset2\\\\images\\\\video_0147\\\\00135.png', '../subset2\\\\images\\\\video_0147\\\\00150.png', '../subset2\\\\images\\\\video_0147\\\\00165.png', '../subset2\\\\images\\\\video_0147\\\\00180.png', '../subset2\\\\images\\\\video_0147\\\\00195.png', '../subset2\\\\images\\\\video_0147\\\\00210.png', '../subset2\\\\images\\\\video_0147\\\\00225.png', '../subset2\\\\images\\\\video_0147\\\\00240.png', '../subset2\\\\images\\\\video_0147\\\\00255.png', '../subset2\\\\images\\\\video_0147\\\\00270.png', '../subset2\\\\images\\\\video_0147\\\\00285.png', '../subset2\\\\images\\\\video_0147\\\\00300.png', '../subset2\\\\images\\\\video_0147\\\\00315.png', '../subset2\\\\images\\\\video_0147\\\\00330.png', '../subset2\\\\images\\\\video_0147\\\\00345.png'], ['../subset2\\\\images\\\\video_0154\\\\00000.png', '../subset2\\\\images\\\\video_0154\\\\00015.png', '../subset2\\\\images\\\\video_0154\\\\00030.png', '../subset2\\\\images\\\\video_0154\\\\00045.png', '../subset2\\\\images\\\\video_0154\\\\00060.png', '../subset2\\\\images\\\\video_0154\\\\00075.png', '../subset2\\\\images\\\\video_0154\\\\00090.png', '../subset2\\\\images\\\\video_0154\\\\00105.png', '../subset2\\\\images\\\\video_0154\\\\00120.png', '../subset2\\\\images\\\\video_0154\\\\00135.png', '../subset2\\\\images\\\\video_0154\\\\00150.png', '../subset2\\\\images\\\\video_0154\\\\00165.png', '../subset2\\\\images\\\\video_0154\\\\00180.png', '../subset2\\\\images\\\\video_0154\\\\00195.png', '../subset2\\\\images\\\\video_0154\\\\00210.png', '../subset2\\\\images\\\\video_0154\\\\00225.png', '../subset2\\\\images\\\\video_0154\\\\00240.png', '../subset2\\\\images\\\\video_0154\\\\00255.png', '../subset2\\\\images\\\\video_0154\\\\00270.png', '../subset2\\\\images\\\\video_0154\\\\00285.png', '../subset2\\\\images\\\\video_0154\\\\00300.png', '../subset2\\\\images\\\\video_0154\\\\00315.png'], ['../subset2\\\\images\\\\video_0154\\\\00000.png', '../subset2\\\\images\\\\video_0154\\\\00015.png', '../subset2\\\\images\\\\video_0154\\\\00030.png', '../subset2\\\\images\\\\video_0154\\\\00045.png', '../subset2\\\\images\\\\video_0154\\\\00060.png', '../subset2\\\\images\\\\video_0154\\\\00075.png', '../subset2\\\\images\\\\video_0154\\\\00191.png', '../subset2\\\\images\\\\video_0154\\\\00206.png', '../subset2\\\\images\\\\video_0154\\\\00221.png', '../subset2\\\\images\\\\video_0154\\\\00236.png', '../subset2\\\\images\\\\video_0154\\\\00251.png', '../subset2\\\\images\\\\video_0154\\\\00266.png', '../subset2\\\\images\\\\video_0154\\\\00281.png', '../subset2\\\\images\\\\video_0154\\\\00296.png', '../subset2\\\\images\\\\video_0154\\\\00311.png', '../subset2\\\\images\\\\video_0154\\\\00326.png'], ['../subset2\\\\images\\\\video_0154\\\\00000.png', '../subset2\\\\images\\\\video_0154\\\\00015.png', '../subset2\\\\images\\\\video_0154\\\\00030.png', '../subset2\\\\images\\\\video_0154\\\\00045.png', '../subset2\\\\images\\\\video_0154\\\\00060.png', '../subset2\\\\images\\\\video_0154\\\\00075.png', '../subset2\\\\images\\\\video_0154\\\\00090.png', '../subset2\\\\images\\\\video_0154\\\\00105.png', '../subset2\\\\images\\\\video_0154\\\\00120.png', '../subset2\\\\images\\\\video_0154\\\\00135.png', '../subset2\\\\images\\\\video_0154\\\\00150.png', '../subset2\\\\images\\\\video_0154\\\\00165.png', '../subset2\\\\images\\\\video_0154\\\\00180.png', '../subset2\\\\images\\\\video_0154\\\\00195.png', '../subset2\\\\images\\\\video_0154\\\\00210.png', '../subset2\\\\images\\\\video_0154\\\\00225.png', '../subset2\\\\images\\\\video_0154\\\\00240.png', '../subset2\\\\images\\\\video_0154\\\\00255.png', '../subset2\\\\images\\\\video_0154\\\\00270.png', '../subset2\\\\images\\\\video_0154\\\\00285.png', '../subset2\\\\images\\\\video_0154\\\\00300.png', '../subset2\\\\images\\\\video_0154\\\\00315.png'], ['../subset2\\\\images\\\\video_0154\\\\00000.png', '../subset2\\\\images\\\\video_0154\\\\00015.png', '../subset2\\\\images\\\\video_0154\\\\00030.png', '../subset2\\\\images\\\\video_0154\\\\00045.png', '../subset2\\\\images\\\\video_0154\\\\00060.png', '../subset2\\\\images\\\\video_0154\\\\00075.png', '../subset2\\\\images\\\\video_0154\\\\00090.png', '../subset2\\\\images\\\\video_0154\\\\00105.png', '../subset2\\\\images\\\\video_0154\\\\00120.png', '../subset2\\\\images\\\\video_0154\\\\00135.png', '../subset2\\\\images\\\\video_0154\\\\00150.png', '../subset2\\\\images\\\\video_0154\\\\00165.png', '../subset2\\\\images\\\\video_0154\\\\00180.png', '../subset2\\\\images\\\\video_0154\\\\00195.png', '../subset2\\\\images\\\\video_0154\\\\00210.png', '../subset2\\\\images\\\\video_0154\\\\00225.png', '../subset2\\\\images\\\\video_0154\\\\00240.png', '../subset2\\\\images\\\\video_0154\\\\00255.png', '../subset2\\\\images\\\\video_0154\\\\00270.png', '../subset2\\\\images\\\\video_0154\\\\00285.png', '../subset2\\\\images\\\\video_0154\\\\00300.png', '../subset2\\\\images\\\\video_0154\\\\00315.png'], ['../subset2\\\\images\\\\video_0154\\\\00087.png', '../subset2\\\\images\\\\video_0154\\\\00102.png', '../subset2\\\\images\\\\video_0154\\\\00117.png', '../subset2\\\\images\\\\video_0154\\\\00132.png', '../subset2\\\\images\\\\video_0154\\\\00147.png', '../subset2\\\\images\\\\video_0154\\\\00162.png', '../subset2\\\\images\\\\video_0154\\\\00177.png', '../subset2\\\\images\\\\video_0154\\\\00192.png', '../subset2\\\\images\\\\video_0154\\\\00207.png', '../subset2\\\\images\\\\video_0154\\\\00222.png', '../subset2\\\\images\\\\video_0154\\\\00237.png', '../subset2\\\\images\\\\video_0154\\\\00252.png', '../subset2\\\\images\\\\video_0154\\\\00267.png', '../subset2\\\\images\\\\video_0154\\\\00282.png', '../subset2\\\\images\\\\video_0154\\\\00297.png', '../subset2\\\\images\\\\video_0154\\\\00312.png'], ['../subset2\\\\images\\\\video_0154\\\\00000.png', '../subset2\\\\images\\\\video_0154\\\\00015.png', '../subset2\\\\images\\\\video_0154\\\\00030.png', '../subset2\\\\images\\\\video_0154\\\\00045.png', '../subset2\\\\images\\\\video_0154\\\\00060.png', '../subset2\\\\images\\\\video_0154\\\\00075.png', '../subset2\\\\images\\\\video_0154\\\\00090.png', '../subset2\\\\images\\\\video_0154\\\\00105.png', '../subset2\\\\images\\\\video_0154\\\\00120.png', '../subset2\\\\images\\\\video_0154\\\\00135.png', '../subset2\\\\images\\\\video_0154\\\\00150.png', '../subset2\\\\images\\\\video_0154\\\\00165.png', '../subset2\\\\images\\\\video_0154\\\\00180.png', '../subset2\\\\images\\\\video_0154\\\\00195.png', '../subset2\\\\images\\\\video_0154\\\\00210.png', '../subset2\\\\images\\\\video_0154\\\\00225.png', '../subset2\\\\images\\\\video_0154\\\\00240.png', '../subset2\\\\images\\\\video_0154\\\\00255.png', '../subset2\\\\images\\\\video_0154\\\\00270.png', '../subset2\\\\images\\\\video_0154\\\\00285.png', '../subset2\\\\images\\\\video_0154\\\\00300.png', '../subset2\\\\images\\\\video_0154\\\\00315.png'], ['../subset2\\\\images\\\\video_0159\\\\00000.png', '../subset2\\\\images\\\\video_0159\\\\00015.png', '../subset2\\\\images\\\\video_0159\\\\00030.png', '../subset2\\\\images\\\\video_0159\\\\00045.png', '../subset2\\\\images\\\\video_0159\\\\00060.png', '../subset2\\\\images\\\\video_0159\\\\00075.png', '../subset2\\\\images\\\\video_0159\\\\00090.png', '../subset2\\\\images\\\\video_0159\\\\00105.png', '../subset2\\\\images\\\\video_0159\\\\00120.png', '../subset2\\\\images\\\\video_0159\\\\00135.png', '../subset2\\\\images\\\\video_0159\\\\00150.png', '../subset2\\\\images\\\\video_0159\\\\00165.png', '../subset2\\\\images\\\\video_0159\\\\00180.png', '../subset2\\\\images\\\\video_0159\\\\00195.png', '../subset2\\\\images\\\\video_0159\\\\00210.png', '../subset2\\\\images\\\\video_0159\\\\00225.png'], ['../subset2\\\\images\\\\video_0159\\\\00000.png', '../subset2\\\\images\\\\video_0159\\\\00015.png', '../subset2\\\\images\\\\video_0159\\\\00030.png', '../subset2\\\\images\\\\video_0159\\\\00045.png', '../subset2\\\\images\\\\video_0159\\\\00060.png', '../subset2\\\\images\\\\video_0159\\\\00075.png', '../subset2\\\\images\\\\video_0159\\\\00090.png', '../subset2\\\\images\\\\video_0159\\\\00105.png', '../subset2\\\\images\\\\video_0159\\\\00120.png', '../subset2\\\\images\\\\video_0159\\\\00135.png', '../subset2\\\\images\\\\video_0159\\\\00150.png', '../subset2\\\\images\\\\video_0159\\\\00165.png', '../subset2\\\\images\\\\video_0159\\\\00180.png', '../subset2\\\\images\\\\video_0159\\\\00195.png', '../subset2\\\\images\\\\video_0159\\\\00210.png', '../subset2\\\\images\\\\video_0159\\\\00225.png'], ['../subset2\\\\images\\\\video_0208\\\\00000.png', '../subset2\\\\images\\\\video_0208\\\\00015.png', '../subset2\\\\images\\\\video_0208\\\\00030.png', '../subset2\\\\images\\\\video_0208\\\\00045.png', '../subset2\\\\images\\\\video_0208\\\\00060.png', '../subset2\\\\images\\\\video_0208\\\\00075.png', '../subset2\\\\images\\\\video_0208\\\\00090.png', '../subset2\\\\images\\\\video_0208\\\\00105.png', '../subset2\\\\images\\\\video_0208\\\\00120.png', '../subset2\\\\images\\\\video_0208\\\\00135.png', '../subset2\\\\images\\\\video_0208\\\\00150.png', '../subset2\\\\images\\\\video_0208\\\\00165.png', '../subset2\\\\images\\\\video_0208\\\\00180.png', '../subset2\\\\images\\\\video_0208\\\\00195.png', '../subset2\\\\images\\\\video_0208\\\\00210.png'], ['../subset2\\\\images\\\\video_0208\\\\00010.png', '../subset2\\\\images\\\\video_0208\\\\00025.png', '../subset2\\\\images\\\\video_0208\\\\00040.png', '../subset2\\\\images\\\\video_0208\\\\00055.png', '../subset2\\\\images\\\\video_0208\\\\00070.png', '../subset2\\\\images\\\\video_0208\\\\00085.png', '../subset2\\\\images\\\\video_0208\\\\00100.png', '../subset2\\\\images\\\\video_0208\\\\00115.png', '../subset2\\\\images\\\\video_0208\\\\00130.png', '../subset2\\\\images\\\\video_0208\\\\00145.png', '../subset2\\\\images\\\\video_0208\\\\00160.png', '../subset2\\\\images\\\\video_0208\\\\00175.png', '../subset2\\\\images\\\\video_0208\\\\00190.png', '../subset2\\\\images\\\\video_0208\\\\00205.png', '../subset2\\\\images\\\\video_0208\\\\00220.png', '../subset2\\\\images\\\\video_0208\\\\00235.png'], ['../subset2\\\\images\\\\video_0266\\\\00000.png', '../subset2\\\\images\\\\video_0266\\\\00015.png', '../subset2\\\\images\\\\video_0266\\\\00030.png', '../subset2\\\\images\\\\video_0266\\\\00045.png', '../subset2\\\\images\\\\video_0266\\\\00060.png', '../subset2\\\\images\\\\video_0266\\\\00075.png', '../subset2\\\\images\\\\video_0266\\\\00090.png', '../subset2\\\\images\\\\video_0266\\\\00105.png', '../subset2\\\\images\\\\video_0266\\\\00120.png', '../subset2\\\\images\\\\video_0266\\\\00135.png', '../subset2\\\\images\\\\video_0266\\\\00150.png', '../subset2\\\\images\\\\video_0266\\\\00165.png', '../subset2\\\\images\\\\video_0266\\\\00180.png', '../subset2\\\\images\\\\video_0266\\\\00195.png', '../subset2\\\\images\\\\video_0266\\\\00210.png', '../subset2\\\\images\\\\video_0266\\\\00225.png', '../subset2\\\\images\\\\video_0266\\\\00240.png', '../subset2\\\\images\\\\video_0266\\\\00255.png', '../subset2\\\\images\\\\video_0266\\\\00270.png'], ['../subset2\\\\images\\\\video_0266\\\\00000.png', '../subset2\\\\images\\\\video_0266\\\\00015.png', '../subset2\\\\images\\\\video_0266\\\\00030.png', '../subset2\\\\images\\\\video_0266\\\\00045.png', '../subset2\\\\images\\\\video_0266\\\\00060.png', '../subset2\\\\images\\\\video_0266\\\\00075.png', '../subset2\\\\images\\\\video_0266\\\\00090.png', '../subset2\\\\images\\\\video_0266\\\\00105.png', '../subset2\\\\images\\\\video_0266\\\\00120.png', '../subset2\\\\images\\\\video_0266\\\\00135.png', '../subset2\\\\images\\\\video_0266\\\\00150.png', '../subset2\\\\images\\\\video_0266\\\\00165.png', '../subset2\\\\images\\\\video_0266\\\\00180.png', '../subset2\\\\images\\\\video_0266\\\\00195.png', '../subset2\\\\images\\\\video_0266\\\\00210.png', '../subset2\\\\images\\\\video_0266\\\\00225.png', '../subset2\\\\images\\\\video_0266\\\\00240.png', '../subset2\\\\images\\\\video_0266\\\\00255.png', '../subset2\\\\images\\\\video_0266\\\\00270.png', '../subset2\\\\images\\\\video_0266\\\\00285.png'], ['../subset2\\\\images\\\\video_0266\\\\00000.png', '../subset2\\\\images\\\\video_0266\\\\00015.png', '../subset2\\\\images\\\\video_0266\\\\00030.png', '../subset2\\\\images\\\\video_0266\\\\00045.png', '../subset2\\\\images\\\\video_0266\\\\00060.png', '../subset2\\\\images\\\\video_0266\\\\00075.png', '../subset2\\\\images\\\\video_0266\\\\00090.png', '../subset2\\\\images\\\\video_0266\\\\00105.png', '../subset2\\\\images\\\\video_0266\\\\00120.png', '../subset2\\\\images\\\\video_0266\\\\00135.png', '../subset2\\\\images\\\\video_0266\\\\00150.png', '../subset2\\\\images\\\\video_0266\\\\00165.png', '../subset2\\\\images\\\\video_0266\\\\00180.png', '../subset2\\\\images\\\\video_0266\\\\00195.png', '../subset2\\\\images\\\\video_0266\\\\00210.png', '../subset2\\\\images\\\\video_0266\\\\00225.png', '../subset2\\\\images\\\\video_0266\\\\00240.png', '../subset2\\\\images\\\\video_0266\\\\00255.png', '../subset2\\\\images\\\\video_0266\\\\00270.png', '../subset2\\\\images\\\\video_0266\\\\00285.png', '../subset2\\\\images\\\\video_0266\\\\00300.png', '../subset2\\\\images\\\\video_0266\\\\00315.png', '../subset2\\\\images\\\\video_0266\\\\00330.png', '../subset2\\\\images\\\\video_0266\\\\00345.png'], ['../subset2\\\\images\\\\video_0266\\\\00000.png', '../subset2\\\\images\\\\video_0266\\\\00015.png', '../subset2\\\\images\\\\video_0266\\\\00030.png', '../subset2\\\\images\\\\video_0266\\\\00045.png', '../subset2\\\\images\\\\video_0266\\\\00060.png', '../subset2\\\\images\\\\video_0266\\\\00075.png', '../subset2\\\\images\\\\video_0266\\\\00090.png', '../subset2\\\\images\\\\video_0266\\\\00105.png', '../subset2\\\\images\\\\video_0266\\\\00120.png', '../subset2\\\\images\\\\video_0266\\\\00135.png', '../subset2\\\\images\\\\video_0266\\\\00150.png', '../subset2\\\\images\\\\video_0266\\\\00165.png', '../subset2\\\\images\\\\video_0266\\\\00180.png', '../subset2\\\\images\\\\video_0266\\\\00195.png', '../subset2\\\\images\\\\video_0266\\\\00210.png', '../subset2\\\\images\\\\video_0266\\\\00225.png', '../subset2\\\\images\\\\video_0266\\\\00240.png', '../subset2\\\\images\\\\video_0266\\\\00255.png', '../subset2\\\\images\\\\video_0266\\\\00270.png', '../subset2\\\\images\\\\video_0266\\\\00285.png', '../subset2\\\\images\\\\video_0266\\\\00300.png', '../subset2\\\\images\\\\video_0266\\\\00315.png', '../subset2\\\\images\\\\video_0266\\\\00330.png', '../subset2\\\\images\\\\video_0266\\\\00345.png'], ['../subset2\\\\images\\\\video_0266\\\\00009.png', '../subset2\\\\images\\\\video_0266\\\\00024.png', '../subset2\\\\images\\\\video_0266\\\\00039.png', '../subset2\\\\images\\\\video_0266\\\\00054.png', '../subset2\\\\images\\\\video_0266\\\\00069.png', '../subset2\\\\images\\\\video_0266\\\\00084.png', '../subset2\\\\images\\\\video_0266\\\\00099.png', '../subset2\\\\images\\\\video_0266\\\\00114.png', '../subset2\\\\images\\\\video_0266\\\\00129.png', '../subset2\\\\images\\\\video_0266\\\\00144.png', '../subset2\\\\images\\\\video_0266\\\\00159.png', '../subset2\\\\images\\\\video_0266\\\\00174.png', '../subset2\\\\images\\\\video_0266\\\\00189.png', '../subset2\\\\images\\\\video_0266\\\\00204.png', '../subset2\\\\images\\\\video_0266\\\\00219.png', '../subset2\\\\images\\\\video_0266\\\\00234.png', '../subset2\\\\images\\\\video_0266\\\\00249.png', '../subset2\\\\images\\\\video_0266\\\\00264.png', '../subset2\\\\images\\\\video_0266\\\\00279.png', '../subset2\\\\images\\\\video_0266\\\\00294.png', '../subset2\\\\images\\\\video_0266\\\\00309.png', '../subset2\\\\images\\\\video_0266\\\\00324.png', '../subset2\\\\images\\\\video_0266\\\\00339.png', '../subset2\\\\images\\\\video_0266\\\\00354.png'], ['../subset2\\\\images\\\\video_0293\\\\00000.png', '../subset2\\\\images\\\\video_0293\\\\00015.png', '../subset2\\\\images\\\\video_0293\\\\00030.png', '../subset2\\\\images\\\\video_0293\\\\00045.png', '../subset2\\\\images\\\\video_0293\\\\00060.png', '../subset2\\\\images\\\\video_0293\\\\00075.png', '../subset2\\\\images\\\\video_0293\\\\00090.png', '../subset2\\\\images\\\\video_0293\\\\00105.png', '../subset2\\\\images\\\\video_0293\\\\00120.png', '../subset2\\\\images\\\\video_0293\\\\00135.png', '../subset2\\\\images\\\\video_0293\\\\00150.png', '../subset2\\\\images\\\\video_0293\\\\00165.png', '../subset2\\\\images\\\\video_0293\\\\00180.png', '../subset2\\\\images\\\\video_0293\\\\00195.png', '../subset2\\\\images\\\\video_0293\\\\00210.png', '../subset2\\\\images\\\\video_0293\\\\00225.png', '../subset2\\\\images\\\\video_0293\\\\00240.png', '../subset2\\\\images\\\\video_0293\\\\00255.png'], ['../subset2\\\\images\\\\video_0293\\\\00000.png', '../subset2\\\\images\\\\video_0293\\\\00015.png', '../subset2\\\\images\\\\video_0293\\\\00030.png', '../subset2\\\\images\\\\video_0293\\\\00045.png', '../subset2\\\\images\\\\video_0293\\\\00060.png', '../subset2\\\\images\\\\video_0293\\\\00075.png', '../subset2\\\\images\\\\video_0293\\\\00090.png', '../subset2\\\\images\\\\video_0293\\\\00105.png', '../subset2\\\\images\\\\video_0293\\\\00120.png', '../subset2\\\\images\\\\video_0293\\\\00135.png', '../subset2\\\\images\\\\video_0293\\\\00150.png', '../subset2\\\\images\\\\video_0293\\\\00165.png', '../subset2\\\\images\\\\video_0293\\\\00180.png', '../subset2\\\\images\\\\video_0293\\\\00195.png', '../subset2\\\\images\\\\video_0293\\\\00210.png', '../subset2\\\\images\\\\video_0293\\\\00225.png'], ['../subset2\\\\images\\\\video_0293\\\\00000.png', '../subset2\\\\images\\\\video_0293\\\\00015.png', '../subset2\\\\images\\\\video_0293\\\\00030.png', '../subset2\\\\images\\\\video_0293\\\\00045.png', '../subset2\\\\images\\\\video_0293\\\\00060.png', '../subset2\\\\images\\\\video_0293\\\\00075.png', '../subset2\\\\images\\\\video_0293\\\\00090.png', '../subset2\\\\images\\\\video_0293\\\\00105.png', '../subset2\\\\images\\\\video_0293\\\\00120.png', '../subset2\\\\images\\\\video_0293\\\\00135.png', '../subset2\\\\images\\\\video_0293\\\\00150.png', '../subset2\\\\images\\\\video_0293\\\\00165.png', '../subset2\\\\images\\\\video_0293\\\\00180.png', '../subset2\\\\images\\\\video_0293\\\\00195.png', '../subset2\\\\images\\\\video_0293\\\\00210.png', '../subset2\\\\images\\\\video_0293\\\\00225.png', '../subset2\\\\images\\\\video_0293\\\\00240.png', '../subset2\\\\images\\\\video_0293\\\\00255.png'], ['../subset2\\\\images\\\\video_0293\\\\00000.png', '../subset2\\\\images\\\\video_0293\\\\00015.png', '../subset2\\\\images\\\\video_0293\\\\00030.png', '../subset2\\\\images\\\\video_0293\\\\00045.png', '../subset2\\\\images\\\\video_0293\\\\00060.png', '../subset2\\\\images\\\\video_0293\\\\00075.png', '../subset2\\\\images\\\\video_0293\\\\00090.png', '../subset2\\\\images\\\\video_0293\\\\00105.png', '../subset2\\\\images\\\\video_0293\\\\00120.png', '../subset2\\\\images\\\\video_0293\\\\00135.png', '../subset2\\\\images\\\\video_0293\\\\00150.png', '../subset2\\\\images\\\\video_0293\\\\00165.png', '../subset2\\\\images\\\\video_0293\\\\00180.png', '../subset2\\\\images\\\\video_0293\\\\00195.png', '../subset2\\\\images\\\\video_0293\\\\00210.png', '../subset2\\\\images\\\\video_0293\\\\00225.png', '../subset2\\\\images\\\\video_0293\\\\00240.png', '../subset2\\\\images\\\\video_0293\\\\00255.png'], ['../subset2\\\\images\\\\video_0317\\\\00008.png', '../subset2\\\\images\\\\video_0317\\\\00023.png', '../subset2\\\\images\\\\video_0317\\\\00038.png', '../subset2\\\\images\\\\video_0317\\\\00053.png', '../subset2\\\\images\\\\video_0317\\\\00068.png', '../subset2\\\\images\\\\video_0317\\\\00083.png', '../subset2\\\\images\\\\video_0317\\\\00098.png', '../subset2\\\\images\\\\video_0317\\\\00113.png', '../subset2\\\\images\\\\video_0317\\\\00128.png', '../subset2\\\\images\\\\video_0317\\\\00143.png', '../subset2\\\\images\\\\video_0317\\\\00158.png', '../subset2\\\\images\\\\video_0317\\\\00173.png', '../subset2\\\\images\\\\video_0317\\\\00188.png', '../subset2\\\\images\\\\video_0317\\\\00203.png', '../subset2\\\\images\\\\video_0317\\\\00218.png', '../subset2\\\\images\\\\video_0317\\\\00233.png', '../subset2\\\\images\\\\video_0317\\\\00248.png'], ['../subset2\\\\images\\\\video_0317\\\\00000.png', '../subset2\\\\images\\\\video_0317\\\\00015.png', '../subset2\\\\images\\\\video_0317\\\\00030.png', '../subset2\\\\images\\\\video_0317\\\\00045.png', '../subset2\\\\images\\\\video_0317\\\\00060.png', '../subset2\\\\images\\\\video_0317\\\\00075.png', '../subset2\\\\images\\\\video_0317\\\\00090.png', '../subset2\\\\images\\\\video_0317\\\\00105.png', '../subset2\\\\images\\\\video_0317\\\\00120.png', '../subset2\\\\images\\\\video_0317\\\\00135.png', '../subset2\\\\images\\\\video_0317\\\\00150.png', '../subset2\\\\images\\\\video_0317\\\\00165.png', '../subset2\\\\images\\\\video_0317\\\\00180.png', '../subset2\\\\images\\\\video_0317\\\\00195.png', '../subset2\\\\images\\\\video_0317\\\\00210.png', '../subset2\\\\images\\\\video_0317\\\\00225.png', '../subset2\\\\images\\\\video_0317\\\\00240.png'], ['../subset2\\\\images\\\\video_0341\\\\00000.png', '../subset2\\\\images\\\\video_0341\\\\00015.png', '../subset2\\\\images\\\\video_0341\\\\00030.png', '../subset2\\\\images\\\\video_0341\\\\00045.png', '../subset2\\\\images\\\\video_0341\\\\00060.png', '../subset2\\\\images\\\\video_0341\\\\00075.png', '../subset2\\\\images\\\\video_0341\\\\00090.png', '../subset2\\\\images\\\\video_0341\\\\00105.png', '../subset2\\\\images\\\\video_0341\\\\00120.png', '../subset2\\\\images\\\\video_0341\\\\00135.png', '../subset2\\\\images\\\\video_0341\\\\00150.png', '../subset2\\\\images\\\\video_0341\\\\00165.png', '../subset2\\\\images\\\\video_0341\\\\00180.png', '../subset2\\\\images\\\\video_0341\\\\00195.png', '../subset2\\\\images\\\\video_0341\\\\00210.png', '../subset2\\\\images\\\\video_0341\\\\00225.png', '../subset2\\\\images\\\\video_0341\\\\00240.png', '../subset2\\\\images\\\\video_0341\\\\00255.png', '../subset2\\\\images\\\\video_0341\\\\00270.png'], ['../subset2\\\\images\\\\video_0341\\\\00000.png', '../subset2\\\\images\\\\video_0341\\\\00015.png', '../subset2\\\\images\\\\video_0341\\\\00030.png', '../subset2\\\\images\\\\video_0341\\\\00045.png', '../subset2\\\\images\\\\video_0341\\\\00060.png', '../subset2\\\\images\\\\video_0341\\\\00075.png', '../subset2\\\\images\\\\video_0341\\\\00090.png', '../subset2\\\\images\\\\video_0341\\\\00105.png', '../subset2\\\\images\\\\video_0341\\\\00120.png', '../subset2\\\\images\\\\video_0341\\\\00135.png', '../subset2\\\\images\\\\video_0341\\\\00150.png', '../subset2\\\\images\\\\video_0341\\\\00165.png', '../subset2\\\\images\\\\video_0341\\\\00180.png', '../subset2\\\\images\\\\video_0341\\\\00195.png', '../subset2\\\\images\\\\video_0341\\\\00210.png', '../subset2\\\\images\\\\video_0341\\\\00225.png', '../subset2\\\\images\\\\video_0341\\\\00240.png', '../subset2\\\\images\\\\video_0341\\\\00255.png', '../subset2\\\\images\\\\video_0341\\\\00270.png', '../subset2\\\\images\\\\video_0341\\\\00285.png']]\n",
      "['../subset2\\\\images\\\\video_0028\\\\00000.png', '../subset2\\\\images\\\\video_0028\\\\00015.png', '../subset2\\\\images\\\\video_0028\\\\00030.png', '../subset2\\\\images\\\\video_0028\\\\00045.png', '../subset2\\\\images\\\\video_0028\\\\00060.png', '../subset2\\\\images\\\\video_0028\\\\00075.png', '../subset2\\\\images\\\\video_0028\\\\00090.png', '../subset2\\\\images\\\\video_0028\\\\00105.png', '../subset2\\\\images\\\\video_0028\\\\00120.png', '../subset2\\\\images\\\\video_0028\\\\00135.png', '../subset2\\\\images\\\\video_0028\\\\00150.png', '../subset2\\\\images\\\\video_0028\\\\00165.png', '../subset2\\\\images\\\\video_0028\\\\00180.png', '../subset2\\\\images\\\\video_0028\\\\00195.png', '../subset2\\\\images\\\\video_0028\\\\00210.png', '../subset2\\\\images\\\\video_0028\\\\00225.png']\n"
     ]
    }
   ],
   "source": [
    "print((seq_train['image']))\n",
    "print((seq_test['image'][1]))\n",
    "# print(len(seq_train['bbox']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL CONTEXT EXTRACTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    deeplab_model = network.modeling.__dict__['deeplabv3plus_resnet101'](num_classes=19)\n",
    "    deeplab_model.load_state_dict(torch.load(DEEPLAB_PATH)['model_state'])\n",
    "    deeplab_model.to(device)\n",
    "    deeplab_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trasformazioni che vengono usate dentro global context (modifica l'input prima che vada in deeplab)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Ridimensiona le immagini a 512x512\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#processa output deeplab\n",
    "GC_trans = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ridimensiona le immagini a 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza la maschera semantica\n",
    "def visualize_mask(image_path, mask):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    #image = image.resize((512, 512))  # Ridimensiona per la visualizzazione\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap='jet')\n",
    "    plt.title(\"Semantic Mask\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_mask(image_path, model, preprocess):\n",
    "    \"\"\" funzione che prende in input le path delle imagini, il modello e la funzione di preprocessamento \n",
    "    e restituisce la maschera segmentata dell'immagine resizata a 224x224\"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = preprocess(input_image).to(device)\n",
    "    input_batch = input_tensor.unsqueeze(0)  \n",
    "    \n",
    "    # Pass the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    # Check if output is a tensor or a dictionary (auxiliary control)\n",
    "    if isinstance(output, dict):\n",
    "        output = output['out'][0]\n",
    "    elif isinstance(output, torch.Tensor):\n",
    "        output = output[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected output type: {type(output)}\")\n",
    "    \n",
    "    # Convert the output to a mask\n",
    "    output_predictions = output.argmax(0)\n",
    "\n",
    "####################################################################################\n",
    "    #OPTIONAL FOR DEBUG AND PLOTTING:\n",
    "    # output_predictions_pic = output_predictions.clone().cpu()\n",
    "    # visualize_mask(image_path,output_predictions_pic) \n",
    "####################################################################################\n",
    "\n",
    "    # Fix dimensions for the mask and convert to float\n",
    "    output_predictions = output_predictions.unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "    #convert to RGB and resize to (224,224), because VGG want(3,224,224) as input\n",
    "    tr = transforms.ToPILImage()\n",
    "    pic = tr(output_predictions.squeeze(1))\n",
    "    pic= pic.convert(\"RGB\")\n",
    "    resized_mask = GC_trans(pic)\n",
    "\n",
    "    return resized_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_frames(seq_train, model, preprocess):\n",
    "    \"\"\"funzione che prende in input la sequenza di training, il modello e la funzione di preprocessamento, restituisce una\n",
    "    lista di segmentation mask per ogni frame di ogni video della sequenza di training\"\"\"\n",
    "    \n",
    "    all_masks = []\n",
    "    for video_frames in tqdm(seq_train['image'], desc=\"Processing videos\"):\n",
    "        video_masks = []\n",
    "\n",
    "        for frame_path in tqdm(video_frames, desc=\"Processing frames\", leave=False):\n",
    "            mask = get_segmentation_mask(frame_path, model, preprocess)\n",
    "            video_masks.append(mask)\n",
    "        all_masks.append(video_masks)\n",
    "    \n",
    "    return all_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    all_video_masks = process_video_frames(seq_train, deeplab_model, train_transforms)\n",
    "    seq_train['masks'] = all_video_masks\n",
    "    all_video_masks_test = process_video_frames(seq_test, deeplab_model, train_transforms)\n",
    "    seq_test['masks'] = all_video_masks_test\n",
    "\n",
    "    #cleaning for my poor gpu\n",
    "    del deeplab_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # save data in the .pkl files \n",
    "    with open(MASK_CMD, 'wb') as f:\n",
    "        pickle.dump(seq_train['masks'], f)\n",
    "    with open(MASK_CMD_TEST, 'wb') as f:\n",
    "        pickle.dump(seq_test['masks'], f)\n",
    "else:\n",
    "    #recover data:\n",
    "    with open(MASK_CMD, 'rb') as f:\n",
    "        seq_train['masks'] = pickle.load(f)\n",
    "    with open(MASK_CMD_TEST, 'rb') as f:\n",
    "        seq_test['masks'] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOCAL CONTEXT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasformazioni per le immaginin che vengono per il local context \n",
    "transform_lc = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_cv2(img, bbox):\n",
    "    \"\"\" funzione che croppa i frames sul bounding boxes, le imagini sono in formato cv2\"\"\"\n",
    "    \n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return img[int(y1):int(y2), int(x1):int(x2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|| 60/60 [00:33<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Trasformation for the local context's images, enhance the quality of the images by appling gaussian filter, unsharp mask e bilateral filter\"\"\"\n",
    "all_images = [] #list of the images\n",
    "for i in tqdm(range(len(seq_train['image'])), desc=\"Processing videos\"):\n",
    "    aux_list = []\n",
    "    for j in tqdm(range(len(seq_train['image'][i])), desc=\"Processing frames\", leave=False):\n",
    "        \n",
    "        # Open the images from the paths\n",
    "        img_path = seq_train['image'][i][j]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        \n",
    "        #get the bbox and crop arround it\n",
    "        bbox = seq_train['bbox'][i][j]\n",
    "        cropped_images = crop_image_cv2(img, bbox)\n",
    "        \n",
    "        # Resize to 224x224 (VGG input size)\n",
    "        final_image = cv2.resize(cropped_images, (224, 224))\n",
    "        aux_list.append(final_image)\n",
    "    all_images.append(aux_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|| 97/97 [00:54<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#same of above, but for the test set\n",
    "all_images_test = []\n",
    "for i in tqdm(range(len(seq_test['image'])), desc=\"Processing videos\"):\n",
    "    aux_list = []\n",
    "    for j in tqdm(range(len(seq_test['image'][i])), desc=\"Processing frames\", leave=False):\n",
    "        \n",
    "        # Open the images from the paths\n",
    "        img_path = seq_test['image'][i][j]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "        #get the bbox and crop arround it\n",
    "        bbox = seq_test['bbox'][i][j]\n",
    "        cropped_images = crop_image_cv2(img, bbox)\n",
    "        \n",
    "        # Resize to 224x224 (VGG input size)\n",
    "        final_image = cv2.resize(cropped_images, (224, 224))\n",
    "        aux_list.append(final_image)\n",
    "    all_images_test.append(aux_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSE KEYPOINTS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_sequence(frames, body_model):\n",
    "    \"\"\"funzione che prende in input i frames e il modello di openpose e restituisce una lista di tensori di pose \n",
    "    per ciascuna persona nel tempo, se non prende nessuna posa mette un placeholder\"\"\"\n",
    "    #print(frames)\n",
    "    pose_sequences = []  # Lista di pose per ciascuna persona nel tempo\n",
    "    pose_placeholder = torch.zeros((36,), dtype=torch.float32)\n",
    "    for frame in frames:\n",
    "        candidate, subset = body_model(frame)\n",
    "\n",
    "        ###############################################################\n",
    "        # DEBUG & PLOTTING: Uncomment the following\n",
    "        canvas = copy.deepcopy(frame)\n",
    "        canvas = util.draw_bodypose(canvas, candidate, subset)\n",
    "        plt.imshow(canvas[:, :, [2, 1, 0]])  \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        ###############################################################\n",
    "\n",
    "        frame_poses = []\n",
    "        for person in subset:\n",
    "            if person[-1] >= 4:  # Almeno 4 punti chiave rilevati\n",
    "                pose = []\n",
    "                for i in range(18):\n",
    "                    if person[i] != -1:\n",
    "                        x, y = candidate[int(person[i])][:2]\n",
    "                    else:\n",
    "                        x, y = -1, -1  # Punti chiave mancanti\n",
    "                    pose.extend([x, y])\n",
    "                frame_poses.append(pose)\n",
    "        if not frame_poses:\n",
    "            frame_poses = [pose_placeholder.tolist()] #use placeholder if no pose detected (for dimensional consistency)\n",
    "        pose_sequences.append(frame_poses)\n",
    "\n",
    "    # Trasponi la lista di liste per ottenere le sequenze temporali per ciascuna persona\n",
    "    person_pose_sequences = list(map(list, zip(*pose_sequences)))\n",
    "    person_pose_sequences = [torch.tensor(person_poses, dtype=torch.float32) for person_poses in person_pose_sequences]\n",
    "    \n",
    "    return person_pose_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if RUN:\n",
    "    body_model = Body(POSE_PATH)\n",
    "\n",
    "    # Caricamento dei frame e estrazione delle pose\n",
    "    all_poses = []\n",
    "    all_poses_test = []\n",
    "    # print(len(all_images))\n",
    "    # print(len(all_images[0]))\n",
    "    # print(len(all_images[0][0]))\n",
    "    # print(all_images[0][0].shape)\n",
    "    #itera tra i video e prendi i frames\n",
    "    for pics in tqdm(all_images, desc=\"Extracting poses from image sequences\"):\n",
    "        #print(len(pics))\n",
    "        pose_sequences = extract_pose_sequence(pics, body_model)\n",
    "        all_poses.append(pose_sequences)\n",
    "        #print(pose_sequences.shape)\n",
    "    # Aggiungi le pose estratte alla sequenza di allenamento\n",
    "    seq_train['poses'] = all_poses\n",
    "\n",
    "    for pics in tqdm(all_images_test, desc=\"Extracting poses from image sequences of test set\"):\n",
    "        pose_sequences = extract_pose_sequence(pics, body_model)\n",
    "        all_poses_test.append(pose_sequences)\n",
    "    # Aggiungi le pose estratte alla sequenza di allenamento\n",
    "    seq_test['poses'] = all_poses_test\n",
    "    del body_model\n",
    "    torch.cuda.empty_cache()\n",
    "    # Apri il file in modalit scrittura binaria e salva il dizionario\n",
    "\n",
    "    with open(POSE_CMD, 'wb') as f:\n",
    "        pickle.dump(seq_train['poses'], f)\n",
    "    with open(POSE_CMD_TEST, 'wb') as f:\n",
    "        pickle.dump(seq_test['poses'], f)\n",
    "else:\n",
    "    #recover data:\n",
    "    with open(POSE_CMD, 'rb') as f:\n",
    "        seq_train['poses'] = pickle.load(f)\n",
    "    with open(POSE_CMD_TEST, 'rb') as f:\n",
    "        seq_test['poses'] = pickle.load(f)\n",
    "    # Verifica che i risultati siano stati caricati correttamente\n",
    "    #print(seq_train['masks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionBranchLocal(torch.nn.Module):\n",
    "    \"\"\"definizione del modello per il local context, prende in input le immagini croppate e restituisce un tensore,\n",
    "    le immagini croppate vengono fatte passare dentro una VGG16, una GRU e un attention block\"\"\"\n",
    "\n",
    "    def __init__(self, vgg16):\n",
    "        super(VisionBranchLocal, self).__init__()\n",
    "        self.vgg16 = vgg16\n",
    "        self.avgpool = torch.nn.AvgPool2d(kernel_size=14)  # Pooling layer con kernel 14x14\n",
    "        self.gru = torch.nn.GRU(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(256, 2)    # Fully connected layer\n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, cropped_images):\n",
    "        seq_len, c, h, w = cropped_images.size()\n",
    "        \n",
    "        # Estrai feature dalle immagini con VGG16\n",
    "        vgg_features = []\n",
    "        for i in range(seq_len):            \n",
    "\n",
    "            img = cropped_images[i]            \n",
    "            vgg_feat_img = self.vgg16.features(img)\n",
    "            pooled_feat_img = self.avgpool(vgg_feat_img)  # Applica il pooling\n",
    "            vgg_feat_img = pooled_feat_img.view(pooled_feat_img.size(0), -1)  # Flatten features\n",
    "            vgg_features.append(vgg_feat_img)\n",
    "        \n",
    "        vgg_features = torch.stack(vgg_features, dim=1).permute(2,1,0)\n",
    "\n",
    "        #applica la gru\n",
    "        gru_out, _ = self.gru(vgg_features)\n",
    "\n",
    "        #applica l'attention\n",
    "        attn_weights = torch.softmax(self.attn(gru_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n",
    "        \n",
    "       # print(\"SIZE vgg features:\",vgg_features.shape)\n",
    "       # print(\"SIZE context v local context:\",context_vector.shape)\n",
    "        #out = self.sigmoid(self.fc(gru_out[:, -1, :]))   \n",
    "        out = self.tanh((context_vector))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionBranchGlobal(torch.nn.Module):\n",
    "    \"\"\"definizione del modello per il global context, prende in input le maskere semantiche e restituisce un tensore,\n",
    "    le maskere semantiche vengono fatte passare dentro una VGG16, una GRU e un attention block\"\"\"\n",
    "\n",
    "    def __init__(self, vgg16):\n",
    "        super(VisionBranchGlobal, self).__init__()\n",
    "        self.vgg16 = vgg16\n",
    "        self.avgpool = torch.nn.AvgPool2d(kernel_size=14)  # Pooling layer con kernel 14x14\n",
    "        self.gru = torch.nn.GRU(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(256, 2)    # Fully connected layer\n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, masks):\n",
    "        seq_len = masks.size()[0]\n",
    "        #print(\"size forward:\",seq_len)\n",
    "        # Estrai feature dalle immagini con VGG16\n",
    "        vgg_features = []\n",
    "        for i in range(seq_len):            \n",
    "            img = masks[i]            \n",
    "            vgg_feat_img = self.vgg16.features(img)\n",
    "            pooled_feat_img = self.avgpool(vgg_feat_img)  # Applica il pooling\n",
    "            vgg_feat_img = pooled_feat_img.view(pooled_feat_img.size(0), -1)  # Flatten features\n",
    "            vgg_features.append(vgg_feat_img)\n",
    "        \n",
    "        vgg_features = torch.stack(vgg_features, dim=1).permute(2,1,0)\n",
    "\n",
    "        gru_out, _ = self.gru(vgg_features)\n",
    "        attn_scores = self.attn(gru_out)  # shape: (batch_size, seq_length, 1)\n",
    "       # print(\"SIZE attention scores GLO:\", attn_scores.shape)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)  # shape: (batch_size, seq_length, 1)\n",
    "        #print(\"SIZE attention weights GLO:\", attn_weights.shape)\n",
    "        \n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)  # shape: (batch_size, 256)\n",
    "        #print(\"SIZE context vector GLO:\", context_vector.shape)\n",
    "        \n",
    "        out = self.tanh((context_vector))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NVisionBranch(torch.nn.Module):\n",
    "    \"\"\"classe relativa al non-vision brach, prende in input le pose e le bbox in formato tensore, esse vengono fatte passare\n",
    "    dentro una GRU e un attention block, l'ordine influenza la prestazioni\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NVisionBranch, self).__init__()\n",
    "        self.gru = torch.nn.GRU(input_size=36, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.gru2 = torch.nn.GRU(input_size=256+4, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        #self.fc = torch.nn.Linear(256, 2)  \n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, poses,bbox):\n",
    "        gru_out, _ = self.gru(poses)\n",
    "        #print(\"SIZE outuyput gru posa:\",gru_out.shape)\n",
    "        #print(\"bbox:\",bbox.shape)\n",
    "        LP = torch.cat((gru_out,bbox),dim=-1)\n",
    "        #print(\"SIZE outuyput gru posa + bbox:\",LP.shape)\n",
    "        gru_out, _ = self.gru2(LP)\n",
    "       # print(\"SIZE output gru:\",gru_out.shape)\n",
    "\n",
    "        # Attention mechanism\n",
    "        #features = torch.stack([gru_out[:,i,:] for i in range(gru_out.size(1))], dim=1)\n",
    "        attn_scores = self.attn(gru_out)  # shape: (batch_size, seq_length, 1)\n",
    "       # print(\"SIZE attention scores:\", attn_scores.shape)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)  # shape: (batch_size, seq_length, 1)\n",
    "        #print(\"SIZE attention weights:\", attn_weights.shape)\n",
    "        \n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)  # shape: (batch_size, 256)\n",
    "        #print(\"SIZE context vector:\", context_vector.shape)\n",
    "        \n",
    "        #out = self.sigmoid(self.fc(gru_out[:, -1, :]))   \n",
    "        out = self.tanh(context_vector)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedestrianIntentModel(torch.nn.Module):\n",
    "    \"\"\"definizione del modello finale, prende in input gli output del vision brach e del non vision branch, viene fatta\n",
    "    una concatenazione che in seguito passa dentro un attention e un fully connected layer, l'output  la predizione,\n",
    "    (passa non passa)\"\"\"\n",
    "\n",
    "    def __init__(self, vision_branch_local,vision_branch_global,non_vision_branch):\n",
    "        super(PedestrianIntentModel, self).__init__()\n",
    "        self.vision_branch_local = vision_branch_local #output of the vision branch local\n",
    "        self.vision_branch_global = vision_branch_global #output of the vision branch global\n",
    "        self.non_vision_branch = non_vision_branch #output of the non vision branch\n",
    "        self.attn = torch.nn.Linear(768, 768)  # Attention layer\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(768, 256) # fully connected layer\n",
    "        self.fc2 = torch.nn.Linear(256,1) # Output: crossing or not crossing\n",
    "\n",
    "        \n",
    "    def forward(self, cropped_images, bboxes, masks, poses):\n",
    "        vision_out_local = self.vision_branch_local(cropped_images)\n",
    "        vision_out_global = self.vision_branch_global(masks)\n",
    "        non_vision_out = self.non_vision_branch(poses, bboxes)\n",
    "\n",
    "        vision_out = torch.cat((vision_out_local, vision_out_global), dim=-1)\n",
    "        final_fusion = torch.cat((vision_out, non_vision_out), dim=-1)\n",
    "\n",
    "        attn_scores = self.attn(final_fusion)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        context_vector = torch.sum(attn_weights * final_fusion, dim=0)\n",
    "\n",
    "        out = (self.fc2(self.fc1(context_vector))) #raw output\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_FeatureExtractor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGG16_FeatureExtractor, self).__init__()\n",
    "        self.features = torch.nn.Sequential(*list(vgg16.features.children())[:24]) # block4_pool  il 24 livello\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Carica il modello VGG19 pre-addestrato\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "#cut the model at the 24th layer:\n",
    "vgg16_fe = VGG16_FeatureExtractor()\n",
    "vgg16_fe\n",
    "\n",
    "\n",
    "# define the models of each branches\n",
    "model_local = VisionBranchLocal(vgg16_fe).to(device)\n",
    "model_global = VisionBranchGlobal(vgg16_fe).to(device)\n",
    "model_non_vision = NVisionBranch().to(device)\n",
    "model = PedestrianIntentModel(model_local,model_global,model_non_vision).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET & DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JAADDataset(Dataset):\n",
    "    \"\"\"definizione della classe per il custom dataset, prende in input la seq_train, le immagini e le trasformazioni,\n",
    "    restituisce il tensore delle immagini croppate, le bboxes, le maschere, le pose e la lables\"\"\"\n",
    "\n",
    "    def __init__(self, seq_data, all_images, transform=None):\n",
    "        self.seq_data = seq_data\n",
    "        self.all_images = all_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bbox_sequence = self.seq_data['bbox'][idx]\n",
    "        masks = self.seq_data['masks'][idx]\n",
    "        poses = self.seq_data['poses'][idx]\n",
    "        all_images = self.all_images[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            tensor_images = [self.transform(img) for img in all_images]\n",
    "\n",
    "        bboxes = torch.tensor(self.seq_data['bbox'][idx], dtype=torch.float32)\n",
    "        intents = torch.tensor(self.seq_data['intent'][idx], dtype=torch.float32)\n",
    "        return  tensor_images, bboxes, masks, poses, intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JAADDataset(seq_train,all_images=all_images, transform=transform_lc)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataset = JAADDataset(seq_test,all_images=all_images_test, transform=transform_lc)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss() #input are raw output\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(net, loader, device):\n",
    "    acc = BinaryAccuracy().to(device)\n",
    "    precision = BinaryPrecision().to(device)\n",
    "    recall = BinaryRecall().to(device)\n",
    "    f1_score = BinaryF1Score().to(device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for tensor_images, bboxes, masks, poses, intents in loader:\n",
    "        poses = torch.stack(poses, dim=0)  \n",
    "        poses = poses.squeeze(0)\n",
    "        poses = poses.view(len(tensor_images), -1, 36).permute(1, 0, 2) \n",
    "        \n",
    "        # Move tensors to device\n",
    "        tensor_images = torch.stack(tensor_images, dim=1).squeeze(0).permute(0, 1, 2, 3).to(device)  # Convert image list to tensor\n",
    "        masks = torch.stack(masks, dim=1).squeeze(0).float().to(device)  # Convert mask list to tensor\n",
    "        bboxes = bboxes.to(device)\n",
    "        poses = poses.to(device)\n",
    "        intents = intents.squeeze(0)[0].to(device)\n",
    "        \n",
    "        ypred = net(tensor_images, bboxes, masks, poses)\n",
    "\n",
    "        # Move tensors back to CPU and clean up memory\n",
    "        tensor_images.cpu()\n",
    "        masks.cpu()\n",
    "        bboxes.cpu()\n",
    "        poses.cpu()\n",
    "        del tensor_images, bboxes, masks, poses\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update metrics\n",
    "        acc.update(ypred, intents)\n",
    "        precision.update(ypred, intents)\n",
    "        recall.update(ypred, intents)\n",
    "        f1_score.update(ypred, intents)\n",
    "        \n",
    "        intents.cpu()\n",
    "        del intents\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': acc.compute().item(),\n",
    "        'precision': precision.compute().item(),\n",
    "        'recall': recall.compute().item(),\n",
    "        'f1_score': f1_score.compute().item()\n",
    "    }\n",
    "    \n",
    "    # Reset metrics for the next evaluation\n",
    "    acc.reset()\n",
    "    precision.reset()\n",
    "    recall.reset()\n",
    "    f1_score.reset()\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:51<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.7873\n",
      "Accuracy at epoch 0: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:49<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.8152\n",
      "Accuracy at epoch 1: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:43<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 0.7681\n",
      "Accuracy at epoch 2: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:46<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.5769\n",
      "Accuracy at epoch 3: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:44<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.5858\n",
      "Accuracy at epoch 4: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:44<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.4555\n",
      "Accuracy at epoch 5: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:45<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 0.3512\n",
      "Accuracy at epoch 6: {'accuracy': 0.8041236996650696, 'precision': 0.5384615659713745, 'recall': 0.3499999940395355, 'f1_score': 0.42424243688583374}\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:49<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 0.2366\n",
      "Accuracy at epoch 7: {'accuracy': 0.8247422575950623, 'precision': 0.5517241358757019, 'recall': 0.800000011920929, 'f1_score': 0.6530612111091614}\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:46<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Loss: 0.3022\n",
      "Accuracy at epoch 8: {'accuracy': 0.7525773048400879, 'precision': 0.4444444477558136, 'recall': 0.800000011920929, 'f1_score': 0.5714285969734192}\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:47<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.3525\n",
      "Accuracy at epoch 9: {'accuracy': 0.7319587469100952, 'precision': 0.4285714328289032, 'recall': 0.8999999761581421, 'f1_score': 0.5806451439857483}\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:48<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Loss: 0.2873\n",
      "Accuracy at epoch 10: {'accuracy': 0.7835051417350769, 'precision': 0.4864864945411682, 'recall': 0.8999999761581421, 'f1_score': 0.6315789222717285}\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:47<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Loss: 0.2601\n",
      "Accuracy at epoch 11: {'accuracy': 0.8041236996650696, 'precision': 0.52173912525177, 'recall': 0.6000000238418579, 'f1_score': 0.5581395626068115}\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:44<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Loss: 0.3172\n",
      "Accuracy at epoch 12: {'accuracy': 0.8453608155250549, 'precision': 0.6190476417541504, 'recall': 0.6499999761581421, 'f1_score': 0.6341463327407837}\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:43<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Loss: 0.2422\n",
      "Accuracy at epoch 13: {'accuracy': 0.7835051417350769, 'precision': 0.4864864945411682, 'recall': 0.8999999761581421, 'f1_score': 0.6315789222717285}\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:43<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Loss: 0.6211\n",
      "Accuracy at epoch 14: {'accuracy': 0.8453608155250549, 'precision': 0.6315789222717285, 'recall': 0.6000000238418579, 'f1_score': 0.6153846383094788}\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:43<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Loss: 0.5774\n",
      "Accuracy at epoch 15: {'accuracy': 0.8556700944900513, 'precision': 0.6363636255264282, 'recall': 0.699999988079071, 'f1_score': 0.6666666865348816}\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:45<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Loss: 1.5249\n",
      "Accuracy at epoch 16: {'accuracy': 0.8556700944900513, 'precision': 0.6071428656578064, 'recall': 0.8500000238418579, 'f1_score': 0.7083333134651184}\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:46<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Loss: 0.4481\n",
      "Accuracy at epoch 17: {'accuracy': 0.7525773048400879, 'precision': 0.44999998807907104, 'recall': 0.8999999761581421, 'f1_score': 0.6000000238418579}\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:43<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Loss: 0.1984\n",
      "Accuracy at epoch 18: {'accuracy': 0.8041236996650696, 'precision': 0.5161290168762207, 'recall': 0.800000011920929, 'f1_score': 0.6274510025978088}\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:43<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 0.2851\n",
      "Accuracy at epoch 19: {'accuracy': 0.7835051417350769, 'precision': 0.4838709533214569, 'recall': 0.75, 'f1_score': 0.5882353186607361}\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:45<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Loss: 1.6654\n",
      "Accuracy at epoch 20: {'accuracy': 0.6804123520851135, 'precision': 0.3777777850627899, 'recall': 0.8500000238418579, 'f1_score': 0.5230769515037537}\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:43<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Loss: 0.2151\n",
      "Accuracy at epoch 21: {'accuracy': 0.8247422575950623, 'precision': 0.5454545617103577, 'recall': 0.8999999761581421, 'f1_score': 0.6792452931404114}\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:42<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Loss: 0.2506\n",
      "Accuracy at epoch 22: {'accuracy': 0.8247422575950623, 'precision': 0.5483871102333069, 'recall': 0.8500000238418579, 'f1_score': 0.6666666865348816}\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:42<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Loss: 0.4728\n",
      "Accuracy at epoch 23: {'accuracy': 0.8453608155250549, 'precision': 0.692307710647583, 'recall': 0.44999998807907104, 'f1_score': 0.5454545617103577}\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:42<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Loss: 1.3689\n",
      "Accuracy at epoch 24: {'accuracy': 0.8041236996650696, 'precision': 1.0, 'recall': 0.05000000074505806, 'f1_score': 0.095238097012043}\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:42<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Loss: 0.3466\n",
      "Accuracy at epoch 25: {'accuracy': 0.8659793734550476, 'precision': 0.7333333492279053, 'recall': 0.550000011920929, 'f1_score': 0.6285714507102966}\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:42<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Loss: 1.3658\n",
      "Accuracy at epoch 26: {'accuracy': 0.6082473993301392, 'precision': 0.3333333432674408, 'recall': 0.8999999761581421, 'f1_score': 0.4864864945411682}\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:42<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Loss: 1.4587\n",
      "Accuracy at epoch 27: {'accuracy': 0.8350515365600586, 'precision': 0.8333333134651184, 'recall': 0.25, 'f1_score': 0.38461539149284363}\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Loss: 0.3674\n",
      "Accuracy at epoch 28: {'accuracy': 0.8556700944900513, 'precision': 0.7142857313156128, 'recall': 0.5, 'f1_score': 0.5882353186607361}\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Loss: 0.9154\n",
      "Accuracy at epoch 29: {'accuracy': 0.8247422575950623, 'precision': 0.5882353186607361, 'recall': 0.5, 'f1_score': 0.5405405163764954}\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Loss: 1.0470\n",
      "Accuracy at epoch 30: {'accuracy': 0.7835051417350769, 'precision': 0.47826087474823, 'recall': 0.550000011920929, 'f1_score': 0.5116279125213623}\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Loss: 0.3210\n",
      "Accuracy at epoch 31: {'accuracy': 0.8247422575950623, 'precision': 0.5789473652839661, 'recall': 0.550000011920929, 'f1_score': 0.5641025900840759}\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Loss: 1.6607\n",
      "Accuracy at epoch 32: {'accuracy': 0.8144329786300659, 'precision': 0.5714285969734192, 'recall': 0.4000000059604645, 'f1_score': 0.47058823704719543}\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:42<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Loss: 0.4748\n",
      "Accuracy at epoch 33: {'accuracy': 0.7938144207000732, 'precision': 0.5, 'recall': 0.6000000238418579, 'f1_score': 0.5454545617103577}\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Loss: 0.3933\n",
      "Accuracy at epoch 34: {'accuracy': 0.8144329786300659, 'precision': 0.5555555820465088, 'recall': 0.5, 'f1_score': 0.5263158082962036}\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Loss: 0.2487\n",
      "Accuracy at epoch 35: {'accuracy': 0.8247422575950623, 'precision': 0.5789473652839661, 'recall': 0.550000011920929, 'f1_score': 0.5641025900840759}\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100], Loss: 0.2993\n",
      "Accuracy at epoch 36: {'accuracy': 0.8041236996650696, 'precision': 0.52173912525177, 'recall': 0.6000000238418579, 'f1_score': 0.5581395626068115}\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Loss: 0.1277\n",
      "Accuracy at epoch 37: {'accuracy': 0.6907216310501099, 'precision': 0.3863636255264282, 'recall': 0.8500000238418579, 'f1_score': 0.53125}\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100], Loss: 1.4723\n",
      "Accuracy at epoch 38: {'accuracy': 0.7938144207000732, 'precision': 0.5, 'recall': 0.6499999761581421, 'f1_score': 0.5652173757553101}\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Loss: 0.4461\n",
      "Accuracy at epoch 39: {'accuracy': 0.8453608155250549, 'precision': 0.6315789222717285, 'recall': 0.6000000238418579, 'f1_score': 0.6153846383094788}\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Loss: 1.1612\n",
      "Accuracy at epoch 40: {'accuracy': 0.7938144207000732, 'precision': 0.5, 'recall': 0.75, 'f1_score': 0.6000000238418579}\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Loss: 0.8844\n",
      "Accuracy at epoch 41: {'accuracy': 0.7319587469100952, 'precision': 0.38461539149284363, 'recall': 0.5, 'f1_score': 0.43478259444236755}\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100], Loss: 0.4063\n",
      "Accuracy at epoch 42: {'accuracy': 0.8453608155250549, 'precision': 0.6666666865348816, 'recall': 0.5, 'f1_score': 0.5714285969734192}\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Loss: 0.1340\n",
      "Accuracy at epoch 43: {'accuracy': 0.8350515365600586, 'precision': 0.625, 'recall': 0.5, 'f1_score': 0.5555555820465088}\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100], Loss: 0.9983\n",
      "Accuracy at epoch 44: {'accuracy': 0.8453608155250549, 'precision': 0.6470588445663452, 'recall': 0.550000011920929, 'f1_score': 0.5945945978164673}\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Loss: 0.2831\n",
      "Accuracy at epoch 45: {'accuracy': 0.8041236996650696, 'precision': 0.52173912525177, 'recall': 0.6000000238418579, 'f1_score': 0.5581395626068115}\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:39<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100], Loss: 0.9437\n",
      "Accuracy at epoch 46: {'accuracy': 0.8247422575950623, 'precision': 0.5714285969734192, 'recall': 0.6000000238418579, 'f1_score': 0.5853658318519592}\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:39<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100], Loss: 0.1885\n",
      "Accuracy at epoch 47: {'accuracy': 0.8041236996650696, 'precision': 0.5185185074806213, 'recall': 0.699999988079071, 'f1_score': 0.5957446694374084}\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:39<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Loss: 0.3126\n",
      "Accuracy at epoch 48: {'accuracy': 0.8041236996650696, 'precision': 0.5161290168762207, 'recall': 0.800000011920929, 'f1_score': 0.6274510025978088}\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:39<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Loss: 0.8947\n",
      "Accuracy at epoch 49: {'accuracy': 0.8556700944900513, 'precision': 0.6875, 'recall': 0.550000011920929, 'f1_score': 0.6111111044883728}\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:42<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100], Loss: 0.3324\n",
      "Accuracy at epoch 50: {'accuracy': 0.7835051417350769, 'precision': 0.47826087474823, 'recall': 0.550000011920929, 'f1_score': 0.5116279125213623}\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:42<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100], Loss: 0.0354\n",
      "Accuracy at epoch 51: {'accuracy': 0.7525773048400879, 'precision': 0.44999998807907104, 'recall': 0.8999999761581421, 'f1_score': 0.6000000238418579}\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:40<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Loss: 1.7011\n",
      "Accuracy at epoch 52: {'accuracy': 0.8453608155250549, 'precision': 0.692307710647583, 'recall': 0.44999998807907104, 'f1_score': 0.5454545617103577}\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Loss: 1.4700\n",
      "Accuracy at epoch 53: {'accuracy': 0.8453608155250549, 'precision': 0.692307710647583, 'recall': 0.44999998807907104, 'f1_score': 0.5454545617103577}\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:40<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100], Loss: 0.1105\n",
      "Accuracy at epoch 54: {'accuracy': 0.8247422575950623, 'precision': 0.5789473652839661, 'recall': 0.550000011920929, 'f1_score': 0.5641025900840759}\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Loss: 1.1755\n",
      "Accuracy at epoch 55: {'accuracy': 0.3814432919025421, 'precision': 0.25, 'recall': 1.0, 'f1_score': 0.4000000059604645}\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100], Loss: 0.2135\n",
      "Accuracy at epoch 56: {'accuracy': 0.8350515365600586, 'precision': 0.6666666865348816, 'recall': 0.4000000059604645, 'f1_score': 0.5}\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:40<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Loss: 0.0767\n",
      "Accuracy at epoch 57: {'accuracy': 0.8556700944900513, 'precision': 0.6875, 'recall': 0.550000011920929, 'f1_score': 0.6111111044883728}\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:40<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100], Loss: 0.6095\n",
      "Accuracy at epoch 58: {'accuracy': 0.8453608155250549, 'precision': 0.6470588445663452, 'recall': 0.550000011920929, 'f1_score': 0.5945945978164673}\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100], Loss: 0.1875\n",
      "Accuracy at epoch 59: {'accuracy': 0.8350515365600586, 'precision': 0.6428571343421936, 'recall': 0.44999998807907104, 'f1_score': 0.529411792755127}\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Loss: 0.4723\n",
      "Accuracy at epoch 60: {'accuracy': 0.5773195624351501, 'precision': 0.32203391194343567, 'recall': 0.949999988079071, 'f1_score': 0.4810126721858978}\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:40<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Loss: 0.2292\n",
      "Accuracy at epoch 61: {'accuracy': 0.8350515365600586, 'precision': 0.625, 'recall': 0.5, 'f1_score': 0.5555555820465088}\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:39<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100], Loss: 0.4672\n",
      "Accuracy at epoch 62: {'accuracy': 0.8144329786300659, 'precision': 0.5357142686843872, 'recall': 0.75, 'f1_score': 0.625}\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:38<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Loss: 0.0379\n",
      "Accuracy at epoch 63: {'accuracy': 0.8453608155250549, 'precision': 0.6666666865348816, 'recall': 0.5, 'f1_score': 0.5714285969734192}\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60/60 [00:39<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100], Loss: 0.9037\n",
      "Accuracy at epoch 64: {'accuracy': 0.8453608155250549, 'precision': 0.6470588445663452, 'recall': 0.550000011920929, 'f1_score': 0.5945945978164673}\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 4/60 [00:01<00:25,  2.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 50\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# print(\"ti\",tensor_images.shape)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# print(\"BBOX:\", bboxes.shape)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# print(\"MASKS:\", masks.shape)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# print(\"POSES:\", poses.shape)\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# print(\"INTENTS:\", intents.shape)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 50\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m tensor_images\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#print(\"TRAIN out vs intents\",outputs,intents)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 18\u001b[0m, in \u001b[0;36mPedestrianIntentModel.forward\u001b[1;34m(self, cropped_images, bboxes, masks, poses)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, cropped_images, bboxes, masks, poses):\n\u001b[1;32m---> 18\u001b[0m     vision_out_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_branch_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcropped_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     vision_out_global \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_branch_global(masks)\n\u001b[0;32m     20\u001b[0m     non_vision_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_vision_branch(poses, bboxes)\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 22\u001b[0m, in \u001b[0;36mVisionBranchLocal.forward\u001b[1;34m(self, cropped_images)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):            \n\u001b[0;32m     21\u001b[0m     img \u001b[38;5;241m=\u001b[39m cropped_images[i]            \n\u001b[1;32m---> 22\u001b[0m     vgg_feat_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvgg16\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     pooled_feat_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(vgg_feat_img)  \u001b[38;5;66;03m# Applica il pooling\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     vgg_feat_img \u001b[38;5;241m=\u001b[39m pooled_feat_img\u001b[38;5;241m.\u001b[39mview(pooled_feat_img\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten features\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filippo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:791\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    790\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# free the memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Placeholder tensor for empty poses\n",
    "pose_placeholder = torch.zeros((36,), dtype=torch.float32) \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    model.train()\n",
    "\n",
    "    for tensor_images, bboxes, masks, poses, intents in tqdm(train_loader):\n",
    "        #case when there are no poses in the frame \n",
    "        # if len(poses) == 0:\n",
    "        #     print(\"Empty poses detected, adding placeholder tensor.\")\n",
    "        #     poses = [pose_placeholder for _ in range(len(tensor_images))]\n",
    "        #     poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        #     #poses = poses.squeeze(0)\n",
    "        #     poses = poses.view(len(tensor_images), -1, 36).permute(1,0,2)  # Reshape to (batch_size, numeroFrames, 36)\n",
    "        # else:\n",
    "        #     #poses = [torch.tensor(p, dtype=torch.float32) for p in poses]\n",
    "        #     poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        #     poses = poses.squeeze(0)\n",
    "        # poses = [pose_placeholder for _ in range(len(tensor_images))]\n",
    "        # poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        poses = poses.squeeze(0)\n",
    "        #poses = poses.squeeze(0)\n",
    "        poses = poses.view(len(tensor_images), -1, 36).permute(1,0,2)  # Reshape to (batch_size, numeroFrames, 36)\n",
    "        #print(\"TRAINING:\")\n",
    "        #print(\"poses\",len(poses))\n",
    "        # Convert poses to tensor and reshape\n",
    "        #print(\"poses shape:\",poses.shape)   \n",
    "\n",
    "        # Move tensors to device\n",
    "        tensor_images = torch.stack(tensor_images, dim=1).squeeze(0).permute(0, 1, 2,3).to(device)  # Converte la lista di immagini in un tensor\n",
    "        masks = torch.stack(masks,dim=1).squeeze(0).float().to(device)  # Converte la lista di maschere in un tensor\n",
    "        bboxes = bboxes.to(device)\n",
    "        poses = poses.to(device)\n",
    "        intents = intents.squeeze(0)[0].to(device)\n",
    "        # print(\"ti\",tensor_images.shape)\n",
    "        # print(\"BBOX:\", bboxes.shape)\n",
    "        # print(\"MASKS:\", masks.shape)\n",
    "        # print(\"POSES:\", poses.shape)\n",
    "        # print(\"INTENTS:\", intents.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(tensor_images, bboxes, masks, poses)\n",
    "        tensor_images.cpu()\n",
    "        #print(\"TRAIN out vs intents\",outputs,intents)\n",
    "        masks.cpu()\n",
    "        bboxes.cpu()\n",
    "        poses.cpu()\n",
    "        #print(outputs)\n",
    "        #outputs = torch.argmax((outputs))\n",
    "        loss = criterion(outputs, intents.float())\n",
    "        loss.backward()\n",
    "        #for name, param in model.named_parameters():\n",
    "            #if param.grad is not None:\n",
    "                #print(f\"Gradiente di {name}: {param.grad.norm()}\")\n",
    "\n",
    "        optimizer.step()\n",
    "        del tensor_images, masks, bboxes, poses, intents, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    with torch.no_grad():\n",
    "        print(f'Accuracy at epoch {epoch}: {evaluate_metrics(model, test_loader, device)}')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##########################################################################################################################################################################\n",
    "# # ONLY FOR A VERY GOOD SETUP (not for our poor gpu :( )) \n",
    "# # DIVIDE A SEQUENCE INTO GROUPS OF FRMAES, FOR EACH GROUP MAKE A PREDICTION AND FINALLY MAKE A VOTING AMONG THE PREDICTIONS FOR GETTING THE PREVISION OF THE SEQUENCE\n",
    "\n",
    "\n",
    "# num_epochs = 100\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "#     model.train()\n",
    "\n",
    "#     for tensor_images, bboxes, masks, poses, intents in tqdm(train_loader):\n",
    "#         poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "#         poses = poses.squeeze(0)\n",
    "#         poses = poses.view(len(tensor_images), -1, 36).permute(1,0,2)  # Reshape to (batch_size, numeroFrames, 36)\n",
    "\n",
    "#         # Move tensors to device\n",
    "#         tensor_images = torch.stack(tensor_images, dim=1).squeeze(0).permute(0, 1, 2,3).to(device) # Converte la lista di immagini in un tensor\n",
    "#         masks = torch.stack(masks,dim=1).squeeze(0).float().to(device)  # Converte la lista di maschere in un tensor\n",
    "#         bboxes = bboxes.to(device)\n",
    "#         poses = poses.to(device)\n",
    "#         intents = intents.squeeze(0)[0].to(device)\n",
    "\n",
    "\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs_list = []\n",
    "#         num_frames = 10\n",
    "#         sub_tensors = tensor_images.unfold(0, num_frames, num_frames)\n",
    "#         sub_tensors = sub_tensors.contiguous().view(-1, num_frames, 3, 224, 224)\n",
    "#         tensor_list = [sub_tensor for sub_tensor in sub_tensors]\n",
    "#         for tensor in tensor_list:\n",
    "#             output = model(tensor, bboxes, masks, poses)\n",
    "#             outputs_list.append(output.cpu())\n",
    "\n",
    "\n",
    "#             del tensor,output\n",
    "#             torch.cuda.empty_cache()\n",
    "#         print(\"hello\")\n",
    "#         outputs = sum(outputs_list)/len(outputs_list)\n",
    "#         outputs_list.clear()\n",
    "\n",
    "#         loss = criterion(outputs.to(device), intents.float())\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         del tensor_images, masks, bboxes, poses, intents, outputs\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "#     with torch.no_grad():\n",
    "#         print(f'Accuracy at epoch {epoch}: {evaluate_metrics(model, test_loader, device)}')\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
