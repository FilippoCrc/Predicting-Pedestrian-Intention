{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaad_data import JAAD\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import network\n",
    "import openpose\n",
    "from openpose import model\n",
    "from openpose import util\n",
    "from openpose.body import Body\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchmetrics\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JAAD_PATH = '../JAAD'\n",
    "DEEPLAB_PATH = '../best_deeplabv3plus_resnet101_cityscapes_os16.pth'\n",
    "SUBSET_PATH = '../subset2'\n",
    "#SUBSET_BALANCED ='../subset2'\n",
    "\n",
    "RESULTS_MASK_SUB = '../masks_results_sub.pkl'\n",
    "RESULTS_MASK_BIG = '../masks_results_big.pkl'\n",
    "RESULTS_MASK_BIG_TEST = '../masks_results_big_test.pkl'\n",
    "RESULTS_MASK_SUB_TEST = '../masks_results_sub_test.pkl'\n",
    "\n",
    "RESULTS_POSE_BIG = '../pose_results_big.pkl'\n",
    "RESULTS_POSE_SUB = '../pose_results_sub.pkl'\n",
    "RESULTS_POSE_BIG_TEST = '../pose_results_big_test.pkl'\n",
    "RESULTS_POSE_SUB_TEST = '../pose_results_sub_test.pkl'\n",
    "\n",
    "POSE_PATH = '../body_pose_model.pth'   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG: \n",
    "RUN = False\n",
    "BIG =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BIG:\n",
    "    MASK_CMD = RESULTS_MASK_BIG\n",
    "    POSE_CMD = RESULTS_POSE_BIG\n",
    "    MASK_CMD_TEST = RESULTS_MASK_BIG_TEST\n",
    "    POSE_CMD_TEST = RESULTS_POSE_BIG_TEST\n",
    "    DT_CMD = JAAD_PATH\n",
    "else:\n",
    "    MASK_CMD = RESULTS_MASK_SUB\n",
    "    POSE_CMD = RESULTS_POSE_SUB\n",
    "    MASK_CMD_TEST = RESULTS_MASK_SUB_TEST\n",
    "    POSE_CMD_TEST = RESULTS_POSE_SUB_TEST\n",
    "    DT_CMD = SUBSET_PATH\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stampa una sequenza di informazioni del dataset\n",
    "\n",
    "#jaad_dt.generate_database() # in particolare, questo stampa un sacco di informazioni come borse vestiti semafori ecc\n",
    "#jaad_dt.get_data_stats() # mentre questo stampa informazioni sui numeri di frame, video, pedoni, bbox ecc\n",
    "#print('Train sequences:', len(seq_train['image'][0][0]))  # stampa la lunghezza della sequenza di training selezionata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Jaad path does not exist: ../subset2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the JAAD dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m jaad_dt \u001b[38;5;241m=\u001b[39m \u001b[43mJAAD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../subset2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m data_opts \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfstride\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#'subset': 'high_visibility',\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m seq_train \u001b[38;5;241m=\u001b[39m jaad_dt\u001b[38;5;241m.\u001b[39mgenerate_data_trajectory_sequence(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_opts)  \n",
      "File \u001b[1;32mc:\\Users\\Filippo\\Documents\\VSCode\\Pedestrian_Intention\\Predicting-Pedestrian-Intention\\jaad_data.py:64\u001b[0m, in \u001b[0;36mJAAD.__init__\u001b[1;34m(self, data_path, regen_pkl)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Paths\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jaad_path \u001b[38;5;241m=\u001b[39m data_path \u001b[38;5;28;01mif\u001b[39;00m data_path \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_default_path()\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jaad_path), \\\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJaad path does not exist: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jaad_path)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_split_ids_path \u001b[38;5;241m=\u001b[39m join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jaad_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_ids\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_annotation_path \u001b[38;5;241m=\u001b[39m join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jaad_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Jaad path does not exist: ../subset2"
     ]
    }
   ],
   "source": [
    "# Load the JAAD dataset\n",
    "jaad_dt = JAAD(data_path='../subset2')\n",
    "\n",
    "data_opts = {\n",
    "    'fstride': 15,\n",
    "    #'subset': 'high_visibility',\n",
    "    'sample_type': 'all'\n",
    "}\n",
    "\n",
    "seq_train = jaad_dt.generate_data_trajectory_sequence('train', **data_opts)  \n",
    "seq_test = jaad_dt.generate_data_trajectory_sequence('test', **data_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((seq_train['image']))\n",
    "print((seq_test['image'][1]))\n",
    "# print(len(seq_train['bbox']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasformazioni per le immaginin che vengono usate nel local context e nel pose extractor\n",
    "transform_lc = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL CONTEXT EXTRACTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    deeplab_model = network.modeling.__dict__['deeplabv3plus_resnet101'](num_classes=19)\n",
    "    deeplab_model.load_state_dict(torch.load(DEEPLAB_PATH)['model_state'])\n",
    "    deeplab_model.to(device)\n",
    "    deeplab_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trasformazioni che vengono usate dentro global context (modifica l'input prima che vada in deeplab)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Ridimensiona le immagini a 512x512\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#processa output deeplab\n",
    "GC_trans = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ridimensiona le immagini a 256x256\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_mask(image_path, model, preprocess):\n",
    "    \"\"\" funzione che prende in input le path delle imagini, il modello e la funzione di preprocessamento \n",
    "    e restituisce la maschera segmentata dell'immagine resizata a 224x224\"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = preprocess(input_image).to(device)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Create a batch with a single image\n",
    "    \n",
    "    # Pass the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    # Check if output is a tensor or a dictionary\n",
    "    if isinstance(output, dict):\n",
    "        output = output['out'][0]\n",
    "    elif isinstance(output, torch.Tensor):\n",
    "        output = output[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected output type: {type(output)}\")\n",
    "    \n",
    "    # Convert the output to a mask\n",
    "    output_predictions = output.argmax(0)\n",
    "    \n",
    "    # Aggiungi una dimensione batch e canale alla maschera per il ridimensionamento\n",
    "    output_predictions = output_predictions.unsqueeze(0).unsqueeze(0).float()\n",
    "    #print(output_predictions.shape)\n",
    "    tr = transforms.ToPILImage()\n",
    "    pic = tr(output_predictions.squeeze(1))\n",
    "    pic= pic.convert(\"RGB\")\n",
    "    resized_mask = GC_trans(pic)\n",
    "    #print(resized_mask.shape)\n",
    "    # resized_maskk = tr(resized_mask)\n",
    "    # plt.imshow(resized_maskk)\n",
    "    # plt.show()\n",
    "    return resized_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_frames(seq_train, model, preprocess):\n",
    "    \"\"\"funzione che prende in input la sequenza di training, il modello e la funzione di preprocessamento, restituisce una\n",
    "    lista di segmentation mask per ogni frame di ogni video della sequenza di training\"\"\"\n",
    "    \n",
    "    all_masks = []\n",
    "    for video_frames in tqdm(seq_train['image'], desc=\"Processing videos\"):\n",
    "        video_masks = []\n",
    "\n",
    "        for frame_path in tqdm(video_frames, desc=\"Processing frames\", leave=False):\n",
    "            mask = get_segmentation_mask(frame_path, model, preprocess)\n",
    "            #visualize_mask(frame_path, mask)\n",
    "            video_masks.append(mask)\n",
    "        all_masks.append(video_masks)\n",
    "    \n",
    "    return all_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza la maschera semantica\n",
    "def visualize_mask(image_path, mask):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((256, 256))  # Ridimensiona per la visualizzazione\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap='jet')\n",
    "    plt.title(\"Semantic Mask\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    all_video_masks = process_video_frames(seq_train, deeplab_model, train_transforms)\n",
    "    seq_train['masks'] = all_video_masks\n",
    "    all_video_masks_test = process_video_frames(seq_test, deeplab_model, train_transforms)\n",
    "    seq_test['masks'] = all_video_masks_test\n",
    "    del deeplab_model\n",
    "    torch.cuda.empty_cache()\n",
    "    # Apri il file in modalità scrittura binaria e salva il dizionario\n",
    "    with open(MASK_CMD, 'wb') as f:\n",
    "        pickle.dump(seq_train['masks'], f)\n",
    "    with open(MASK_CMD_TEST, 'wb') as f:\n",
    "        pickle.dump(seq_test['masks'], f)\n",
    "else:\n",
    "    #recover data:\n",
    "    with open(MASK_CMD, 'rb') as f:\n",
    "        seq_train['masks'] = pickle.load(f)\n",
    "    with open(MASK_CMD_TEST, 'rb') as f:\n",
    "        seq_test['masks'] = pickle.load(f)\n",
    "    # Verifica che i risultati siano stati caricati correttamente\n",
    "    #print(seq_train['masks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOCAL CONTEXT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_cv2(img, bbox):\n",
    "    \"\"\" funzione che croppa i frames sul bounding boxes, le imagini sono in formato cv2\"\"\"\n",
    "    \n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return img[int(y1):int(y2), int(x1):int(x2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trasformation for the local context's images, enhance the quality of the images by appling gaussian filter, unsharp mask e bilateral filter\"\"\"\n",
    "all_images = []\n",
    "for i in tqdm(range(len(seq_train['image'])), desc=\"Processing videos\"):\n",
    "    aux_list = []\n",
    "    for j in tqdm(range(len(seq_train['image'][i])), desc=\"Processing frames\", leave=False):\n",
    "        \n",
    "        # Open the images from the paths\n",
    "        img_path = seq_train['image'][i][j]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        \n",
    "        bbox = seq_train['bbox'][i][j]\n",
    "        # Compute local context cropping images around the bboxes\n",
    "        cropped_images = crop_image_cv2(img, bbox)\n",
    "        \n",
    "        # Enhance the image using various filters\n",
    "        # blurred_image = cv2.GaussianBlur(cropped_images, (5, 5), 0)\n",
    "\n",
    "        # sharpness = 1.5  # Sharpness factor\n",
    "        # blurred_for_sharp = cv2.GaussianBlur(blurred_image, (0, 0), 5)\n",
    "        # sharpened_image = cv2.addWeighted(blurred_image, 1.0 + sharpness, blurred_for_sharp, -sharpness, 0)\n",
    "\n",
    "        # # Noise reduction using bilateral filter\n",
    "        #denoised_image = cv2.bilateralFilter(cropped_images, 9, 75, 75)\n",
    "        final_image = cv2.resize(cropped_images, (224, 224))\n",
    "        aux_list.append(final_image)\n",
    "    all_images.append(aux_list)\n",
    "\n",
    "# print(type(all_images))\n",
    "# print(all_images[0][0])\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(all_images[0][0])\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Original (Denormalized) Image\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((all_images[0][0].shape))\n",
    "plt.imshow(all_images[0][0])\n",
    "plt.axis('off')\n",
    "plt.title(\"Original (Denormalized) Image\")\n",
    "plt.show()\n",
    "print((seq_train['image'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_test = []\n",
    "for i in tqdm(range(len(seq_test['image'])), desc=\"Processing videos\"):\n",
    "    aux_list = []\n",
    "    for j in tqdm(range(len(seq_test['image'][i])), desc=\"Processing frames\", leave=False):\n",
    "        \n",
    "        # Open the images from the paths\n",
    "        img_path = seq_test['image'][i][j]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        \n",
    "        bbox = seq_test['bbox'][i][j]\n",
    "        # Compute local context cropping images around the bboxes\n",
    "        cropped_images = crop_image_cv2(img, bbox)\n",
    "        \n",
    "        # # Enhance the image using various filters\n",
    "        # blurred_image = cv2.GaussianBlur(cropped_images, (5, 5), 0)\n",
    "\n",
    "        # sharpness = 1.5  # Sharpness factor\n",
    "        # blurred_for_sharp = cv2.GaussianBlur(blurred_image, (0, 0), 5)\n",
    "        # sharpened_image = cv2.addWeighted(blurred_image, 1.0 + sharpness, blurred_for_sharp, -sharpness, 0)\n",
    "\n",
    "        # Noise reduction using bilateral filter\n",
    "        #denoised_image = cv2.bilateralFilter(cropped_images, 9, 75, 75)\n",
    "        final_image = cv2.resize(cropped_images, (224, 224))\n",
    "        aux_list.append(final_image)\n",
    "    all_images_test.append(aux_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((all_images_test[0][0].shape))\n",
    "plt.imshow(all_images_test[0][0])\n",
    "plt.axis('off')\n",
    "plt.title(\"Original (Denormalized) Image\")\n",
    "plt.show()\n",
    "print((seq_test['image'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSE KEYPOINTS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_sequence(frames, body_model):\n",
    "    \"\"\"funzione che prende in input i frames e il modello di openpose e restituisce una lista di tensori di pose \n",
    "    per ciascuna persona nel tempo, se non prende nessuna posa mette un placeholder\"\"\"\n",
    "    #print(frames)\n",
    "    pose_sequences = []  # Lista di pose per ciascuna persona nel tempo\n",
    "    pose_placeholder = torch.zeros((36,), dtype=torch.float32)\n",
    "    for frame in frames:\n",
    "        candidate, subset = body_model(frame)\n",
    "\n",
    "        # block to visualize the poses on the images by printing both\n",
    "        # canvas = copy.deepcopy(frame)\n",
    "        # canvas = util.draw_bodypose(canvas, candidate, subset)\n",
    "        # plt.imshow(canvas[:, :, [2, 1, 0]])  # OpenCV usa BGR, matplotlib usa RGB\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "\n",
    "        frame_poses = []\n",
    "        for person in subset:\n",
    "            if person[-1] >= 4:  # Almeno 4 punti chiave rilevati\n",
    "                pose = []\n",
    "                for i in range(18):\n",
    "                    if person[i] != -1:\n",
    "                        x, y = candidate[int(person[i])][:2]\n",
    "                    else:\n",
    "                        x, y = -1, -1  # Punti chiave mancanti\n",
    "                    pose.extend([x, y])\n",
    "                frame_poses.append(pose)\n",
    "        if not frame_poses:\n",
    "            frame_poses = [pose_placeholder.tolist()]\n",
    "        pose_sequences.append(frame_poses)\n",
    "\n",
    "    # Trasponi la lista di liste per ottenere le sequenze temporali per ciascuna persona\n",
    "    person_pose_sequences = list(map(list, zip(*pose_sequences)))\n",
    "    person_pose_sequences = [torch.tensor(person_poses, dtype=torch.float32) for person_poses in person_pose_sequences]\n",
    "    return person_pose_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if RUN:\n",
    "    body_model = Body(POSE_PATH)\n",
    "\n",
    "    # Caricamento dei frame e estrazione delle pose\n",
    "    all_poses = []\n",
    "    all_poses_test = []\n",
    "    # print(len(all_images))\n",
    "    # print(len(all_images[0]))\n",
    "    # print(len(all_images[0][0]))\n",
    "    # print(all_images[0][0].shape)\n",
    "    #itera tra i video e prendi i frames\n",
    "    for pics in tqdm(all_images, desc=\"Extracting poses from image sequences\"):\n",
    "        #print(len(pics))\n",
    "        pose_sequences = extract_pose_sequence(pics, body_model)\n",
    "        all_poses.append(pose_sequences)\n",
    "        #print(pose_sequences.shape)\n",
    "    # Aggiungi le pose estratte alla sequenza di allenamento\n",
    "    seq_train['poses'] = all_poses\n",
    "\n",
    "    for pics in tqdm(all_images_test, desc=\"Extracting poses from image sequences of test set\"):\n",
    "        pose_sequences = extract_pose_sequence(pics, body_model)\n",
    "        all_poses_test.append(pose_sequences)\n",
    "    # Aggiungi le pose estratte alla sequenza di allenamento\n",
    "    seq_test['poses'] = all_poses_test\n",
    "    del body_model\n",
    "    torch.cuda.empty_cache()\n",
    "    # Apri il file in modalità scrittura binaria e salva il dizionario\n",
    "\n",
    "    with open(POSE_CMD, 'wb') as f:\n",
    "        pickle.dump(seq_train['poses'], f)\n",
    "    with open(POSE_CMD_TEST, 'wb') as f:\n",
    "        pickle.dump(seq_test['poses'], f)\n",
    "else:\n",
    "    #recover data:\n",
    "    with open(POSE_CMD, 'rb') as f:\n",
    "        seq_train['poses'] = pickle.load(f)\n",
    "    with open(POSE_CMD_TEST, 'rb') as f:\n",
    "        seq_test['poses'] = pickle.load(f)\n",
    "    # Verifica che i risultati siano stati caricati correttamente\n",
    "    #print(seq_train['masks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((seq_train['poses']))\n",
    "print((seq_test['poses']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionBranchLocal(torch.nn.Module):\n",
    "    \"\"\"definizione del modello per il local context, prende in input le immagini croppate e restituisce un tensore,\n",
    "    le immagini croppate vengono fatte passare dentro una VGG16, una GRU e un attention block\"\"\"\n",
    "\n",
    "    def __init__(self, vgg16):\n",
    "        super(VisionBranchLocal, self).__init__()\n",
    "        self.vgg16 = vgg16\n",
    "        self.avgpool = torch.nn.AvgPool2d(kernel_size=14)  # Pooling layer con kernel 14x14\n",
    "        self.gru = torch.nn.GRU(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(256, 2)    # Fully connected layer\n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, cropped_images):\n",
    "        seq_len, c, h, w = cropped_images.size()\n",
    "        \n",
    "        # Estrai feature dalle immagini con VGG16\n",
    "        vgg_features = []\n",
    "        for i in range(seq_len):            \n",
    "\n",
    "            img = cropped_images[i]            \n",
    "            vgg_feat_img = self.vgg16.features(img)\n",
    "            pooled_feat_img = self.avgpool(vgg_feat_img)  # Applica il pooling\n",
    "            vgg_feat_img = pooled_feat_img.view(pooled_feat_img.size(0), -1)  # Flatten features\n",
    "            vgg_features.append(vgg_feat_img)\n",
    "        \n",
    "        vgg_features = torch.stack(vgg_features, dim=1).permute(2,1,0)\n",
    "\n",
    "        gru_out, _ = self.gru(vgg_features)\n",
    "        attn_weights = torch.softmax(self.attn(gru_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n",
    "        \n",
    "       # print(\"SIZE vgg features:\",vgg_features.shape)\n",
    "       # print(\"SIZE context v local context:\",context_vector.shape)\n",
    "        #out = self.sigmoid(self.fc(gru_out[:, -1, :]))   \n",
    "        out = self.tanh((context_vector))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionBranchGlobal(torch.nn.Module):\n",
    "    \"\"\"definizione del modello per il global context, prende in input le maskere semantiche e restituisce un tensore,\n",
    "    le maskere semantiche vengono fatte passare dentro una VGG16, una GRU e un attention block\"\"\"\n",
    "\n",
    "    def __init__(self, vgg16):\n",
    "        super(VisionBranchGlobal, self).__init__()\n",
    "        self.vgg16 = vgg16\n",
    "        self.avgpool = torch.nn.AvgPool2d(kernel_size=14)  # Pooling layer con kernel 14x14\n",
    "        self.gru = torch.nn.GRU(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(256, 2)    # Fully connected layer\n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, masks):\n",
    "        seq_len = masks.size()[0]\n",
    "        #print(\"size forward:\",seq_len)\n",
    "        # Estrai feature dalle immagini con VGG16\n",
    "        vgg_features = []\n",
    "        for i in range(seq_len):            \n",
    "            img = masks[i]            \n",
    "            vgg_feat_img = self.vgg16.features(img)\n",
    "            pooled_feat_img = self.avgpool(vgg_feat_img)  # Applica il pooling\n",
    "            vgg_feat_img = pooled_feat_img.view(pooled_feat_img.size(0), -1)  # Flatten features\n",
    "            vgg_features.append(vgg_feat_img)\n",
    "        \n",
    "        vgg_features = torch.stack(vgg_features, dim=1).permute(2,1,0)\n",
    "\n",
    "        gru_out, _ = self.gru(vgg_features)\n",
    "        attn_scores = self.attn(gru_out)  # shape: (batch_size, seq_length, 1)\n",
    "       # print(\"SIZE attention scores GLO:\", attn_scores.shape)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)  # shape: (batch_size, seq_length, 1)\n",
    "        #print(\"SIZE attention weights GLO:\", attn_weights.shape)\n",
    "        \n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)  # shape: (batch_size, 256)\n",
    "        #print(\"SIZE context vector GLO:\", context_vector.shape)\n",
    "        \n",
    "        out = self.tanh((context_vector))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NVisionBranch(torch.nn.Module):\n",
    "    \"\"\"classe relativa al non-vision brach, prende in input le pose e le bbox in formato tensore, esse vengono fatte passare\n",
    "    dentro una GRU e un attention block, l'ordine influenza la prestazioni\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NVisionBranch, self).__init__()\n",
    "        self.gru = torch.nn.GRU(input_size=36, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.gru2 = torch.nn.GRU(input_size=256+4, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        #self.fc = torch.nn.Linear(256, 2)  \n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, poses,bbox):\n",
    "        gru_out, _ = self.gru(poses)\n",
    "        #print(\"SIZE outuyput gru posa:\",gru_out.shape)\n",
    "        #print(\"bbox:\",bbox.shape)\n",
    "        LP = torch.cat((gru_out,bbox),dim=-1)\n",
    "        #print(\"SIZE outuyput gru posa + bbox:\",LP.shape)\n",
    "        gru_out, _ = self.gru2(LP)\n",
    "       # print(\"SIZE output gru:\",gru_out.shape)\n",
    "\n",
    "        # Attention mechanism\n",
    "        #features = torch.stack([gru_out[:,i,:] for i in range(gru_out.size(1))], dim=1)\n",
    "        attn_scores = self.attn(gru_out)  # shape: (batch_size, seq_length, 1)\n",
    "       # print(\"SIZE attention scores:\", attn_scores.shape)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)  # shape: (batch_size, seq_length, 1)\n",
    "        #print(\"SIZE attention weights:\", attn_weights.shape)\n",
    "        \n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)  # shape: (batch_size, 256)\n",
    "        #print(\"SIZE context vector:\", context_vector.shape)\n",
    "        \n",
    "        #out = self.sigmoid(self.fc(gru_out[:, -1, :]))   \n",
    "        out = self.tanh(context_vector)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PedestrianIntentModel(torch.nn.Module):\n",
    "#     \"\"\"definizione del modello finale, prende in input gli output del vision brach e del non vision branch, viene fatta\n",
    "#     una concatenazione che in seguito passa dentro un attention e un fully connected layer, l'output è la predizione,\n",
    "#     (passa non passa)\"\"\"\n",
    "\n",
    "#     def __init__(self, vision_branch_local,vision_branch_global,non_vision_branch):\n",
    "#         super(PedestrianIntentModel, self).__init__()\n",
    "#         self.vision_branch_local = vision_branch_local\n",
    "#         self.vision_branch_global = vision_branch_global\n",
    "#         self.non_vision_branch = non_vision_branch\n",
    "#         self.attn = torch.nn.Linear(768, 768)  # Attention layer\n",
    "\n",
    "#         self.fc1 = torch.nn.Linear(768, 256) # Output: crossing or not crossing\n",
    "#         self.fc2 = torch.nn.Linear(256,1) # Output: crossing or not crossing\n",
    "\n",
    "#         self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, cropped_images, bboxes, masks, poses):\n",
    "#         vision_out_local = self.vision_branch_local(cropped_images)\n",
    "#         vision_out_global = self.vision_branch_global(masks)\n",
    "#         non_vision_out = self.non_vision_branch(poses, bboxes)\n",
    "\n",
    "#         vision_out = torch.cat((vision_out_local, vision_out_global), dim=-1)\n",
    "#         final_fusion = torch.cat((vision_out, non_vision_out), dim=-1)\n",
    "\n",
    "#         attn_scores = self.attn(final_fusion)\n",
    "#         attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "#         context_vector = torch.sum(attn_weights * final_fusion, dim=0)\n",
    "\n",
    "#         logits = self.fc2(self.fc1(context_vector))\n",
    "#         probs = self.sigmoid(logits)\n",
    "#         return probs\n",
    "\n",
    "#     def predict(self, cropped_images, bboxes, masks, poses, threshold=0.5):\n",
    "#         \"\"\"\n",
    "#         Metodo per ottenere la predizione finale in termini di classe (0 o 1).\n",
    "#         \"\"\"\n",
    "#         probs = self.forward(cropped_images, bboxes, masks, poses)\n",
    "#         predictions = (probs >= threshold).float()\n",
    "#         return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedestrianIntentModel(torch.nn.Module):\n",
    "    \"\"\"definizione del modello finale, prende in input gli output del vision brach e del non vision branch, viene fatta\n",
    "    una concatenazione che in seguito passa dentro un attention e un fully connected layer, l'output è la predizione,\n",
    "    (passa non passa)\"\"\"\n",
    "\n",
    "    def __init__(self, vision_branch_local,vision_branch_global,non_vision_branch):\n",
    "        super(PedestrianIntentModel, self).__init__()\n",
    "        self.vision_branch_local = vision_branch_local\n",
    "        self.vision_branch_global = vision_branch_global\n",
    "        self.non_vision_branch = non_vision_branch\n",
    "        self.attn = torch.nn.Linear(768, 768)  # Attention layer\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(768, 256) # Output: crossing or not crossing\n",
    "        self.fc2 = torch.nn.Linear(256,1) # Output: crossing or not crossing\n",
    "\n",
    "        #self.softmax = torch.nn.Softamx()\n",
    "        \n",
    "    def forward(self, cropped_images, bboxes, masks, poses):\n",
    "        vision_out_local = self.vision_branch_local(cropped_images)\n",
    "        vision_out_global = self.vision_branch_global(masks)\n",
    "        non_vision_out = self.non_vision_branch(poses, bboxes)\n",
    "\n",
    "        vision_out = torch.cat((vision_out_local, vision_out_global), dim=-1)\n",
    "        final_fusion = torch.cat((vision_out, non_vision_out), dim=-1)\n",
    "\n",
    "        attn_scores = self.attn(final_fusion)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        context_vector = torch.sum(attn_weights * final_fusion, dim=0)\n",
    "\n",
    "        out = (self.fc2(self.fc1(context_vector)))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_FeatureExtractor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGG16_FeatureExtractor, self).__init__()\n",
    "        self.features = torch.nn.Sequential(*list(vgg16.features.children())[:24]) # block4_pool è il 23° livello\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello VGG19 pre-addestrato\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "#cut the model at the 24th layer:\n",
    "vgg16_fe = VGG16_FeatureExtractor()\n",
    "vgg16_fe\n",
    "\n",
    "\n",
    "# define the models of each branches\n",
    "model_local = VisionBranchLocal(vgg16_fe).to(device)\n",
    "model_global = VisionBranchGlobal(vgg16_fe).to(device)\n",
    "model_non_vision = NVisionBranch().to(device)\n",
    "model = PedestrianIntentModel(model_local,model_global,model_non_vision).to(device)\n",
    "#model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET & DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JAADDataset(Dataset):\n",
    "    \"\"\"definizione della classe per il custom dataset, prende in input la seq_train, le immagini e le trasformazioni,\n",
    "    restituisce il tensore delle immagini croppate, le bboxes, le maschere, le pose e la lables\"\"\"\n",
    "\n",
    "    def __init__(self, seq_data, all_images, transform=None):\n",
    "        self.seq_data = seq_data\n",
    "        self.all_images = all_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bbox_sequence = self.seq_data['bbox'][idx]\n",
    "        masks = self.seq_data['masks'][idx]\n",
    "        poses = self.seq_data['poses'][idx]\n",
    "        all_images = self.all_images[idx]\n",
    "        #open the images from the paths \n",
    "        #images = [Image.open(img_path).convert(\"RGB\") for img_path in img_paths]\n",
    "\n",
    "        #compute local context cropping images arroun the bboxes\n",
    "        #cropped_images = [crop_image(img, bbox) for img, bbox in zip(images, bbox_sequence)]\n",
    "        # #mask_paths = self.seq_data['masks'][idx]\n",
    "\n",
    "        #masks = [Image.fromarray(mask.numpy()) for mask in mask_paths]\n",
    "        if self.transform:\n",
    "            #images = [self.transform(img) for img in images]\n",
    "            tensor_images = [self.transform(img) for img in all_images]\n",
    "            #\n",
    "            #masks = [mask.Resize(224, 224) for mask in masks]\n",
    "        #print(seq_train['poses'][idx])\n",
    "        #print(len(poses))\n",
    "        bboxes = torch.tensor(self.seq_data['bbox'][idx], dtype=torch.float32)\n",
    "        intents = torch.tensor(self.seq_data['intent'][idx], dtype=torch.float32)\n",
    "        return  tensor_images, bboxes, masks, poses, intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JAADDataset(seq_train,all_images=all_images, transform=transform_lc)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataset = JAADDataset(seq_test,all_images=all_images_test, transform=transform_lc)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def evaluate_metrics(net, loader, device):\n",
    "#     acc = torchmetrics.Accuracy(num_classes=2, average='macro').to(device)\n",
    "#     precision = torchmetrics.Precision(num_classes=2, average='macro').to(device)\n",
    "#     recall = torchmetrics.Recall(num_classes=2, average='macro').to(device)\n",
    "#     f1_score = torchmetrics.F1Score(num_classes=2, average='macro').to(device)\n",
    "    \n",
    "#     net.eval()\n",
    "\n",
    "#     for tensor_images, bboxes, masks, poses, intents in loader:\n",
    "#         poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "#         poses = poses.squeeze(0)\n",
    "#         poses = poses.view(len(tensor_images), -1, 36).permute(1, 0, 2)  # Reshape to (batch_size, numeroFrames, 36)\n",
    "        \n",
    "#         # Move tensors to device\n",
    "#         tensor_images = torch.stack(tensor_images, dim=1).squeeze(0).permute(0, 1, 2, 3).to(device)  # Convert image list to tensor\n",
    "#         masks = torch.stack(masks, dim=1).squeeze(0).float().to(device)  # Convert mask list to tensor\n",
    "#         bboxes = bboxes.to(device)\n",
    "#         poses = poses.to(device)\n",
    "#         intents = intents.squeeze(0)[0].to(device)\n",
    "        \n",
    "#         ypred = net(tensor_images, bboxes, masks, poses)\n",
    "#         ypred = torch.softmax(ypred, dim=1)  # Apply softmax to the output\n",
    "        \n",
    "#         # Convert probabilities to class predictions using argmax\n",
    "#         ypred_class = torch.argmax(ypred, dim=1)\n",
    "        \n",
    "#         # Move tensors back to CPU and clean up memory\n",
    "#         tensor_images.cpu()\n",
    "#         masks.cpu()\n",
    "#         bboxes.cpu()\n",
    "#         poses.cpu()\n",
    "#         del tensor_images, bboxes, masks, poses\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "#         # Update metrics\n",
    "#         acc.update(ypred_class, intents)\n",
    "#         precision.update(ypred_class, intents)\n",
    "#         recall.update(ypred_class, intents)\n",
    "#         f1_score.update(ypred_class, intents)\n",
    "        \n",
    "#         intents.cpu()\n",
    "#         del intents\n",
    "#         torch.cuda.empty_cache()\n",
    "    \n",
    "#     metrics = {\n",
    "#         'accuracy': acc.compute().item(),\n",
    "#         'precision': precision.compute().item(),\n",
    "#         'recall': recall.compute().item(),\n",
    "#         'f1_score': f1_score.compute().item()\n",
    "#     }\n",
    "    \n",
    "#     # Reset metrics for the next evaluation\n",
    "#     acc.reset()\n",
    "#     precision.reset()\n",
    "#     recall.reset()\n",
    "#     f1_score.reset()\n",
    "    \n",
    "#     return metrics\n",
    "\n",
    "# # Example usage\n",
    "# # metrics = evaluate_metrics(net, loader, device)\n",
    "# # print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(net, loader, device):\n",
    "    acc = BinaryAccuracy().to(device)\n",
    "    precision = BinaryPrecision().to(device)\n",
    "    recall = BinaryRecall().to(device)\n",
    "    f1_score = BinaryF1Score().to(device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for tensor_images, bboxes, masks, poses, intents in loader:\n",
    "        poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        poses = poses.squeeze(0)\n",
    "        poses = poses.view(len(tensor_images), -1, 36).permute(1, 0, 2)  # Reshape to (batch_size, numeroFrames, 36)\n",
    "        \n",
    "        # Move tensors to device\n",
    "        tensor_images = torch.stack(tensor_images, dim=1).squeeze(0).permute(0, 1, 2, 3).to(device)  # Convert image list to tensor\n",
    "        masks = torch.stack(masks, dim=1).squeeze(0).float().to(device)  # Convert mask list to tensor\n",
    "        bboxes = bboxes.to(device)\n",
    "        poses = poses.to(device)\n",
    "        intents = intents.squeeze(0)[0].to(device)\n",
    "        \n",
    "        ypred = net(tensor_images, bboxes, masks, poses)\n",
    "        #print(\"pred vs intents\",ypred,intents)\n",
    "\n",
    "        # Move tensors back to CPU and clean up memory\n",
    "        tensor_images.cpu()\n",
    "        masks.cpu()\n",
    "        bboxes.cpu()\n",
    "        poses.cpu()\n",
    "        del tensor_images, bboxes, masks, poses\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update metrics\n",
    "        acc.update(ypred, intents)\n",
    "        precision.update(ypred, intents)\n",
    "        recall.update(ypred, intents)\n",
    "        f1_score.update(ypred, intents)\n",
    "        \n",
    "        intents.cpu()\n",
    "        del intents\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': acc.compute().item(),\n",
    "        'precision': precision.compute().item(),\n",
    "        'recall': recall.compute().item(),\n",
    "        'f1_score': f1_score.compute().item()\n",
    "    }\n",
    "    \n",
    "    # Reset metrics for the next evaluation\n",
    "    acc.reset()\n",
    "    precision.reset()\n",
    "    recall.reset()\n",
    "    f1_score.reset()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example usage\n",
    "# metrics = evaluate_metrics(net, loader, device)\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# free the memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Placeholder tensor for empty poses\n",
    "pose_placeholder = torch.zeros((36,), dtype=torch.float32) \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    model.train()\n",
    "\n",
    "    for tensor_images, bboxes, masks, poses, intents in tqdm(train_loader):\n",
    "        #case when there are no poses in the frame \n",
    "        # if len(poses) == 0:\n",
    "        #     print(\"Empty poses detected, adding placeholder tensor.\")\n",
    "        #     poses = [pose_placeholder for _ in range(len(tensor_images))]\n",
    "        #     poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        #     #poses = poses.squeeze(0)\n",
    "        #     poses = poses.view(len(tensor_images), -1, 36).permute(1,0,2)  # Reshape to (batch_size, numeroFrames, 36)\n",
    "        # else:\n",
    "        #     #poses = [torch.tensor(p, dtype=torch.float32) for p in poses]\n",
    "        #     poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        #     poses = poses.squeeze(0)\n",
    "        # poses = [pose_placeholder for _ in range(len(tensor_images))]\n",
    "        # poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        poses = poses.squeeze(0)\n",
    "        #poses = poses.squeeze(0)\n",
    "        poses = poses.view(len(tensor_images), -1, 36).permute(1,0,2)  # Reshape to (batch_size, numeroFrames, 36)\n",
    "        #print(\"TRAINING:\")\n",
    "        #print(\"poses\",len(poses))\n",
    "        # Convert poses to tensor and reshape\n",
    "        #print(\"poses shape:\",poses.shape)   \n",
    "\n",
    "        # Move tensors to device\n",
    "        tensor_images = torch.stack(tensor_images, dim=1).squeeze(0).permute(0, 1, 2,3).to(device)  # Converte la lista di immagini in un tensor\n",
    "        masks = torch.stack(masks,dim=1).squeeze(0).float().to(device)  # Converte la lista di maschere in un tensor\n",
    "        bboxes = bboxes.to(device)\n",
    "        poses = poses.to(device)\n",
    "        intents = intents.squeeze(0)[0].to(device)\n",
    "        # print(\"ti\",tensor_images.shape)\n",
    "        # print(\"BBOX:\", bboxes.shape)\n",
    "        # print(\"MASKS:\", masks.shape)\n",
    "        # print(\"POSES:\", poses.shape)\n",
    "        # print(\"INTENTS:\", intents.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(tensor_images, bboxes, masks, poses)\n",
    "        tensor_images.cpu()\n",
    "        #print(\"TRAIN out vs intents\",outputs,intents)\n",
    "        masks.cpu()\n",
    "        bboxes.cpu()\n",
    "        poses.cpu()\n",
    "        #print(outputs)\n",
    "        #outputs = torch.argmax((outputs))\n",
    "        loss = criterion(outputs, intents.float())\n",
    "        loss.backward()\n",
    "        #for name, param in model.named_parameters():\n",
    "            #if param.grad is not None:\n",
    "                #print(f\"Gradiente di {name}: {param.grad.norm()}\")\n",
    "\n",
    "        optimizer.step()\n",
    "        del tensor_images, masks, bboxes, poses, intents, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    with torch.no_grad():\n",
    "        print(f'Accuracy at epoch {epoch}: {evaluate_metrics(model, test_loader, device)}')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq_train['intent'])\n",
    "print(seq_train['poses'])\n",
    "print(len(seq_train['intent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq_test['intent'])\n",
    "print(len(seq_test['intent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq_train['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tensor_images, masks, bboxes, poses, intents\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
