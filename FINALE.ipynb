{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaad_data import JAAD\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import network\n",
    "import openpose\n",
    "from openpose import model\n",
    "from openpose import util\n",
    "from openpose.body import Body\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchmetrics\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JAAD_PATH = '../JAAD'\n",
    "DEEPLAB_PATH = '../best_deeplabv3plus_resnet101_cityscapes_os16.pth'\n",
    "SUBSET_PATH = '../subset2'\n",
    "\n",
    "RESULTS_MASK_SUB = '../masks_results_sub.pkl'\n",
    "RESULTS_MASK_BIG = '../masks_results_big.pkl'\n",
    "RESULTS_MASK_BIG_TEST = '../masks_results_big_test.pkl'\n",
    "RESULTS_MASK_SUB_TEST = '../masks_results_sub_test.pkl'\n",
    "\n",
    "RESULTS_POSE_BIG = '../pose_results_big.pkl'\n",
    "RESULTS_POSE_SUB = '../pose_results_sub.pkl'\n",
    "RESULTS_POSE_BIG_TEST = '../pose_results_big_test.pkl'\n",
    "RESULTS_POSE_SUB_TEST = '../pose_results_sub_test.pkl'\n",
    "\n",
    "POSE_PATH = '../body_pose_model.pth'   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG (change these params for changing the experiment): \n",
    "# BiG = True for big dataset, False for small dataset\n",
    "RUN = True\n",
    "BIG =False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BIG:\n",
    "    MASK_CMD = RESULTS_MASK_BIG\n",
    "    POSE_CMD = RESULTS_POSE_BIG\n",
    "    MASK_CMD_TEST = RESULTS_MASK_BIG_TEST\n",
    "    POSE_CMD_TEST = RESULTS_POSE_BIG_TEST\n",
    "    DT_CMD = JAAD_PATH\n",
    "else:\n",
    "    MASK_CMD = RESULTS_MASK_SUB\n",
    "    POSE_CMD = RESULTS_POSE_SUB\n",
    "    MASK_CMD_TEST = RESULTS_MASK_SUB_TEST\n",
    "    POSE_CMD_TEST = RESULTS_POSE_SUB_TEST\n",
    "    DT_CMD = SUBSET_PATH\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Generating action sequence data\n",
      "fstride: 15\n",
      "sample_type: all\n",
      "subset: default\n",
      "height_rng: [0, inf]\n",
      "squarify_ratio: 0\n",
      "data_split_type: default\n",
      "seq_type: intention\n",
      "min_track_size: 15\n",
      "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
      "kfold_params: {'num_folds': 5, 'fold': 1}\n",
      "---------------------------------------------------------\n",
      "Generating database for jaad\n",
      "jaad database loaded from c:\\Users\\jacop\\Documents\\ComputerVision\\subset2\\data_cache\\jaad_database.pkl\n",
      "---------------------------------------------------------\n",
      "Generating intention data\n",
      "Split: train\n",
      "Number of pedestrians: 185 \n",
      "Total number of samples: 60 \n",
      "---------------------------------------------------------\n",
      "Generating action sequence data\n",
      "fstride: 15\n",
      "sample_type: all\n",
      "subset: default\n",
      "height_rng: [0, inf]\n",
      "squarify_ratio: 0\n",
      "data_split_type: default\n",
      "seq_type: intention\n",
      "min_track_size: 15\n",
      "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
      "kfold_params: {'num_folds': 5, 'fold': 1}\n",
      "---------------------------------------------------------\n",
      "Generating database for jaad\n",
      "jaad database loaded from c:\\Users\\jacop\\Documents\\ComputerVision\\subset2\\data_cache\\jaad_database.pkl\n",
      "---------------------------------------------------------\n",
      "Generating intention data\n",
      "Split: test\n",
      "Number of pedestrians: 251 \n",
      "Total number of samples: 97 \n"
     ]
    }
   ],
   "source": [
    "# Load the JAAD dataset\n",
    "jaad_dt = JAAD(data_path='../subset2')\n",
    "\n",
    "data_opts = {\n",
    "    'fstride': 15,\n",
    "    'sample_type': 'all'\n",
    "}\n",
    "\n",
    "seq_train = jaad_dt.generate_data_trajectory_sequence('train', **data_opts)  \n",
    "seq_test = jaad_dt.generate_data_trajectory_sequence('test', **data_opts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL CONTEXT EXTRACTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    deeplab_model = network.modeling.__dict__['deeplabv3plus_resnet101'](num_classes=19)\n",
    "    deeplab_model.load_state_dict(torch.load(DEEPLAB_PATH)['model_state'])\n",
    "    deeplab_model.to(device)\n",
    "    deeplab_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trasformazioni che vengono usate dentro global context (modifica l'input prima che vada in deeplab)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Ridimensiona le immagini a 512x512\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#processa output deeplab\n",
    "GC_trans = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ridimensiona le immagini a 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza la maschera semantica\n",
    "def visualize_mask(image_path, mask):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    #image = image.resize((512, 512))  # Ridimensiona per la visualizzazione\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap='jet')\n",
    "    plt.title(\"Semantic Mask\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_mask(image_path, model, preprocess):\n",
    "    \"\"\" funzione che prende in input le path delle imagini, il modello e la funzione di preprocessamento \n",
    "    e restituisce la maschera segmentata dell'immagine resizata a 224x224\"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = preprocess(input_image).to(device)\n",
    "    input_batch = input_tensor.unsqueeze(0)  \n",
    "    \n",
    "    # Pass the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    # Check if output is a tensor or a dictionary (auxiliary control)\n",
    "    if isinstance(output, dict):\n",
    "        output = output['out'][0]\n",
    "    elif isinstance(output, torch.Tensor):\n",
    "        output = output[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected output type: {type(output)}\")\n",
    "    \n",
    "    # Convert the output to a mask\n",
    "    output_predictions = output.argmax(0)\n",
    "\n",
    "####################################################################################\n",
    "    #OPTIONAL FOR DEBUG AND PLOTTING:\n",
    "    # output_predictions_pic = output_predictions.clone().cpu()\n",
    "    # visualize_mask(image_path,output_predictions_pic) \n",
    "####################################################################################\n",
    "\n",
    "    # Fix dimensions for the mask and convert to float\n",
    "    output_predictions = output_predictions.unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "    #convert to RGB and resize to (224,224), because VGG want(3,224,224) as input\n",
    "    tr = transforms.ToPILImage()\n",
    "    pic = tr(output_predictions.squeeze(1))\n",
    "    pic= pic.convert(\"RGB\")\n",
    "    resized_mask = GC_trans(pic)\n",
    "\n",
    "    return resized_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_frames(seq_train, model, preprocess):\n",
    "    \"\"\"funzione che prende in input la sequenza di training, il modello e la funzione di preprocessamento, restituisce una\n",
    "    lista di segmentation mask per ogni frame di ogni video della sequenza di training\"\"\"\n",
    "    \n",
    "    all_masks = []\n",
    "    for video_frames in tqdm(seq_train['image'], desc=\"Processing videos\"):\n",
    "        video_masks = []\n",
    "\n",
    "        for frame_path in tqdm(video_frames, desc=\"Processing frames\", leave=False):\n",
    "            mask = get_segmentation_mask(frame_path, model, preprocess)\n",
    "            video_masks.append(mask)\n",
    "        all_masks.append(video_masks)\n",
    "    \n",
    "    return all_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    all_video_masks = process_video_frames(seq_train, deeplab_model, train_transforms)\n",
    "    seq_train['masks'] = all_video_masks\n",
    "    all_video_masks_test = process_video_frames(seq_test, deeplab_model, train_transforms)\n",
    "    seq_test['masks'] = all_video_masks_test\n",
    "\n",
    "    #cleaning for my poor gpu\n",
    "    del deeplab_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # save data in the .pkl files \n",
    "    with open(MASK_CMD, 'wb') as f:\n",
    "        pickle.dump(seq_train['masks'], f)\n",
    "    with open(MASK_CMD_TEST, 'wb') as f:\n",
    "        pickle.dump(seq_test['masks'], f)\n",
    "else:\n",
    "    #recover data:\n",
    "    with open(MASK_CMD, 'rb') as f:\n",
    "        seq_train['masks'] = pickle.load(f)\n",
    "    with open(MASK_CMD_TEST, 'rb') as f:\n",
    "        seq_test['masks'] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOCAL CONTEXT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasformazioni per le immaginin che vengono per il local context \n",
    "transform_lc = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_cv2(img, bbox):\n",
    "    \"\"\" funzione che croppa i frames sul bounding boxes, le imagini sono in formato cv2\"\"\"\n",
    "    \n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return img[int(y1):int(y2), int(x1):int(x2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 60/60 [00:38<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Trasformation for the local context's images, enhance the quality of the images by appling gaussian filter, unsharp mask e bilateral filter\"\"\"\n",
    "all_images = [] #list of the images\n",
    "for i in tqdm(range(len(seq_train['image'])), desc=\"Processing videos\"):\n",
    "    aux_list = []\n",
    "    for j in tqdm(range(len(seq_train['image'][i])), desc=\"Processing frames\", leave=False):\n",
    "        \n",
    "        # Open the images from the paths\n",
    "        img_path = seq_train['image'][i][j]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        \n",
    "        #get the bbox and crop arround it\n",
    "        bbox = seq_train['bbox'][i][j]\n",
    "        cropped_images = crop_image_cv2(img, bbox)\n",
    "        \n",
    "        # Resize to 224x224 (VGG input size)\n",
    "        final_image = cv2.resize(cropped_images, (224, 224))\n",
    "        aux_list.append(final_image)\n",
    "    all_images.append(aux_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 97/97 [01:02<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "#same of above, but for the test set\n",
    "all_images_test = []\n",
    "for i in tqdm(range(len(seq_test['image'])), desc=\"Processing videos\"):\n",
    "    aux_list = []\n",
    "    for j in tqdm(range(len(seq_test['image'][i])), desc=\"Processing frames\", leave=False):\n",
    "        \n",
    "        # Open the images from the paths\n",
    "        img_path = seq_test['image'][i][j]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "        #get the bbox and crop arround it\n",
    "        bbox = seq_test['bbox'][i][j]\n",
    "        cropped_images = crop_image_cv2(img, bbox)\n",
    "        \n",
    "        # Resize to 224x224 (VGG input size)\n",
    "        final_image = cv2.resize(cropped_images, (224, 224))\n",
    "        aux_list.append(final_image)\n",
    "    all_images_test.append(aux_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSE KEYPOINTS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_sequence(frames, body_model):\n",
    "    \"\"\"funzione che prende in input i frames e il modello di openpose e restituisce una lista di tensori di pose \n",
    "    per ciascuna persona nel tempo, se non prende nessuna posa mette un placeholder\"\"\"\n",
    "    #print(frames)\n",
    "    pose_sequences = []  # Lista di pose per ciascuna persona nel tempo\n",
    "    pose_placeholder = torch.zeros((36,), dtype=torch.float32)\n",
    "    for frame in frames:\n",
    "        candidate, subset = body_model(frame)\n",
    "\n",
    "        ###############################################################\n",
    "        # DEBUG & PLOTTING: Uncomment the following\n",
    "        canvas = copy.deepcopy(frame)\n",
    "        canvas = util.draw_bodypose(canvas, candidate, subset)\n",
    "        plt.imshow(canvas[:, :, [2, 1, 0]])  \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        ###############################################################\n",
    "\n",
    "        frame_poses = []\n",
    "        for person in subset:\n",
    "            if person[-1] >= 4:  # Almeno 4 punti chiave rilevati\n",
    "                pose = []\n",
    "                for i in range(18):\n",
    "                    if person[i] != -1:\n",
    "                        x, y = candidate[int(person[i])][:2]\n",
    "                    else:\n",
    "                        x, y = -1, -1  # Punti chiave mancanti\n",
    "                    pose.extend([x, y])\n",
    "                frame_poses.append(pose)\n",
    "        if not frame_poses:\n",
    "            frame_poses = [pose_placeholder.tolist()] #use placeholder if no pose detected (for dimensional consistency)\n",
    "        pose_sequences.append(frame_poses)\n",
    "\n",
    "    # Trasponi la lista di liste per ottenere le sequenze temporali per ciascuna persona\n",
    "    person_pose_sequences = list(map(list, zip(*pose_sequences)))\n",
    "    person_pose_sequences = [torch.tensor(person_poses, dtype=torch.float32) for person_poses in person_pose_sequences]\n",
    "    \n",
    "    return person_pose_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if RUN:\n",
    "    body_model = Body(POSE_PATH)\n",
    "\n",
    "    # Caricamento dei frame e estrazione delle pose\n",
    "    all_poses = []\n",
    "    all_poses_test = []\n",
    "\n",
    "    #itera tra i video e prendi i frames\n",
    "    for pics in tqdm(all_images, desc=\"Extracting poses from image sequences\"):\n",
    "        #print(len(pics))\n",
    "        pose_sequences = extract_pose_sequence(pics, body_model)\n",
    "        all_poses.append(pose_sequences)\n",
    "        #print(pose_sequences.shape)\n",
    "    # Aggiungi le pose estratte alla sequenza di allenamento\n",
    "    seq_train['poses'] = all_poses\n",
    "\n",
    "    for pics in tqdm(all_images_test, desc=\"Extracting poses from image sequences of test set\"):\n",
    "        pose_sequences = extract_pose_sequence(pics, body_model)\n",
    "        all_poses_test.append(pose_sequences)\n",
    "    # Aggiungi le pose estratte alla sequenza di allenamento\n",
    "    seq_test['poses'] = all_poses_test\n",
    "    del body_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # save data in the .pkl files \n",
    "    with open(POSE_CMD, 'wb') as f:\n",
    "        pickle.dump(seq_train['poses'], f)\n",
    "    with open(POSE_CMD_TEST, 'wb') as f:\n",
    "        pickle.dump(seq_test['poses'], f)\n",
    "else:\n",
    "    #recover data:\n",
    "    with open(POSE_CMD, 'rb') as f:\n",
    "        seq_train['poses'] = pickle.load(f)\n",
    "    with open(POSE_CMD_TEST, 'rb') as f:\n",
    "        seq_test['poses'] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionBranchLocal(torch.nn.Module):\n",
    "    \"\"\"definizione del modello per il local context, prende in input le immagini croppate e restituisce un tensore,\n",
    "    le immagini croppate vengono fatte passare dentro una VGG16, una GRU e un attention block\"\"\"\n",
    "\n",
    "    def __init__(self, vgg16):\n",
    "        super(VisionBranchLocal, self).__init__()\n",
    "        self.vgg16 = vgg16\n",
    "        self.avgpool = torch.nn.AvgPool2d(kernel_size=14)  # Pooling layer con kernel 14x14\n",
    "        self.gru = torch.nn.GRU(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(256, 2)    # Fully connected layer\n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Sigmoid()\n",
    "        #self.tanh = torch.nn.Tanh() #another try\n",
    "    def forward(self, cropped_images):\n",
    "        seq_len, c, h, w = cropped_images.size()\n",
    "        \n",
    "        # Estrai feature dalle immagini con VGG16\n",
    "        vgg_features = []\n",
    "        for i in range(seq_len):            \n",
    "\n",
    "            img = cropped_images[i]            \n",
    "            vgg_feat_img = self.vgg16.features(img)\n",
    "            pooled_feat_img = self.avgpool(vgg_feat_img)  # Applica il pooling\n",
    "            vgg_feat_img = pooled_feat_img.view(pooled_feat_img.size(0), -1)  # Flatten features\n",
    "            vgg_features.append(vgg_feat_img)\n",
    "        \n",
    "        vgg_features = torch.stack(vgg_features, dim=1).permute(2,1,0)\n",
    "\n",
    "        #applica la gru\n",
    "        gru_out, _ = self.gru(vgg_features)\n",
    "\n",
    "        #applica l'attention\n",
    "        attn_weights = torch.softmax(self.attn(gru_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n",
    "        \n",
    " \n",
    "        out = self.tanh((context_vector))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionBranchGlobal(torch.nn.Module):\n",
    "    \"\"\"definizione del modello per il global context, prende in input le maskere semantiche e restituisce un tensore,\n",
    "    le maskere semantiche vengono fatte passare dentro una VGG16, una GRU e un attention block\"\"\"\n",
    "\n",
    "    def __init__(self, vgg16):\n",
    "        super(VisionBranchGlobal, self).__init__()\n",
    "        self.vgg16 = vgg16\n",
    "        self.avgpool = torch.nn.AvgPool2d(kernel_size=14)  # Pooling layer con kernel 14x14\n",
    "        self.gru = torch.nn.GRU(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(256, 2)    # Fully connected layer\n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, masks):\n",
    "        seq_len = masks.size()[0]\n",
    "\n",
    "        # Estrai feature dalle immagini con VGG16\n",
    "        vgg_features = []\n",
    "        for i in range(seq_len):            \n",
    "            img = masks[i]            \n",
    "            vgg_feat_img = self.vgg16.features(img)\n",
    "            pooled_feat_img = self.avgpool(vgg_feat_img)  # Applica il pooling\n",
    "            vgg_feat_img = pooled_feat_img.view(pooled_feat_img.size(0), -1)  # Flatten features\n",
    "            vgg_features.append(vgg_feat_img)\n",
    "        \n",
    "        vgg_features = torch.stack(vgg_features, dim=1).permute(2,1,0)\n",
    "\n",
    "        gru_out, _ = self.gru(vgg_features)\n",
    "        attn_scores = self.attn(gru_out)  \n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)  \n",
    "        \n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)  \n",
    "        \n",
    "        out = self.tanh((context_vector))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NVisionBranch(torch.nn.Module):\n",
    "    \"\"\"classe relativa al non-vision brach, prende in input le pose e le bbox in formato tensore, esse vengono fatte passare\n",
    "    dentro una GRU e un attention block, l'ordine influenza la prestazioni\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NVisionBranch, self).__init__()\n",
    "        self.gru = torch.nn.GRU(input_size=36, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.gru2 = torch.nn.GRU(input_size=256+4, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.attn = torch.nn.Linear(256, 1)  # Attention layer\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, poses,bbox):\n",
    "        gru_out, _ = self.gru(poses)\n",
    "        LP = torch.cat((gru_out,bbox),dim=-1)\n",
    "        gru_out, _ = self.gru2(LP)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_scores = self.attn(gru_out) \n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)  \n",
    "        \n",
    "        context_vector = torch.sum(attn_weights * gru_out, dim=1)  \n",
    "        \n",
    "        out = self.tanh(context_vector)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedestrianIntentModel(torch.nn.Module):\n",
    "    \"\"\"definizione del modello finale, prende in input gli output del vision brach e del non vision branch, viene fatta\n",
    "    una concatenazione che in seguito passa dentro un attention e un fully connected layer, l'output è la predizione,\n",
    "    (passa non passa)\"\"\"\n",
    "\n",
    "    def __init__(self, vision_branch_local,vision_branch_global,non_vision_branch):\n",
    "        super(PedestrianIntentModel, self).__init__()\n",
    "        self.vision_branch_local = vision_branch_local #output of the vision branch local\n",
    "        self.vision_branch_global = vision_branch_global #output of the vision branch global\n",
    "        self.non_vision_branch = non_vision_branch #output of the non vision branch\n",
    "        self.attn = torch.nn.Linear(768, 768)  # Attention layer\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(768, 256) # fully connected layer\n",
    "        self.fc2 = torch.nn.Linear(256,1) # Output: crossing or not crossing\n",
    "\n",
    "        \n",
    "    def forward(self, cropped_images, bboxes, masks, poses):\n",
    "        vision_out_local = self.vision_branch_local(cropped_images)\n",
    "        vision_out_global = self.vision_branch_global(masks)\n",
    "        non_vision_out = self.non_vision_branch(poses, bboxes)\n",
    "\n",
    "        vision_out = torch.cat((vision_out_local, vision_out_global), dim=-1)\n",
    "        final_fusion = torch.cat((vision_out, non_vision_out), dim=-1)\n",
    "\n",
    "        attn_scores = self.attn(final_fusion)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        context_vector = torch.sum(attn_weights * final_fusion, dim=0)\n",
    "\n",
    "        out = (self.fc2(self.fc1(context_vector))) #raw output\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_FeatureExtractor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGG16_FeatureExtractor, self).__init__()\n",
    "        self.features = torch.nn.Sequential(*list(vgg16.features.children())[:24]) # block4_pool è il 24° livello\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Carica il modello VGG19 pre-addestrato\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "#cut the model at the 24th layer:\n",
    "vgg16_fe = VGG16_FeatureExtractor()\n",
    "vgg16_fe\n",
    "\n",
    "\n",
    "# define the models of each branches\n",
    "model_local = VisionBranchLocal(vgg16_fe).to(device)\n",
    "model_global = VisionBranchGlobal(vgg16_fe).to(device)\n",
    "model_non_vision = NVisionBranch().to(device)\n",
    "model = PedestrianIntentModel(model_local,model_global,model_non_vision).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET & DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JAADDataset(Dataset):\n",
    "    \"\"\"definizione della classe per il custom dataset, prende in input la seq_train, le immagini e le trasformazioni,\n",
    "    restituisce il tensore delle immagini croppate, le bboxes, le maschere, le pose e la lables\"\"\"\n",
    "\n",
    "    def __init__(self, seq_data, all_images, transform=None):\n",
    "        self.seq_data = seq_data\n",
    "        self.all_images = all_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bbox_sequence = self.seq_data['bbox'][idx]\n",
    "        masks = self.seq_data['masks'][idx]\n",
    "        poses = self.seq_data['poses'][idx]\n",
    "        all_images = self.all_images[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            tensor_images = [self.transform(img) for img in all_images]\n",
    "\n",
    "        bboxes = torch.tensor(self.seq_data['bbox'][idx], dtype=torch.float32)\n",
    "        intents = torch.tensor(self.seq_data['intent'][idx], dtype=torch.float32)\n",
    "        return  tensor_images, bboxes, masks, poses, intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JAADDataset(seq_train,all_images=all_images, transform=transform_lc)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataset = JAADDataset(seq_test,all_images=all_images_test, transform=transform_lc)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss() #input are raw output\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(net, loader, device):\n",
    "    acc = BinaryAccuracy().to(device)\n",
    "    precision = BinaryPrecision().to(device)\n",
    "    recall = BinaryRecall().to(device)\n",
    "    f1_score = BinaryF1Score().to(device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for tensor_images, bboxes, masks, poses, intents in loader:\n",
    "        poses = torch.stack(poses, dim=0)  \n",
    "        poses = poses.squeeze(0)\n",
    "        poses = poses.view(len(tensor_images), -1, 36).permute(1, 0, 2) \n",
    "        \n",
    "        # Move tensors to device\n",
    "        tensor_images = torch.stack(tensor_images, dim=1).squeeze(0).permute(0, 1, 2, 3).to(device)  # Convert image list to tensor\n",
    "        masks = torch.stack(masks, dim=1).squeeze(0).float().to(device)  # Convert mask list to tensor\n",
    "        bboxes = bboxes.to(device)\n",
    "        poses = poses.to(device)\n",
    "        intents = intents.squeeze(0)[0].to(device)\n",
    "        \n",
    "        ypred = net(tensor_images, bboxes, masks, poses)\n",
    "\n",
    "        # Move tensors back to CPU and clean up memory\n",
    "        tensor_images.cpu()\n",
    "        masks.cpu()\n",
    "        bboxes.cpu()\n",
    "        poses.cpu()\n",
    "        del tensor_images, bboxes, masks, poses\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update metrics\n",
    "        acc.update(ypred, intents)\n",
    "        precision.update(ypred, intents)\n",
    "        recall.update(ypred, intents)\n",
    "        f1_score.update(ypred, intents)\n",
    "        \n",
    "        intents.cpu()\n",
    "        del intents\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': acc.compute().item(),\n",
    "        'precision': precision.compute().item(),\n",
    "        'recall': recall.compute().item(),\n",
    "        'f1_score': f1_score.compute().item()\n",
    "    }\n",
    "    \n",
    "    # Reset metrics for the next evaluation\n",
    "    acc.reset()\n",
    "    precision.reset()\n",
    "    recall.reset()\n",
    "    f1_score.reset()\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6502\n",
      "Accuracy at epoch 0: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.6180\n",
      "Accuracy at epoch 1: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 0.8512\n",
      "Accuracy at epoch 2: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.5915\n",
      "Accuracy at epoch 3: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.8073\n",
      "Accuracy at epoch 4: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.8180\n",
      "Accuracy at epoch 5: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:35<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 0.5971\n",
      "Accuracy at epoch 6: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:36<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 0.5669\n",
      "Accuracy at epoch 7: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:35<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Loss: 0.8473\n",
      "Accuracy at epoch 8: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.5608\n",
      "Accuracy at epoch 9: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Loss: 0.5225\n",
      "Accuracy at epoch 10: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Loss: 0.5606\n",
      "Accuracy at epoch 11: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Loss: 0.4781\n",
      "Accuracy at epoch 12: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Loss: 0.8471\n",
      "Accuracy at epoch 13: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Loss: 0.9419\n",
      "Accuracy at epoch 14: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:35<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Loss: 0.5081\n",
      "Accuracy at epoch 15: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:36<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Loss: 0.8426\n",
      "Accuracy at epoch 16: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Loss: 0.4990\n",
      "Accuracy at epoch 17: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Loss: 0.6315\n",
      "Accuracy at epoch 18: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 1.1430\n",
      "Accuracy at epoch 19: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Loss: 0.5024\n",
      "Accuracy at epoch 20: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Loss: 0.6764\n",
      "Accuracy at epoch 21: {'accuracy': 0.8041236996650696, 'precision': 0.6666666865348816, 'recall': 0.10000000149011612, 'f1_score': 0.17391304671764374}\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Loss: 0.6947\n",
      "Accuracy at epoch 22: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Loss: 0.4846\n",
      "Accuracy at epoch 23: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Loss: 1.0578\n",
      "Accuracy at epoch 24: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Loss: 0.4866\n",
      "Accuracy at epoch 25: {'accuracy': 0.7731958627700806, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Loss: 0.6344\n",
      "Accuracy at epoch 26: {'accuracy': 0.7835051417350769, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Loss: 0.3366\n",
      "Accuracy at epoch 27: {'accuracy': 0.7938144207000732, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Loss: 1.2039\n",
      "Accuracy at epoch 28: {'accuracy': 0.7628865838050842, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Loss: 0.5027\n",
      "Accuracy at epoch 29: {'accuracy': 0.7835051417350769, 'precision': 0.3333333432674408, 'recall': 0.05000000074505806, 'f1_score': 0.08695652335882187}\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Loss: 0.3148\n",
      "Accuracy at epoch 30: {'accuracy': 0.7731958627700806, 'precision': 0.25, 'recall': 0.05000000074505806, 'f1_score': 0.0833333358168602}\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Loss: 0.6644\n",
      "Accuracy at epoch 31: {'accuracy': 0.7835051417350769, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Loss: 0.9297\n",
      "Accuracy at epoch 32: {'accuracy': 0.7938144207000732, 'precision': 0.5, 'recall': 0.550000011920929, 'f1_score': 0.523809552192688}\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Loss: 0.3223\n",
      "Accuracy at epoch 33: {'accuracy': 0.8144329786300659, 'precision': 0.625, 'recall': 0.25, 'f1_score': 0.3571428656578064}\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Loss: 0.3265\n",
      "Accuracy at epoch 34: {'accuracy': 0.7938144207000732, 'precision': 0.5, 'recall': 0.15000000596046448, 'f1_score': 0.23076923191547394}\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Loss: 0.4487\n",
      "Accuracy at epoch 35: {'accuracy': 0.7835051417350769, 'precision': 0.4444444477558136, 'recall': 0.20000000298023224, 'f1_score': 0.27586206793785095}\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100], Loss: 0.1894\n",
      "Accuracy at epoch 36: {'accuracy': 0.7628865838050842, 'precision': 0.3636363744735718, 'recall': 0.20000000298023224, 'f1_score': 0.25806450843811035}\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Loss: 0.8527\n",
      "Accuracy at epoch 37: {'accuracy': 0.7835051417350769, 'precision': 0.4000000059604645, 'recall': 0.10000000149011612, 'f1_score': 0.1599999964237213}\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100], Loss: 0.3008\n",
      "Accuracy at epoch 38: {'accuracy': 0.7628865838050842, 'precision': 0.3333333432674408, 'recall': 0.15000000596046448, 'f1_score': 0.2068965584039688}\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Loss: 0.3022\n",
      "Accuracy at epoch 39: {'accuracy': 0.7938144207000732, 'precision': 0.5, 'recall': 0.30000001192092896, 'f1_score': 0.375}\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Loss: 0.6133\n",
      "Accuracy at epoch 40: {'accuracy': 0.7628865838050842, 'precision': 0.4285714328289032, 'recall': 0.44999998807907104, 'f1_score': 0.4390243887901306}\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Loss: 0.4352\n",
      "Accuracy at epoch 41: {'accuracy': 0.7628865838050842, 'precision': 0.4444444477558136, 'recall': 0.6000000238418579, 'f1_score': 0.5106382966041565}\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100], Loss: 1.0539\n",
      "Accuracy at epoch 42: {'accuracy': 0.7938144207000732, 'precision': 0.5, 'recall': 0.05000000074505806, 'f1_score': 0.09090909361839294}\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Loss: 0.2129\n",
      "Accuracy at epoch 43: {'accuracy': 0.8041236996650696, 'precision': 0.5714285969734192, 'recall': 0.20000000298023224, 'f1_score': 0.29629629850387573}\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100], Loss: 0.7359\n",
      "Accuracy at epoch 44: {'accuracy': 0.7938144207000732, 'precision': 0.5, 'recall': 0.699999988079071, 'f1_score': 0.5833333134651184}\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Loss: 0.2407\n",
      "Accuracy at epoch 45: {'accuracy': 0.7731958627700806, 'precision': 0.3333333432674408, 'recall': 0.10000000149011612, 'f1_score': 0.1538461595773697}\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100], Loss: 0.7114\n",
      "Accuracy at epoch 46: {'accuracy': 0.8041236996650696, 'precision': 0.52173912525177, 'recall': 0.6000000238418579, 'f1_score': 0.5581395626068115}\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100], Loss: 0.4545\n",
      "Accuracy at epoch 47: {'accuracy': 0.8247422575950623, 'precision': 0.6666666865348816, 'recall': 0.30000001192092896, 'f1_score': 0.4137931168079376}\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Loss: 0.3808\n",
      "Accuracy at epoch 48: {'accuracy': 0.7938144207000732, 'precision': 0.5, 'recall': 0.15000000596046448, 'f1_score': 0.23076923191547394}\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Loss: 0.8247\n",
      "Accuracy at epoch 49: {'accuracy': 0.7319587469100952, 'precision': 0.42105263471603394, 'recall': 0.800000011920929, 'f1_score': 0.5517241358757019}\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100], Loss: 0.1209\n",
      "Accuracy at epoch 50: {'accuracy': 0.8453608155250549, 'precision': 0.7272727489471436, 'recall': 0.4000000059604645, 'f1_score': 0.5161290168762207}\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100], Loss: 0.6077\n",
      "Accuracy at epoch 51: {'accuracy': 0.8144329786300659, 'precision': 0.5625, 'recall': 0.44999998807907104, 'f1_score': 0.5}\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Loss: 0.7142\n",
      "Accuracy at epoch 52: {'accuracy': 0.8350515365600586, 'precision': 0.6000000238418579, 'recall': 0.6000000238418579, 'f1_score': 0.6000000238418579}\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Loss: 1.3747\n",
      "Accuracy at epoch 53: {'accuracy': 0.7525773048400879, 'precision': 0.40909090638160706, 'recall': 0.44999998807907104, 'f1_score': 0.4285714328289032}\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100], Loss: 0.2539\n",
      "Accuracy at epoch 54: {'accuracy': 0.8041236996650696, 'precision': 0.5454545617103577, 'recall': 0.30000001192092896, 'f1_score': 0.3870967626571655}\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:35<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Loss: 2.0380\n",
      "Accuracy at epoch 55: {'accuracy': 0.8247422575950623, 'precision': 0.6153846383094788, 'recall': 0.4000000059604645, 'f1_score': 0.4848484992980957}\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100], Loss: 0.0635\n",
      "Accuracy at epoch 56: {'accuracy': 0.7835051417350769, 'precision': 0.4545454680919647, 'recall': 0.25, 'f1_score': 0.32258063554763794}\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Loss: 0.3763\n",
      "Accuracy at epoch 57: {'accuracy': 0.8144329786300659, 'precision': 0.6000000238418579, 'recall': 0.30000001192092896, 'f1_score': 0.4000000059604645}\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100], Loss: 0.4422\n",
      "Accuracy at epoch 58: {'accuracy': 0.8144329786300659, 'precision': 0.550000011920929, 'recall': 0.550000011920929, 'f1_score': 0.550000011920929}\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100], Loss: 0.3904\n",
      "Accuracy at epoch 59: {'accuracy': 0.8350515365600586, 'precision': 0.625, 'recall': 0.5, 'f1_score': 0.5555555820465088}\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Loss: 0.3187\n",
      "Accuracy at epoch 60: {'accuracy': 0.8453608155250549, 'precision': 0.8571428656578064, 'recall': 0.30000001192092896, 'f1_score': 0.4444444477558136}\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Loss: 1.6771\n",
      "Accuracy at epoch 61: {'accuracy': 0.8144329786300659, 'precision': 0.5555555820465088, 'recall': 0.5, 'f1_score': 0.5263158082962036}\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100], Loss: 0.9798\n",
      "Accuracy at epoch 62: {'accuracy': 0.8144329786300659, 'precision': 0.6000000238418579, 'recall': 0.30000001192092896, 'f1_score': 0.4000000059604645}\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Loss: 0.3892\n",
      "Accuracy at epoch 63: {'accuracy': 0.8144329786300659, 'precision': 0.550000011920929, 'recall': 0.550000011920929, 'f1_score': 0.550000011920929}\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100], Loss: 0.0659\n",
      "Accuracy at epoch 64: {'accuracy': 0.8144329786300659, 'precision': 0.6666666865348816, 'recall': 0.20000000298023224, 'f1_score': 0.3076923191547394}\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100], Loss: 0.6174\n",
      "Accuracy at epoch 65: {'accuracy': 0.8350515365600586, 'precision': 0.5625, 'recall': 0.8999999761581421, 'f1_score': 0.692307710647583}\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/100], Loss: 0.2225\n",
      "Accuracy at epoch 66: {'accuracy': 0.7731958627700806, 'precision': 0.4000000059604645, 'recall': 0.20000000298023224, 'f1_score': 0.2666666805744171}\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100], Loss: 0.0816\n",
      "Accuracy at epoch 67: {'accuracy': 0.8453608155250549, 'precision': 0.7777777910232544, 'recall': 0.3499999940395355, 'f1_score': 0.48275861144065857}\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/100], Loss: 2.1938\n",
      "Accuracy at epoch 68: {'accuracy': 0.7628865838050842, 'precision': 0.38461539149284363, 'recall': 0.25, 'f1_score': 0.3030303120613098}\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Loss: 0.2916\n",
      "Accuracy at epoch 69: {'accuracy': 0.8247422575950623, 'precision': 0.6363636255264282, 'recall': 0.3499999940395355, 'f1_score': 0.4516128897666931}\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100], Loss: 0.2824\n",
      "Accuracy at epoch 70: {'accuracy': 0.7835051417350769, 'precision': 0.4444444477558136, 'recall': 0.20000000298023224, 'f1_score': 0.27586206793785095}\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Loss: 0.2455\n",
      "Accuracy at epoch 71: {'accuracy': 0.7938144207000732, 'precision': 0.5, 'recall': 0.20000000298023224, 'f1_score': 0.2857142984867096}\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/100], Loss: 0.1113\n",
      "Accuracy at epoch 72: {'accuracy': 0.8144329786300659, 'precision': 0.6666666865348816, 'recall': 0.20000000298023224, 'f1_score': 0.3076923191547394}\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], Loss: 0.7562\n",
      "Accuracy at epoch 73: {'accuracy': 0.8453608155250549, 'precision': 0.8571428656578064, 'recall': 0.30000001192092896, 'f1_score': 0.4444444477558136}\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100], Loss: 0.1709\n",
      "Accuracy at epoch 74: {'accuracy': 0.8144329786300659, 'precision': 0.5833333134651184, 'recall': 0.3499999940395355, 'f1_score': 0.4375}\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100], Loss: 0.4440\n",
      "Accuracy at epoch 75: {'accuracy': 0.8144329786300659, 'precision': 0.5714285969734192, 'recall': 0.4000000059604645, 'f1_score': 0.47058823704719543}\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100], Loss: 0.3845\n",
      "Accuracy at epoch 76: {'accuracy': 0.8144329786300659, 'precision': 0.5714285969734192, 'recall': 0.4000000059604645, 'f1_score': 0.47058823704719543}\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100], Loss: 1.2435\n",
      "Accuracy at epoch 77: {'accuracy': 0.7835051417350769, 'precision': 0.4615384638309479, 'recall': 0.30000001192092896, 'f1_score': 0.3636363744735718}\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/100], Loss: 0.5311\n",
      "Accuracy at epoch 78: {'accuracy': 0.7835051417350769, 'precision': 0.4615384638309479, 'recall': 0.30000001192092896, 'f1_score': 0.3636363744735718}\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Loss: 0.1764\n",
      "Accuracy at epoch 79: {'accuracy': 0.8556700944900513, 'precision': 0.6363636255264282, 'recall': 0.699999988079071, 'f1_score': 0.6666666865348816}\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/100], Loss: 0.1816\n",
      "Accuracy at epoch 80: {'accuracy': 0.8144329786300659, 'precision': 0.5555555820465088, 'recall': 0.5, 'f1_score': 0.5263158082962036}\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100], Loss: 0.2335\n",
      "Accuracy at epoch 81: {'accuracy': 0.8041236996650696, 'precision': 0.5384615659713745, 'recall': 0.3499999940395355, 'f1_score': 0.42424243688583374}\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/100], Loss: 0.2333\n",
      "Accuracy at epoch 82: {'accuracy': 0.8247422575950623, 'precision': 0.6153846383094788, 'recall': 0.4000000059604645, 'f1_score': 0.4848484992980957}\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100], Loss: 0.0509\n",
      "Accuracy at epoch 83: {'accuracy': 0.8144329786300659, 'precision': 0.550000011920929, 'recall': 0.550000011920929, 'f1_score': 0.550000011920929}\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/100], Loss: 0.1288\n",
      "Accuracy at epoch 84: {'accuracy': 0.8247422575950623, 'precision': 0.6153846383094788, 'recall': 0.4000000059604645, 'f1_score': 0.4848484992980957}\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:35<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100], Loss: 0.6130\n",
      "Accuracy at epoch 85: {'accuracy': 0.8350515365600586, 'precision': 0.5625, 'recall': 0.8999999761581421, 'f1_score': 0.692307710647583}\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/100], Loss: 0.2127\n",
      "Accuracy at epoch 86: {'accuracy': 0.8144329786300659, 'precision': 0.5714285969734192, 'recall': 0.4000000059604645, 'f1_score': 0.47058823704719543}\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100], Loss: 1.0294\n",
      "Accuracy at epoch 87: {'accuracy': 0.8350515365600586, 'precision': 0.6000000238418579, 'recall': 0.6000000238418579, 'f1_score': 0.6000000238418579}\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/100], Loss: 0.2884\n",
      "Accuracy at epoch 88: {'accuracy': 0.8247422575950623, 'precision': 0.5789473652839661, 'recall': 0.550000011920929, 'f1_score': 0.5641025900840759}\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Loss: 1.1629\n",
      "Accuracy at epoch 89: {'accuracy': 0.876288652420044, 'precision': 0.6538461446762085, 'recall': 0.8500000238418579, 'f1_score': 0.739130437374115}\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100], Loss: 0.3006\n",
      "Accuracy at epoch 90: {'accuracy': 0.8144329786300659, 'precision': 0.5555555820465088, 'recall': 0.5, 'f1_score': 0.5263158082962036}\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Loss: 0.4685\n",
      "Accuracy at epoch 91: {'accuracy': 0.8247422575950623, 'precision': 0.5789473652839661, 'recall': 0.550000011920929, 'f1_score': 0.5641025900840759}\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/100], Loss: 1.5025\n",
      "Accuracy at epoch 92: {'accuracy': 0.8247422575950623, 'precision': 0.5882353186607361, 'recall': 0.5, 'f1_score': 0.5405405163764954}\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100], Loss: 0.0547\n",
      "Accuracy at epoch 93: {'accuracy': 0.8247422575950623, 'precision': 0.6153846383094788, 'recall': 0.4000000059604645, 'f1_score': 0.4848484992980957}\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/100], Loss: 0.1225\n",
      "Accuracy at epoch 94: {'accuracy': 0.8041236996650696, 'precision': 0.6000000238418579, 'recall': 0.15000000596046448, 'f1_score': 0.23999999463558197}\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:32<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/100], Loss: 0.2501\n",
      "Accuracy at epoch 95: {'accuracy': 0.8350515365600586, 'precision': 0.75, 'recall': 0.30000001192092896, 'f1_score': 0.4285714328289032}\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/100], Loss: 0.9217\n",
      "Accuracy at epoch 96: {'accuracy': 0.8247422575950623, 'precision': 0.5789473652839661, 'recall': 0.550000011920929, 'f1_score': 0.5641025900840759}\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:33<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100], Loss: 0.3176\n",
      "Accuracy at epoch 97: {'accuracy': 0.8659793734550476, 'precision': 0.6521739363670349, 'recall': 0.75, 'f1_score': 0.6976743936538696}\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100], Loss: 0.2809\n",
      "Accuracy at epoch 98: {'accuracy': 0.8247422575950623, 'precision': 0.5789473652839661, 'recall': 0.550000011920929, 'f1_score': 0.5641025900840759}\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 1.7935\n",
      "Accuracy at epoch 99: {'accuracy': 0.8556700944900513, 'precision': 0.6153846383094788, 'recall': 0.800000011920929, 'f1_score': 0.695652186870575}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# free the memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    model.train()\n",
    "\n",
    "    for tensor_images, bboxes, masks, poses, intents in tqdm(train_loader):\n",
    "\n",
    "        poses = torch.stack(poses, dim=0)  # Now the shape is (batch_size, 36)\n",
    "        poses = poses.squeeze(0)\n",
    "        poses = poses.view(len(tensor_images), -1, 36).permute(1,0,2)  # Reshape to (batch_size, numeroFrames, 36)\n",
    "\n",
    "\n",
    "        # Move tensors to device\n",
    "        tensor_images = torch.stack(tensor_images, dim=1).squeeze(0).permute(0, 1, 2,3).to(device)  # Converte la lista di immagini in un tensor\n",
    "        masks = torch.stack(masks,dim=1).squeeze(0).float().to(device)  # Converte la lista di maschere in un tensor\n",
    "        bboxes = bboxes.to(device)\n",
    "        poses = poses.to(device)\n",
    "        intents = intents.squeeze(0)[0].to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(tensor_images, bboxes, masks, poses)\n",
    "        tensor_images.cpu()\n",
    "        bboxes.cpu()\n",
    "        poses.cpu()\n",
    "        loss = criterion(outputs, intents.float())\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        del tensor_images, masks, bboxes, poses, intents, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    with torch.no_grad():\n",
    "        print(f'Accuracy at epoch {epoch}: {evaluate_metrics(model, test_loader, device)}')\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
