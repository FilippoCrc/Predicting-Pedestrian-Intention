{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaad_data import JAAD\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import network\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JAAD_PATH = '../JAAD'\n",
    "DEEPLAB_PATH = '../best_deeplabv3plus_resnet101_cityscapes_os16.pth'\n",
    "SUBSET_PATH = '../subset'\n",
    "FILENAME_SUB = '../masks_results_sub.pkl'\n",
    "FILENAME_BIG = '../masks_results_big.pkl'\n",
    "POSE_PATH = '../body_pose_model.pth'   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN =False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Generating action sequence data\n",
      "fstride: 10\n",
      "sample_type: beh\n",
      "subset: default\n",
      "height_rng: [0, inf]\n",
      "squarify_ratio: 0\n",
      "data_split_type: default\n",
      "seq_type: intention\n",
      "min_track_size: 15\n",
      "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
      "kfold_params: {'num_folds': 5, 'fold': 1}\n",
      "---------------------------------------------------------\n",
      "Generating database for jaad\n",
      "jaad database loaded from c:\\Users\\jacop\\Documents\\ComputerVision\\subset\\data_cache\\jaad_database.pkl\n",
      "---------------------------------------------------------\n",
      "Generating intention data\n",
      "Split: train\n",
      "Number of pedestrians: 4 \n",
      "Total number of samples: 2 \n",
      "---------------------------------------------------------\n",
      "Generating action sequence data\n",
      "fstride: 10\n",
      "sample_type: beh\n",
      "subset: default\n",
      "height_rng: [0, inf]\n",
      "squarify_ratio: 0\n",
      "data_split_type: default\n",
      "seq_type: intention\n",
      "min_track_size: 15\n",
      "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
      "kfold_params: {'num_folds': 5, 'fold': 1}\n",
      "---------------------------------------------------------\n",
      "Generating database for jaad\n",
      "jaad database loaded from c:\\Users\\jacop\\Documents\\ComputerVision\\subset\\data_cache\\jaad_database.pkl\n",
      "---------------------------------------------------------\n",
      "Generating intention data\n",
      "Split: test\n",
      "Number of pedestrians: 6 \n",
      "Total number of samples: 0 \n"
     ]
    }
   ],
   "source": [
    "# Load the JAAD dataset\n",
    "jaad_dt = JAAD(data_path=SUBSET_PATH)\n",
    "#jaad.generate_database()\n",
    "#jaad_dt.get_data_stats()\n",
    "\n",
    "data_opts = {\n",
    "    'fstride': 10,\n",
    "    'sample_type': 'beh'\n",
    "}\n",
    "\n",
    "seq_train = jaad_dt.generate_data_trajectory_sequence('train', **data_opts)  \n",
    "seq_test = jaad_dt.generate_data_trajectory_sequence('test', **data_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequences: 37\n"
     ]
    }
   ],
   "source": [
    "print('Train sequences:', len(seq_train['image'][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' class JAADDataset(Dataset):\\n    def __init__(self, seq_data, transform=None):\\n        self.seq_data = seq_data\\n        self.transform = transform\\n\\n    def __len__(self):\\n        return len(self.seq_data[\\'image\\'])\\n\\n    def __getitem__(self, idx):\\n        # Carica le immagini e le maschere\\n        print(\"ciao1\")\\n        img_paths = self.seq_data[\\'image\\'][idx]\\n\\n        images = [Image.open(img_path).convert(\"RGB\") for img_path in img_paths]\\n        print(\"ciao2\")\\n\\n        #mask_paths = self.seq_data[\\'masks\\'][idx]\\n        print(\"ciao3\")\\n\\n        #masks = [Image.fromarray(mask.numpy()) for mask in mask_paths]\\n        print(\"ciao4\")\\n        if self.transform:\\n            images = [self.transform(img) for img in images]\\n            #masks = [self.transform(mask) for mask in masks]\\n\\n        # Carica i metadati\\n        #pid = torch.tensor(self.seq_data[\\'pid\\'][idx], dtype=torch.string)\\n        print(len(images))\\n        bboxes = torch.tensor(self.seq_data[\\'bbox\\'][idx], dtype=torch.float32)\\n        centers = torch.tensor(self.seq_data[\\'center\\'][idx], dtype=torch.float32)\\n        occlusions = torch.tensor(self.seq_data[\\'occlusion\\'][idx], dtype=torch.float32)\\n        intents = torch.tensor(self.seq_data[\\'intent\\'][idx], dtype=torch.long)\\n        #return images,masks, bboxes, centers, occlusions, intents\\n        return images, bboxes, centers, occlusions, intents '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" class JAADDataset(Dataset):\n",
    "    def __init__(self, seq_data, transform=None):\n",
    "        self.seq_data = seq_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Carica le immagini e le maschere\n",
    "        print(\"ciao1\")\n",
    "        img_paths = self.seq_data['image'][idx]\n",
    "\n",
    "        images = [Image.open(img_path).convert(\"RGB\") for img_path in img_paths]\n",
    "        print(\"ciao2\")\n",
    "\n",
    "        #mask_paths = self.seq_data['masks'][idx]\n",
    "        print(\"ciao3\")\n",
    "\n",
    "        #masks = [Image.fromarray(mask.numpy()) for mask in mask_paths]\n",
    "        print(\"ciao4\")\n",
    "        if self.transform:\n",
    "            images = [self.transform(img) for img in images]\n",
    "            #masks = [self.transform(mask) for mask in masks]\n",
    "\n",
    "        # Carica i metadati\n",
    "        #pid = torch.tensor(self.seq_data['pid'][idx], dtype=torch.string)\n",
    "        print(len(images))\n",
    "        bboxes = torch.tensor(self.seq_data['bbox'][idx], dtype=torch.float32)\n",
    "        centers = torch.tensor(self.seq_data['center'][idx], dtype=torch.float32)\n",
    "        occlusions = torch.tensor(self.seq_data['occlusion'][idx], dtype=torch.float32)\n",
    "        intents = torch.tensor(self.seq_data['intent'][idx], dtype=torch.long)\n",
    "        #return images,masks, bboxes, centers, occlusions, intents\n",
    "        return images, bboxes, centers, occlusions, intents \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JAADDataset(Dataset):\n",
    "    def __init__(self, seq_data, transform=None):\n",
    "        self.seq_data = seq_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Carica le immagini e le maschere\n",
    "        print(\"ciao1\")\n",
    "        #get path of images\n",
    "        img_paths = self.seq_data['image'][idx]\n",
    "        #get bboxes \n",
    "        bbox_sequence = self.seq_data['bbox'][idx]\n",
    "\n",
    "        #open the images from the paths \n",
    "        images = [Image.open(img_path).convert(\"RGB\") for img_path in img_paths]\n",
    "        print(\"ciao2\")\n",
    "\n",
    "        #compute local context cropping images arroun the bboxes\n",
    "        cropped_images = [crop_image(img, bbox) for img, bbox in zip(images, bbox_sequence)]\n",
    "\n",
    "        #mask_paths = self.seq_data['masks'][idx]\n",
    "        print(\"ciao3\")\n",
    "\n",
    "        #masks = [Image.fromarray(mask.numpy()) for mask in mask_paths]\n",
    "        print(\"ciao4\")\n",
    "        if self.transform:\n",
    "            #images = [self.transform(img) for img in images]\n",
    "            cropped_images = [self.transform(img) for img in cropped_images]\n",
    "            #masks = [self.transform(mask) for mask in masks]\n",
    "\n",
    "        # Carica i metadati\n",
    "        #pid = torch.tensor(self.seq_data['pid'][idx], dtype=torch.string)\n",
    "        print(len(images))\n",
    "        print(len(cropped_images))\n",
    "\n",
    "        bboxes = torch.tensor(self.seq_data['bbox'][idx], dtype=torch.float32)\n",
    "\n",
    "        intents = torch.tensor(self.seq_data['intent'][idx], dtype=torch.long)\n",
    "        #return images,masks, bboxes, centers, occlusions, intents\n",
    "        return  cropped_images, bboxes, intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Ridimensiona le immagini a 256x256\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasformazioni per le immagini\n",
    "transform_lc = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL CONTEXT EXTRACTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" deeplab_model = models.segmentation.deeplabv3_resnet101(pretrained=True).to(device)\n",
    "deeplab_model.eval()  # Imposta il modello in modalità di valutazione \"\"\"\n",
    "#deeplab_model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "if RUN:\n",
    "    deeplab_model = network.modeling.__dict__['deeplabv3plus_resnet101'](num_classes=19)\n",
    "    deeplab_model.load_state_dict(torch.load(DEEPLAB_PATH)['model_state'])\n",
    "    deeplab_model.to(device)\n",
    "    deeplab_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_mask(image_path, model, preprocess):\n",
    "    # Load the image\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = preprocess(input_image).to(device)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Create a batch with a single image\n",
    "    \n",
    "    # Pass the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    # Check if output is a tensor or a dictionary\n",
    "    if isinstance(output, dict):\n",
    "        output = output['out'][0]\n",
    "    elif isinstance(output, torch.Tensor):\n",
    "        output = output[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected output type: {type(output)}\")\n",
    "    \n",
    "    # Convert the output to a mask\n",
    "    output_predictions = output.argmax(0).cpu()\n",
    "    return output_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_frames(seq_train, model, preprocess):\n",
    "    all_masks = []\n",
    "    for video_frames in seq_train['image']:\n",
    "        video_masks = []\n",
    "\n",
    "        for frame_path in video_frames:\n",
    "            mask = get_segmentation_mask(frame_path, model, preprocess)\n",
    "            #visualize_mask(frame_path, mask)\n",
    "            video_masks.append(mask)\n",
    "        all_masks.append(video_masks)\n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza la maschera semantica\n",
    "def visualize_mask(image_path, mask):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((256, 256))  # Ridimensiona per la visualizzazione\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap='jet')\n",
    "    plt.title(\"Semantic Mask\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10,  2,  2],\n",
      "        [10, 10, 10,  ..., 10,  2,  2],\n",
      "        [10, 10, 10,  ..., 10,  2,  2],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ...,  2,  2,  2],\n",
      "        [10, 10, 10,  ...,  2,  2,  2],\n",
      "        [10, 10, 10,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10,  2,  2],\n",
      "        [10, 10, 10,  ..., 10,  2,  2],\n",
      "        [10, 10, 10,  ..., 10,  2,  2],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        [10, 10, 10,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]])], [tensor([[ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        [ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        [ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        [ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        [ 2,  2,  2,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        [2, 2, 2,  ..., 2, 2, 2],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])]]\n"
     ]
    }
   ],
   "source": [
    "if RUN:\n",
    "    all_video_masks = process_video_frames(seq_train, deeplab_model, train_transforms)\n",
    "    seq_train['masks'] = all_video_masks\n",
    "    # Apri il file in modalità scrittura binaria e salva il dizionario\n",
    "    with open(FILENAME_BIG, 'wb') as f:\n",
    "        pickle.dump(seq_train['masks'], f)\n",
    "else:\n",
    "    #recover data:\n",
    "    with open(FILENAME_SUB, 'rb') as f:\n",
    "        seq_train['masks'] = pickle.load(f)\n",
    "\n",
    "    # Verifica che i risultati siano stati caricati correttamente\n",
    "    print(seq_train['masks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOCAL CONTEXT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop frames arround bounding boxes\n",
    "def crop_image(img, bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return img.crop((x1, y1, x2, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSE KEYPOINTS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedestrianIntentModel(torch.nn.Module):\n",
    "    def __init__(self, vgg16):\n",
    "        super(PedestrianIntentModel, self).__init__()\n",
    "        self.vgg16 = vgg16\n",
    "        self.avgpool = torch.nn.AvgPool2d(kernel_size=14)  # Pooling layer con kernel 14x14\n",
    "        self.gru = torch.nn.GRU(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(256, 2)  # Output: crossing or not crossing\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, cropped_images, bboxes):\n",
    "        seq_len, c, h, w = cropped_images.size()\n",
    "        \n",
    "        # Estrai feature dalle immagini con VGG19\n",
    "        vgg_features = []\n",
    "        for i in range(seq_len):            \n",
    "\n",
    "            img = cropped_images[i]            \n",
    "            vgg_feat_img = self.vgg16.features(img)\n",
    "            pooled_feat_img = self.avgpool(vgg_feat_img)  # Applica il pooling\n",
    "            vgg_feat_img = pooled_feat_img.view(pooled_feat_img.size(0), -1)  # Flatten features\n",
    "            vgg_features.append(vgg_feat_img)\n",
    "        \n",
    "        vgg_features = torch.stack(vgg_features, dim=1).permute(2,1,0)\n",
    "        #vgg_features = vgg_features.flatten(start_dim=2)\n",
    "        # Combina con i metadati\n",
    "\n",
    "        # metadatas = torch.cat((bboxes, centers, occlusions), dim=-1)\n",
    "        # features = torch.cat((vgg_features, metadatas), dim=-1)\n",
    "        \n",
    "        # # Passa attraverso la GRU\n",
    "#        out = vgg_features\n",
    "        print(\"SIZE:\",vgg_features.shape)\n",
    "        gru_out, _ = self.gru(vgg_features)\n",
    "        print(\"SIZE:\",gru_out.shape)\n",
    "        out = self.sigmoid(self.fc(gru_out[:, -1, :]))   \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16_FeatureExtractor, self).__init__()\n",
    "        self.features = torch.nn.Sequential(*list(vgg16.features.children())[:24]) # block4_pool è il 23° livello\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG16_FeatureExtractor(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carica il modello VGG19 pre-addestrato\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "vgg16_fe = VGG16_FeatureExtractor()\n",
    "\n",
    "# # Congela i pesi dei primi strati\n",
    "# for param in vgg16_fe.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Sostituisci l'ultimo strato della VGG19 per il tuo task specifico\n",
    "# num_features = vgg16_fe.classifier[6].in_features\n",
    "# vgg16_fe.classifier[6] = torch.nn.Linear(num_features, 2)  # Assuming 2 classes for intent\n",
    "\n",
    "# # Abilita il training solo per i nuovi strati aggiunti\n",
    "# for param in vgg16_fe.classifier[6].parameters():\n",
    "#     param.requires_grad = True\n",
    "# # Inizializza il modello con VGG19\n",
    "model = PedestrianIntentModel(vgg16_fe).to(device)\n",
    "\n",
    "# Definisci la loss e l'ottimizzatore\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "vgg16_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello VGG19 pre-addestrato\n",
    "model2 = models.vgg16(pretrained=True).to(device)\n",
    "\n",
    "# Definisci la loss e l'ottimizzatore\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao1\n",
      "ciao2\n",
      "ciao3\n",
      "ciao4\n",
      "37\n",
      "37\n",
      "2\n",
      "ciao1\n",
      "ciao2\n",
      "ciao3\n",
      "ciao4\n",
      "37\n",
      "37\n",
      "torch.Size([3, 224, 224])\n",
      "ciao1\n",
      "ciao2\n",
      "ciao3\n",
      "ciao4\n",
      "37\n",
      "37\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'np'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     img_np \u001b[38;5;241m=\u001b[39m denormalized_tensor[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Clip values to [0, 1] range\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m img_np \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mclip(img_np, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Plot the image\u001b[39;00m\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:2003\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   2000\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m   2001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m-> 2003\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'np'"
     ]
    }
   ],
   "source": [
    "train_dataset = JAADDataset(seq_train, transform=transform_lc)\n",
    "(train_dataset.__getitem__(0))\n",
    "print(len(train_dataset))\n",
    "print(train_dataset.__getitem__(0)[0][0].shape)\n",
    "denormalized_tensor = denormalize(train_dataset.__getitem__(0)[0][0].clone(), mean, std)\n",
    "if denormalized_tensor.dim() == 3:\n",
    "    img_np = denormalized_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "elif denormalized_tensor.dim() == 4:\n",
    "    img_np = denormalized_tensor[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Clip values to [0, 1] range\n",
    "img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img_np)\n",
    "plt.axis('off')\n",
    "plt.title(\"Original (Denormalized) Image\")\n",
    "plt.show()\n",
    "show_pic(train_dataset.__getitem__(0)[0][0].permute(1,2,0))\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pic(image):    \n",
    "    numpy_data = image.cpu().numpy()\n",
    "\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(numpy_data)  # Use 'gray' colormap for grayscale images\n",
    "\n",
    "    # Remove axes ticks\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Add a title if needed\n",
    "    plt.title(\"Your Image Title\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bathc = train_dataset.__getitem__(0)\n",
    "# print(len(bathc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "ciao1\n",
      "ciao2\n",
      "ciao3\n",
      "ciao4\n",
      "18\n",
      "18\n",
      "TRAINING:\n",
      "SIZE: torch.Size([1, 18, 512])\n",
      "SIZE: torch.Size([1, 18, 256])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([18, 1])) that is different to the input size (torch.Size([1, 2])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(cropped_images,bboxes)\n\u001b[1;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3145\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3143\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3148\u001b[0m     )\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([18, 1])) that is different to the input size (torch.Size([1, 2])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    model.train()\n",
    "    for cropped_images, bboxes, intents in train_loader:\n",
    "        print(\"TRAINING:\")\n",
    "        cropped_images = torch.stack(cropped_images, dim=1).squeeze(0).permute(0, 1, 2,3).to(device)  # Converte la lista di immagini in un tensor\n",
    "        #masks = torch.stack(masks, dim=1).squeeze(2).to(device)  # Converte la lista di maschere in un tensor\n",
    "        #pid = pid.stack(images, dim=1).squeeze(2).to(device)\n",
    "        bboxes = bboxes.to(device)\n",
    "        intents = intents.squeeze(0).to(device)\n",
    "    #     bboxes = bboxes.stack(images, dim=1).squeeze(2).to(device)\n",
    "    #     centers = centers.stack(images, dim=1).squeeze(2).to(device)\n",
    "    #     occlusions = occlusions.stack(images, dim=1).squeeze(2).to(device)\n",
    "    #     intents = intents.stack(images, dim=1).squeeze(2).to(device)\n",
    "        #print(images)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cropped_images,bboxes)\n",
    "        loss = criterion(outputs, intents)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, model, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            images = batch['image']\n",
    "            masks = batch['mask']\n",
    "            intents = batch['intent']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images, masks)\n",
    "            loss = criterion(outputs, intents.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "train_model(train_loader, model, criterion, optimizer, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torchvision import datasets, models, transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Step 1: Caricamento delle immagini\n",
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(224),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     'val': transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "# }\n",
    "\n",
    "# data_dir = 'path_to_your_data'\n",
    "# image_datasets = {x: datasets.ImageFolder(root=os.path.join(data_dir, x),\n",
    "#                                           transform=data_transforms[x])\n",
    "#                   for x in ['train', 'val']}\n",
    "# dataloaders = {x: DataLoader(image_datasets[x], batch_size=32,\n",
    "#                              shuffle=True, num_workers=4)\n",
    "#                for x in ['train', 'val']}\n",
    "# dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "# class_names = image_datasets['train'].classes\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Step 2: Caricamento del modello VGG16 preaddestrato\n",
    "# model = models.vgg16(pretrained=True)\n",
    "\n",
    "# # Step 3: Modifica dell'ultimo layer (fine-tuning)\n",
    "# num_ftrs = model.classifier[6].in_features\n",
    "# model.classifier[6] = nn.Linear(num_ftrs, 18)\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Step 4: Impostazione della perdita e dell'ottimizzatore\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# # Step 5: Addestramento del modello\n",
    "# def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "#         print('-' * 10)\n",
    "\n",
    "#         for phase in ['train', 'val']:\n",
    "#             if phase == 'train':\n",
    "#                 model.train()\n",
    "#             else:\n",
    "#                 model.eval()\n",
    "\n",
    "#             running_loss = 0.0\n",
    "#             running_corrects = 0\n",
    "\n",
    "#             for inputs, labels in dataloaders[phase]:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 with torch.set_grad_enabled(phase == 'train'):\n",
    "#                     outputs = model(inputs)\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "\n",
    "#                     if phase == 'train':\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "\n",
    "#                 running_loss += loss.item() * inputs.size(0)\n",
    "#                 running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "#             print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "#     return model\n",
    "\n",
    "# model_ft = train_model(model, criterion, optimizer, num_epochs=25)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
